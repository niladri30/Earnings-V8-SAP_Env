<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html><head><title>Python: module gen_ai_hub.proxy.langchain.openai</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head><body bgcolor="#f0f0f8">

<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="heading">
<tr bgcolor="#7799ee">
<td valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial">&nbsp;<br><big><big><strong><a href="gen_ai_hub.html"><font color="#ffffff">gen_ai_hub</font></a>.<a href="gen_ai_hub.proxy.html"><font color="#ffffff">proxy</font></a>.<a href="gen_ai_hub.proxy.langchain.html"><font color="#ffffff">langchain</font></a>.openai</strong></big></big></font></td
><td align=right valign=bottom
><font color="#ffffff" face="helvetica, arial"><a href=".">index</a><br><a href="file:/home/jenkins/agent/workspace/ation_generative-ai-hub-sdk_main/gen_ai_hub/proxy/langchain/openai.py">/home/jenkins/agent/workspace/ation_generative-ai-hub-sdk_main/gen_ai_hub/proxy/langchain/openai.py</a></font></td></tr></table>
    <p></p>
<p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ee77aa">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Classes</strong></big></font></td></tr>
    
<tr><td bgcolor="#ee77aa"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl>
<dt><font face="helvetica, arial"><a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a>(<a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>)
</font></dt><dd>
<dl>
<dt><font face="helvetica, arial"><a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>
</font></dt><dd>
<dl>
<dt><font face="helvetica, arial"><a href="gen_ai_hub.proxy.langchain.openai.html#ChatOpenAI">ChatOpenAI</a>(<a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>, <a href="langchain_openai.chat_models.base.html#ChatOpenAI">langchain_openai.chat_models.base.ChatOpenAI</a>)
</font></dt><dt><font face="helvetica, arial"><a href="gen_ai_hub.proxy.langchain.openai.html#OpenAI">OpenAI</a>(<a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>, <a href="langchain_openai.llms.base.html#OpenAI">langchain_openai.llms.base.OpenAI</a>)
</font></dt><dt><font face="helvetica, arial"><a href="gen_ai_hub.proxy.langchain.openai.html#OpenAIEmbeddings">OpenAIEmbeddings</a>(<a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>, <a href="langchain_openai.embeddings.base.html#OpenAIEmbeddings">langchain_openai.embeddings.base.OpenAIEmbeddings</a>)
</font></dt></dl>
</dd>
</dl>
</dd>
</dl>
 <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="ChatOpenAI">class <strong>ChatOpenAI</strong></a>(<a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>, <a href="langchain_openai.chat_models.base.html#ChatOpenAI">langchain_openai.chat_models.base.ChatOpenAI</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt><a href="#ChatOpenAI">ChatOpenAI</a>(*args,&nbsp;name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;cache:&nbsp;Union[langchain_core.caches.BaseCache,&nbsp;bool,&nbsp;NoneType]&nbsp;=&nbsp;None,&nbsp;verbose:&nbsp;bool&nbsp;=&nbsp;&amp;lt;factory&amp;gt;,&nbsp;callbacks:&nbsp;Union[list[langchain_core.callbacks.base.BaseCallbackHandler],&nbsp;langchain_core.callbacks.base.BaseCallbackManager,&nbsp;NoneType]&nbsp;=&nbsp;None,&nbsp;tags:&nbsp;Optional[list[str]]&nbsp;=&nbsp;None,&nbsp;metadata:&nbsp;Optional[dict[str,&nbsp;Any]]&nbsp;=&nbsp;None,&nbsp;custom_get_token_ids:&nbsp;Optional[Callable[[str],&nbsp;list[int]]]&nbsp;=&nbsp;None,&nbsp;callback_manager:&nbsp;Optional[langchain_core.callbacks.base.BaseCallbackManager]&nbsp;=&nbsp;None,&nbsp;rate_limiter:&nbsp;Optional[langchain_core.rate_limiters.BaseRateLimiter]&nbsp;=&nbsp;None,&nbsp;disable_streaming:&nbsp;Union[bool,&nbsp;Literal['tool_calling']]&nbsp;=&nbsp;False,&nbsp;client:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;async_client:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;root_client:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;root_async_client:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;model_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;temperature:&nbsp;Optional[float]&nbsp;=&nbsp;None,&nbsp;model_kwargs:&nbsp;Dict[str,&nbsp;Any]&nbsp;=&nbsp;&amp;lt;factory&amp;gt;,&nbsp;api_key:&nbsp;Optional[pydantic.types.SecretStr]&nbsp;=&nbsp;&amp;lt;factory&amp;gt;,&nbsp;base_url:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;organization:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;openai_proxy:&nbsp;Optional[str]&nbsp;=&nbsp;&amp;lt;factory&amp;gt;,&nbsp;timeout:&nbsp;Union[float,&nbsp;Tuple[float,&nbsp;float],&nbsp;Any,&nbsp;NoneType]&nbsp;=&nbsp;None,&nbsp;max_retries:&nbsp;Optional[int]&nbsp;=&nbsp;None,&nbsp;presence_penalty:&nbsp;Optional[float]&nbsp;=&nbsp;None,&nbsp;frequency_penalty:&nbsp;Optional[float]&nbsp;=&nbsp;None,&nbsp;seed:&nbsp;Optional[int]&nbsp;=&nbsp;None,&nbsp;logprobs:&nbsp;Optional[bool]&nbsp;=&nbsp;None,&nbsp;top_logprobs:&nbsp;Optional[int]&nbsp;=&nbsp;None,&nbsp;logit_bias:&nbsp;Optional[Dict[int,&nbsp;int]]&nbsp;=&nbsp;None,&nbsp;streaming:&nbsp;bool&nbsp;=&nbsp;False,&nbsp;n:&nbsp;Optional[int]&nbsp;=&nbsp;None,&nbsp;top_p:&nbsp;Optional[float]&nbsp;=&nbsp;None,&nbsp;max_completion_tokens:&nbsp;Optional[int]&nbsp;=&nbsp;None,&nbsp;reasoning_effort:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;tiktoken_model_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;default_headers:&nbsp;Optional[Mapping[str,&nbsp;str]]&nbsp;=&nbsp;None,&nbsp;default_query:&nbsp;Optional[Mapping[str,&nbsp;object]]&nbsp;=&nbsp;None,&nbsp;http_client:&nbsp;Optional[Any]&nbsp;=&nbsp;None,&nbsp;http_async_client:&nbsp;Optional[Any]&nbsp;=&nbsp;None,&nbsp;stop_sequences:&nbsp;Union[List[str],&nbsp;str,&nbsp;NoneType]&nbsp;=&nbsp;None,&nbsp;extra_body:&nbsp;Optional[Mapping[str,&nbsp;Any]]&nbsp;=&nbsp;None,&nbsp;include_response_headers:&nbsp;bool&nbsp;=&nbsp;False,&nbsp;disabled_params:&nbsp;Optional[Dict[str,&nbsp;Any]]&nbsp;=&nbsp;None,&nbsp;use_responses_api:&nbsp;Optional[bool]&nbsp;=&nbsp;None,&nbsp;stream_usage:&nbsp;bool&nbsp;=&nbsp;False,&nbsp;proxy_client:&nbsp;Optional[Any]&nbsp;=&nbsp;None,&nbsp;deployment_id:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;config_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;config_id:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;proxy_model_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;api_version:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;**kwargs)&nbsp;-&amp;gt;&nbsp;None<br>
&nbsp;<br>
<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="gen_ai_hub.proxy.langchain.openai.html#ChatOpenAI">ChatOpenAI</a></dd>
<dd><a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a></dd>
<dd><a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a></dd>
<dd><a href="langchain_openai.chat_models.base.html#ChatOpenAI">langchain_openai.chat_models.base.ChatOpenAI</a></dd>
<dd><a href="langchain_openai.chat_models.base.html#BaseChatOpenAI">langchain_openai.chat_models.base.BaseChatOpenAI</a></dd>
<dd><a href="langchain_core.language_models.chat_models.html#BaseChatModel">langchain_core.language_models.chat_models.BaseChatModel</a></dd>
<dd><a href="langchain_core.language_models.base.html#BaseLanguageModel[BaseMessage]">langchain_core.language_models.base.BaseLanguageModel[BaseMessage]</a></dd>
<dd><a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a></dd>
<dd><a href="langchain_core.runnables.base.html#RunnableSerializable[Union[PromptValue, str, Sequence[Union[BaseMessage, list[str], tuple[str, str], str, dict[str, Any]]]], ~LanguageModelOutputVar]">langchain_core.runnables.base.RunnableSerializable[Union[PromptValue, str, Sequence[Union[BaseMessage, list[str], tuple[str, str], str, dict[str, Any]]]], ~LanguageModelOutputVar]</a></dd>
<dd><a href="langchain_core.runnables.base.html#RunnableSerializable">langchain_core.runnables.base.RunnableSerializable</a></dd>
<dd><a href="langchain_core.load.serializable.html#Serializable">langchain_core.load.serializable.Serializable</a></dd>
<dd><a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a></dd>
<dd><a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a></dd>
<dd><a href="typing.html#Generic">typing.Generic</a></dd>
<dd><a href="abc.html#ABC">abc.ABC</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="ChatOpenAI-__init__"><strong>__init__</strong></a>(self, *args, **kwargs)</dt><dd><tt>Create&nbsp;a&nbsp;new&nbsp;model&nbsp;by&nbsp;parsing&nbsp;and&nbsp;validating&nbsp;input&nbsp;data&nbsp;from&nbsp;keyword&nbsp;arguments.<br>
&nbsp;<br>
Raises&nbsp;[`ValidationError`][pydantic_core.ValidationError]&nbsp;if&nbsp;the&nbsp;input&nbsp;data&nbsp;cannot&nbsp;be<br>
validated&nbsp;to&nbsp;form&nbsp;a&nbsp;valid&nbsp;model.<br>
&nbsp;<br>
`self`&nbsp;is&nbsp;explicitly&nbsp;positional-only&nbsp;to&nbsp;allow&nbsp;`self`&nbsp;as&nbsp;a&nbsp;field&nbsp;name.</tt></dd></dl>

<hr>
Class methods defined here:<br>
<dl><dt><a name="ChatOpenAI-validate_environment"><strong>validate_environment</strong></a>(values: Dict) -&gt; Dict<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Validate&nbsp;that&nbsp;api&nbsp;key&nbsp;and&nbsp;python&nbsp;package&nbsp;exists&nbsp;in&nbsp;environment.</tt></dd></dl>

<hr>
Static methods defined here:<br>
<dl><dt><a name="ChatOpenAI-__new__"><strong>__new__</strong></a>(cls, **data: Any)</dt><dd><tt>Initialize&nbsp;the&nbsp;<a href="#OpenAI">OpenAI</a>&nbsp;object.<br>
:param&nbsp;data:&nbsp;Additional&nbsp;data&nbsp;to&nbsp;initialize&nbsp;the&nbsp;object<br>
:type&nbsp;data:&nbsp;Any<br>
:return:&nbsp;The&nbsp;initialized&nbsp;<a href="#OpenAI">OpenAI</a>&nbsp;object<br>
:rtype:&nbsp;OpenAIBase</tt></dd></dl>

<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>__abstractmethods__</strong> = frozenset()</dl>

<dl><dt><strong>__annotations__</strong> = {'model_name': typing.Optional[str], 'openai_api_version': typing.Optional[str]}</dl>

<dl><dt><strong>__class_vars__</strong> = set()</dl>

<dl><dt><strong>__parameters__</strong> = ()</dl>

<dl><dt><strong>__private_attributes__</strong> = {}</dl>

<dl><dt><strong>__pydantic_complete__</strong> = True</dl>

<dl><dt><strong>__pydantic_computed_fields__</strong> = {}</dl>

<dl><dt><strong>__pydantic_core_schema__</strong> = {'cls': &lt;class 'gen_ai_hub.proxy.langchain.openai.ChatOpenAI'&gt;, 'config': {'extra_fields_behavior': 'allow', 'populate_by_name': True, 'title': 'ChatOpenAI'}, 'custom_init': True, 'metadata': {'pydantic_js_functions': [&lt;bound method BaseModel.__get_pydantic_json_sche... 'gen_ai_hub.proxy.langchain.openai.ChatOpenAI'&gt;&gt;]}, 'ref': 'gen_ai_hub.proxy.langchain.openai.ChatOpenAI:140673046519584', 'root_model': False, 'schema': {'function': {'function': &lt;bound method ChatOpenAI.validate_environment of... 'gen_ai_hub.proxy.langchain.openai.ChatOpenAI'&gt;&gt;, 'type': 'no-info'}, 'schema': {'function': {'function': &lt;bound method BaseChatOpenAI.validate_temperatur... 'gen_ai_hub.proxy.langchain.openai.ChatOpenAI'&gt;&gt;, 'type': 'no-info'}, 'schema': {'function': {'function': &lt;bound method BaseChatOpenAI.build_extra of &lt;class 'gen_ai_hub.proxy.langchain.openai.ChatOpenAI'&gt;&gt;, 'type': 'no-info'}, 'schema': {'function': {'function': &lt;bound method BaseChatModel.raise_deprecation of... 'gen_ai_hub.proxy.langchain.openai.ChatOpenAI'&gt;&gt;, 'type': 'no-info'}, 'schema': {'computed_fields': [], 'fields': {...}, 'model_name': 'ChatOpenAI', 'type': 'model-fields'}, 'type': 'function-before'}, 'type': 'function-before'}, 'type': 'function-before'}, 'type': 'function-before'}, 'type': 'model'}</dl>

<dl><dt><strong>__pydantic_custom_init__</strong> = True</dl>

<dl><dt><strong>__pydantic_decorators__</strong> = DecoratorInfos(validators={}, field_validators={...coratorInfo(mode='before'))}, computed_fields={})</dl>

<dl><dt><strong>__pydantic_fields__</strong> = {'async_client': FieldInfo(annotation=Any, required=False, default=None, exclude=True), 'cache': FieldInfo(annotation=Union[BaseCache, bool, NoneType], required=False, default=None, exclude=True), 'callback_manager': FieldInfo(annotation=Union[BaseCallbackManager, ... manager to add to the run trace.', exclude=True), 'callbacks': FieldInfo(annotation=Union[list[BaseCallbackHand...ype], required=False, default=None, exclude=True), 'client': FieldInfo(annotation=Any, required=False, default=None, exclude=True), 'config_id': FieldInfo(annotation=Union[str, NoneType], required=False, default=None), 'config_name': FieldInfo(annotation=Union[str, NoneType], required=False, default=None), 'custom_get_token_ids': FieldInfo(annotation=Union[Callable[list, list[i...ype], required=False, default=None, exclude=True), 'default_headers': FieldInfo(annotation=Union[Mapping[str, str], NoneType], required=False, default=None), 'default_query': FieldInfo(annotation=Union[Mapping[str, object], NoneType], required=False, default=None), ...}</dl>

<dl><dt><strong>__pydantic_generic_metadata__</strong> = {'args': (), 'origin': None, 'parameters': ()}</dl>

<dl><dt><strong>__pydantic_parent_namespace__</strong> = None</dl>

<dl><dt><strong>__pydantic_post_init__</strong> = None</dl>

<dl><dt><strong>__pydantic_serializer__</strong> = SchemaSerializer(serializer=Model(
    ModelSeri...    name: "ChatOpenAI",
    },
), definitions=[])</dl>

<dl><dt><strong>__pydantic_validator__</strong> = SchemaValidator(title="ChatOpenAI", validator=Mo...I",
    },
), definitions=[], cache_strings=True)</dl>

<dl><dt><strong>__signature__</strong> = &lt;Signature (*args, name: Optional[str] = None, c...version: Optional[str] = None, **kwargs) -&gt; None&gt;</dl>

<dl><dt><strong>model_config</strong> = {'arbitrary_types_allowed': True, 'extra': 'allow', 'populate_by_name': True, 'protected_namespaces': ()}</dl>

<hr>
Class methods inherited from <a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>:<br>
<dl><dt><a name="ChatOpenAI-validate_clients"><strong>validate_clients</strong></a>(values: Dict) -&gt; Dict<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<hr>
Data descriptors inherited from <a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a>:<br>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Methods inherited from <a href="langchain_openai.chat_models.base.html#ChatOpenAI">langchain_openai.chat_models.base.ChatOpenAI</a>:<br>
<dl><dt><a name="ChatOpenAI-with_structured_output"><strong>with_structured_output</strong></a>(self, schema: 'Optional[_DictOrPydanticClass]' = None, *, method: "Literal['function_calling', 'json_mode', 'json_schema']" = 'json_schema', include_raw: 'bool' = False, strict: 'Optional[bool]' = None, **kwargs: 'Any') -&gt; 'Runnable[LanguageModelInput, _DictOrPydantic]'</dt><dd><tt>Model&nbsp;wrapper&nbsp;that&nbsp;returns&nbsp;outputs&nbsp;formatted&nbsp;to&nbsp;match&nbsp;the&nbsp;given&nbsp;schema.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;schema:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;schema.&nbsp;Can&nbsp;be&nbsp;passed&nbsp;in&nbsp;as:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;a&nbsp;JSON&nbsp;Schema,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;a&nbsp;TypedDict&nbsp;class,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;or&nbsp;a&nbsp;Pydantic&nbsp;class,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;an&nbsp;<a href="#OpenAI">OpenAI</a>&nbsp;function/tool&nbsp;schema.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;``schema``&nbsp;is&nbsp;a&nbsp;Pydantic&nbsp;class&nbsp;then&nbsp;the&nbsp;model&nbsp;output&nbsp;will&nbsp;be&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pydantic&nbsp;instance&nbsp;of&nbsp;that&nbsp;class,&nbsp;and&nbsp;the&nbsp;model-generated&nbsp;fields&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;validated&nbsp;by&nbsp;the&nbsp;Pydantic&nbsp;class.&nbsp;Otherwise&nbsp;the&nbsp;model&nbsp;output&nbsp;will&nbsp;be&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dict&nbsp;and&nbsp;will&nbsp;not&nbsp;be&nbsp;validated.&nbsp;See&nbsp;:meth:`langchain_core.utils.function_calling.convert_to_openai_tool`<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;more&nbsp;on&nbsp;how&nbsp;to&nbsp;properly&nbsp;specify&nbsp;types&nbsp;and&nbsp;descriptions&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;schema&nbsp;fields&nbsp;when&nbsp;specifying&nbsp;a&nbsp;Pydantic&nbsp;or&nbsp;TypedDict&nbsp;class.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;method:&nbsp;The&nbsp;method&nbsp;for&nbsp;steering&nbsp;model&nbsp;generation,&nbsp;one&nbsp;of:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;"json_schema":<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Uses&nbsp;<a href="#OpenAI">OpenAI</a>'s&nbsp;Structured&nbsp;Output&nbsp;API:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://platform.openai.com/docs/guides/structured-outputs">https://platform.openai.com/docs/guides/structured-outputs</a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Supported&nbsp;for&nbsp;"gpt-4o-mini",&nbsp;"gpt-4o-2024-08-06",&nbsp;"o1",&nbsp;and&nbsp;later<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;models.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;"function_calling":<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Uses&nbsp;<a href="#OpenAI">OpenAI</a>'s&nbsp;tool-calling&nbsp;(formerly&nbsp;called&nbsp;function&nbsp;calling)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;API:&nbsp;<a href="https://platform.openai.com/docs/guides/function-calling">https://platform.openai.com/docs/guides/function-calling</a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;"json_mode":<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Uses&nbsp;<a href="#OpenAI">OpenAI</a>'s&nbsp;JSON&nbsp;mode.&nbsp;Note&nbsp;that&nbsp;if&nbsp;using&nbsp;JSON&nbsp;mode&nbsp;then&nbsp;you<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;must&nbsp;include&nbsp;instructions&nbsp;for&nbsp;formatting&nbsp;the&nbsp;output&nbsp;into&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;desired&nbsp;schema&nbsp;into&nbsp;the&nbsp;model&nbsp;call:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://platform.openai.com/docs/guides/structured-outputs/json-mode">https://platform.openai.com/docs/guides/structured-outputs/json-mode</a><br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Learn&nbsp;more&nbsp;about&nbsp;the&nbsp;differences&nbsp;between&nbsp;the&nbsp;methods&nbsp;and&nbsp;which&nbsp;models<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;support&nbsp;which&nbsp;methods&nbsp;here:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;<a href="https://platform.openai.com/docs/guides/structured-outputs/structured-outputs-vs-json-mode">https://platform.openai.com/docs/guides/structured-outputs/structured-outputs-vs-json-mode</a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;<a href="https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format">https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format</a><br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_raw:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;False&nbsp;then&nbsp;only&nbsp;the&nbsp;parsed&nbsp;structured&nbsp;output&nbsp;is&nbsp;returned.&nbsp;If<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;an&nbsp;error&nbsp;occurs&nbsp;during&nbsp;model&nbsp;output&nbsp;parsing&nbsp;it&nbsp;will&nbsp;be&nbsp;raised.&nbsp;If&nbsp;True<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;then&nbsp;both&nbsp;the&nbsp;raw&nbsp;model&nbsp;response&nbsp;(a&nbsp;BaseMessage)&nbsp;and&nbsp;the&nbsp;parsed&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;response&nbsp;will&nbsp;be&nbsp;returned.&nbsp;If&nbsp;an&nbsp;error&nbsp;occurs&nbsp;during&nbsp;output&nbsp;parsing&nbsp;it<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;be&nbsp;caught&nbsp;and&nbsp;returned&nbsp;as&nbsp;well.&nbsp;The&nbsp;final&nbsp;output&nbsp;is&nbsp;always&nbsp;a&nbsp;dict<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;keys&nbsp;"raw",&nbsp;"parsed",&nbsp;and&nbsp;"parsing_error".<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;True:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Model&nbsp;output&nbsp;is&nbsp;guaranteed&nbsp;to&nbsp;exactly&nbsp;match&nbsp;the&nbsp;schema.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;input&nbsp;schema&nbsp;will&nbsp;also&nbsp;be&nbsp;validated&nbsp;according&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://platform.openai.com/docs/guides/structured-outputs/supported-schemas">https://platform.openai.com/docs/guides/structured-outputs/supported-schemas</a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;False:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Input&nbsp;schema&nbsp;will&nbsp;not&nbsp;be&nbsp;validated&nbsp;and&nbsp;model&nbsp;output&nbsp;will&nbsp;not&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;validated.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;None:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``strict``&nbsp;argument&nbsp;will&nbsp;not&nbsp;be&nbsp;passed&nbsp;to&nbsp;the&nbsp;model.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;schema&nbsp;is&nbsp;specified&nbsp;via&nbsp;TypedDict&nbsp;or&nbsp;JSON&nbsp;schema,&nbsp;``strict``&nbsp;is&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enabled&nbsp;by&nbsp;default.&nbsp;Pass&nbsp;``strict=True``&nbsp;to&nbsp;enable&nbsp;it.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Note:&nbsp;``strict``&nbsp;can&nbsp;only&nbsp;be&nbsp;non-null&nbsp;if&nbsp;``method``&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;``"json_schema"``&nbsp;or&nbsp;``"function_calling"``.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;args&nbsp;aren't&nbsp;supported.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;Runnable&nbsp;that&nbsp;takes&nbsp;same&nbsp;inputs&nbsp;as&nbsp;a&nbsp;:class:`langchain_core.language_models.chat.BaseChatModel`.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;If&nbsp;``include_raw``&nbsp;is&nbsp;False&nbsp;and&nbsp;``schema``&nbsp;is&nbsp;a&nbsp;Pydantic&nbsp;class,&nbsp;Runnable&nbsp;outputs&nbsp;an&nbsp;instance&nbsp;of&nbsp;``schema``&nbsp;(i.e.,&nbsp;a&nbsp;Pydantic&nbsp;object).&nbsp;Otherwise,&nbsp;if&nbsp;``include_raw``&nbsp;is&nbsp;False&nbsp;then&nbsp;Runnable&nbsp;outputs&nbsp;a&nbsp;dict.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;If&nbsp;``include_raw``&nbsp;is&nbsp;True,&nbsp;then&nbsp;Runnable&nbsp;outputs&nbsp;a&nbsp;dict&nbsp;with&nbsp;keys:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;"raw":&nbsp;BaseMessage<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;"parsed":&nbsp;None&nbsp;if&nbsp;there&nbsp;was&nbsp;a&nbsp;parsing&nbsp;error,&nbsp;otherwise&nbsp;the&nbsp;type&nbsp;depends&nbsp;on&nbsp;the&nbsp;``schema``&nbsp;as&nbsp;described&nbsp;above.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;"parsing_error":&nbsp;Optional[BaseException]<br>
&nbsp;<br>
..&nbsp;versionchanged::&nbsp;0.1.20<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Added&nbsp;support&nbsp;for&nbsp;TypedDict&nbsp;class&nbsp;``schema``.<br>
&nbsp;<br>
..&nbsp;versionchanged::&nbsp;0.1.21<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Support&nbsp;for&nbsp;``strict``&nbsp;argument&nbsp;added.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Support&nbsp;for&nbsp;``method="json_schema"``&nbsp;added.<br>
&nbsp;<br>
..&nbsp;versionchanged::&nbsp;0.3.0<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;``method``&nbsp;default&nbsp;changed&nbsp;from&nbsp;"function_calling"&nbsp;to&nbsp;"json_schema".<br>
&nbsp;<br>
..&nbsp;dropdown::&nbsp;Example:&nbsp;schema=Pydantic&nbsp;class,&nbsp;method="json_schema",&nbsp;include_raw=False,&nbsp;strict=True<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Note,&nbsp;<a href="#OpenAI">OpenAI</a>&nbsp;has&nbsp;a&nbsp;number&nbsp;of&nbsp;restrictions&nbsp;on&nbsp;what&nbsp;types&nbsp;of&nbsp;schemas&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;provided&nbsp;if&nbsp;``strict``&nbsp;=&nbsp;True.&nbsp;When&nbsp;using&nbsp;Pydantic,&nbsp;our&nbsp;model&nbsp;cannot<br>
&nbsp;&nbsp;&nbsp;&nbsp;specify&nbsp;any&nbsp;Field&nbsp;metadata&nbsp;(like&nbsp;min/max&nbsp;constraints)&nbsp;and&nbsp;fields&nbsp;cannot<br>
&nbsp;&nbsp;&nbsp;&nbsp;have&nbsp;default&nbsp;values.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;See&nbsp;all&nbsp;constraints&nbsp;here:&nbsp;<a href="https://platform.openai.com/docs/guides/structured-outputs/supported-schemas">https://platform.openai.com/docs/guides/structured-outputs/supported-schemas</a><br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing&nbsp;import&nbsp;Optional<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_openai&nbsp;import&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;pydantic&nbsp;import&nbsp;BaseModel,&nbsp;Field<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;AnswerWithJustification(BaseModel):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'''An&nbsp;answer&nbsp;to&nbsp;the&nbsp;user&nbsp;question&nbsp;along&nbsp;with&nbsp;justification&nbsp;for&nbsp;the&nbsp;answer.'''<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;answer:&nbsp;str<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;justification:&nbsp;Optional[str]&nbsp;=&nbsp;Field(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;default=...,&nbsp;description="A&nbsp;justification&nbsp;for&nbsp;the&nbsp;answer."<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;llm&nbsp;=&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a>(model="gpt-4o",&nbsp;temperature=0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm&nbsp;=&nbsp;llm.<a href="#ChatOpenAI-with_structured_output">with_structured_output</a>(AnswerWithJustification)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm.<a href="#ChatOpenAI-invoke">invoke</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"What&nbsp;weighs&nbsp;more&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;or&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;AnswerWithJustification(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;answer='They&nbsp;weigh&nbsp;the&nbsp;same',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;justification='Both&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;and&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers&nbsp;weigh&nbsp;one&nbsp;pound.&nbsp;The&nbsp;weight&nbsp;is&nbsp;the&nbsp;same,&nbsp;but&nbsp;the&nbsp;volume&nbsp;or&nbsp;density&nbsp;of&nbsp;the&nbsp;objects&nbsp;may&nbsp;differ.'<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;)<br>
&nbsp;<br>
..&nbsp;dropdown::&nbsp;Example:&nbsp;schema=Pydantic&nbsp;class,&nbsp;method="function_calling",&nbsp;include_raw=False,&nbsp;strict=False<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing&nbsp;import&nbsp;Optional<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_openai&nbsp;import&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;pydantic&nbsp;import&nbsp;BaseModel,&nbsp;Field<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;AnswerWithJustification(BaseModel):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'''An&nbsp;answer&nbsp;to&nbsp;the&nbsp;user&nbsp;question&nbsp;along&nbsp;with&nbsp;justification&nbsp;for&nbsp;the&nbsp;answer.'''<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;answer:&nbsp;str<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;justification:&nbsp;Optional[str]&nbsp;=&nbsp;Field(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;default=...,&nbsp;description="A&nbsp;justification&nbsp;for&nbsp;the&nbsp;answer."<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;llm&nbsp;=&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a>(model="gpt-4o",&nbsp;temperature=0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm&nbsp;=&nbsp;llm.<a href="#ChatOpenAI-with_structured_output">with_structured_output</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AnswerWithJustification,&nbsp;method="function_calling"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm.<a href="#ChatOpenAI-invoke">invoke</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"What&nbsp;weighs&nbsp;more&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;or&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;AnswerWithJustification(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;answer='They&nbsp;weigh&nbsp;the&nbsp;same',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;justification='Both&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;and&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers&nbsp;weigh&nbsp;one&nbsp;pound.&nbsp;The&nbsp;weight&nbsp;is&nbsp;the&nbsp;same,&nbsp;but&nbsp;the&nbsp;volume&nbsp;or&nbsp;density&nbsp;of&nbsp;the&nbsp;objects&nbsp;may&nbsp;differ.'<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;)<br>
&nbsp;<br>
..&nbsp;dropdown::&nbsp;Example:&nbsp;schema=Pydantic&nbsp;class,&nbsp;method="json_schema",&nbsp;include_raw=True<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_openai&nbsp;import&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;pydantic&nbsp;import&nbsp;BaseModel<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;AnswerWithJustification(BaseModel):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'''An&nbsp;answer&nbsp;to&nbsp;the&nbsp;user&nbsp;question&nbsp;along&nbsp;with&nbsp;justification&nbsp;for&nbsp;the&nbsp;answer.'''<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;answer:&nbsp;str<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;justification:&nbsp;str<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;llm&nbsp;=&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a>(model="gpt-4o",&nbsp;temperature=0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm&nbsp;=&nbsp;llm.<a href="#ChatOpenAI-with_structured_output">with_structured_output</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AnswerWithJustification,&nbsp;include_raw=True<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm.<a href="#ChatOpenAI-invoke">invoke</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"What&nbsp;weighs&nbsp;more&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;or&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'raw':&nbsp;AIMessage(content='',&nbsp;additional_kwargs={'tool_calls':&nbsp;[{'id':&nbsp;'call_Ao02pnFYXD6GN1yzc0uXPsvF',&nbsp;'function':&nbsp;{'arguments':&nbsp;'{"answer":"They&nbsp;weigh&nbsp;the&nbsp;same.","justification":"Both&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;and&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers&nbsp;weigh&nbsp;one&nbsp;pound.&nbsp;The&nbsp;weight&nbsp;is&nbsp;the&nbsp;same,&nbsp;but&nbsp;the&nbsp;volume&nbsp;or&nbsp;density&nbsp;of&nbsp;the&nbsp;objects&nbsp;may&nbsp;differ."}',&nbsp;'name':&nbsp;'AnswerWithJustification'},&nbsp;'type':&nbsp;'function'}]}),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'parsed':&nbsp;AnswerWithJustification(answer='They&nbsp;weigh&nbsp;the&nbsp;same.',&nbsp;justification='Both&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;and&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers&nbsp;weigh&nbsp;one&nbsp;pound.&nbsp;The&nbsp;weight&nbsp;is&nbsp;the&nbsp;same,&nbsp;but&nbsp;the&nbsp;volume&nbsp;or&nbsp;density&nbsp;of&nbsp;the&nbsp;objects&nbsp;may&nbsp;differ.'),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'parsing_error':&nbsp;None<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;}<br>
&nbsp;<br>
..&nbsp;dropdown::&nbsp;Example:&nbsp;schema=TypedDict&nbsp;class,&nbsp;method="json_schema",&nbsp;include_raw=False,&nbsp;strict=False<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;IMPORTANT:&nbsp;If&nbsp;you&nbsp;are&nbsp;using&nbsp;Python&nbsp;&lt;=3.8,&nbsp;you&nbsp;need&nbsp;to&nbsp;import&nbsp;Annotated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;from&nbsp;typing_extensions,&nbsp;not&nbsp;from&nbsp;typing.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing_extensions&nbsp;import&nbsp;Annotated,&nbsp;TypedDict<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_openai&nbsp;import&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a><br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;AnswerWithJustification(TypedDict):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'''An&nbsp;answer&nbsp;to&nbsp;the&nbsp;user&nbsp;question&nbsp;along&nbsp;with&nbsp;justification&nbsp;for&nbsp;the&nbsp;answer.'''<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;answer:&nbsp;str<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;justification:&nbsp;Annotated[<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Optional[str],&nbsp;None,&nbsp;"A&nbsp;justification&nbsp;for&nbsp;the&nbsp;answer."<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;llm&nbsp;=&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a>(model="gpt-4o",&nbsp;temperature=0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm&nbsp;=&nbsp;llm.<a href="#ChatOpenAI-with_structured_output">with_structured_output</a>(AnswerWithJustification)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm.<a href="#ChatOpenAI-invoke">invoke</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"What&nbsp;weighs&nbsp;more&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;or&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'answer':&nbsp;'They&nbsp;weigh&nbsp;the&nbsp;same',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'justification':&nbsp;'Both&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;and&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers&nbsp;weigh&nbsp;one&nbsp;pound.&nbsp;The&nbsp;weight&nbsp;is&nbsp;the&nbsp;same,&nbsp;but&nbsp;the&nbsp;volume&nbsp;and&nbsp;density&nbsp;of&nbsp;the&nbsp;two&nbsp;substances&nbsp;differ.'<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;}<br>
&nbsp;<br>
..&nbsp;dropdown::&nbsp;Example:&nbsp;schema=<a href="#OpenAI">OpenAI</a>&nbsp;function&nbsp;schema,&nbsp;method="json_schema",&nbsp;include_raw=False<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_openai&nbsp;import&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a><br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;oai_schema&nbsp;=&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'name':&nbsp;'AnswerWithJustification',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'description':&nbsp;'An&nbsp;answer&nbsp;to&nbsp;the&nbsp;user&nbsp;question&nbsp;along&nbsp;with&nbsp;justification&nbsp;for&nbsp;the&nbsp;answer.',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'parameters':&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'type':&nbsp;'object',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'properties':&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'answer':&nbsp;{'type':&nbsp;'string'},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'justification':&nbsp;{'description':&nbsp;'A&nbsp;justification&nbsp;for&nbsp;the&nbsp;answer.',&nbsp;'type':&nbsp;'string'}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'required':&nbsp;['answer']<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;llm&nbsp;=&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a>(model="gpt-4o",&nbsp;temperature=0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm&nbsp;=&nbsp;llm.<a href="#ChatOpenAI-with_structured_output">with_structured_output</a>(oai_schema)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm.<a href="#ChatOpenAI-invoke">invoke</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"What&nbsp;weighs&nbsp;more&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;or&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'answer':&nbsp;'They&nbsp;weigh&nbsp;the&nbsp;same',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'justification':&nbsp;'Both&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;and&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers&nbsp;weigh&nbsp;one&nbsp;pound.&nbsp;The&nbsp;weight&nbsp;is&nbsp;the&nbsp;same,&nbsp;but&nbsp;the&nbsp;volume&nbsp;and&nbsp;density&nbsp;of&nbsp;the&nbsp;two&nbsp;substances&nbsp;differ.'<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;}<br>
&nbsp;<br>
..&nbsp;dropdown::&nbsp;Example:&nbsp;schema=Pydantic&nbsp;class,&nbsp;method="json_mode",&nbsp;include_raw=True<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_openai&nbsp;import&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;pydantic&nbsp;import&nbsp;BaseModel<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;AnswerWithJustification(BaseModel):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;answer:&nbsp;str<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;justification:&nbsp;str<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;llm&nbsp;=&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a>(model="gpt-4o",&nbsp;temperature=0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm&nbsp;=&nbsp;llm.<a href="#ChatOpenAI-with_structured_output">with_structured_output</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AnswerWithJustification,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;method="json_mode",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;include_raw=True<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm.<a href="#ChatOpenAI-invoke">invoke</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Answer&nbsp;the&nbsp;following&nbsp;question.&nbsp;"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Make&nbsp;sure&nbsp;to&nbsp;return&nbsp;a&nbsp;JSON&nbsp;blob&nbsp;with&nbsp;keys&nbsp;'answer'&nbsp;and&nbsp;'justification'.\n\n"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"What's&nbsp;heavier&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;or&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers?"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'raw':&nbsp;AIMessage(content='{\n&nbsp;&nbsp;&nbsp;&nbsp;"answer":&nbsp;"They&nbsp;are&nbsp;both&nbsp;the&nbsp;same&nbsp;weight.",\n&nbsp;&nbsp;&nbsp;&nbsp;"justification":&nbsp;"Both&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;and&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers&nbsp;weigh&nbsp;one&nbsp;pound.&nbsp;The&nbsp;difference&nbsp;lies&nbsp;in&nbsp;the&nbsp;volume&nbsp;and&nbsp;density&nbsp;of&nbsp;the&nbsp;materials,&nbsp;not&nbsp;the&nbsp;weight."&nbsp;\n}'),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'parsed':&nbsp;AnswerWithJustification(answer='They&nbsp;are&nbsp;both&nbsp;the&nbsp;same&nbsp;weight.',&nbsp;justification='Both&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;and&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers&nbsp;weigh&nbsp;one&nbsp;pound.&nbsp;The&nbsp;difference&nbsp;lies&nbsp;in&nbsp;the&nbsp;volume&nbsp;and&nbsp;density&nbsp;of&nbsp;the&nbsp;materials,&nbsp;not&nbsp;the&nbsp;weight.'),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'parsing_error':&nbsp;None<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;}<br>
&nbsp;<br>
..&nbsp;dropdown::&nbsp;Example:&nbsp;schema=None,&nbsp;method="json_mode",&nbsp;include_raw=True<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm&nbsp;=&nbsp;llm.<a href="#ChatOpenAI-with_structured_output">with_structured_output</a>(method="json_mode",&nbsp;include_raw=True)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm.<a href="#ChatOpenAI-invoke">invoke</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Answer&nbsp;the&nbsp;following&nbsp;question.&nbsp;"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Make&nbsp;sure&nbsp;to&nbsp;return&nbsp;a&nbsp;JSON&nbsp;blob&nbsp;with&nbsp;keys&nbsp;'answer'&nbsp;and&nbsp;'justification'.\n\n"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"What's&nbsp;heavier&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;or&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers?"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'raw':&nbsp;AIMessage(content='{\n&nbsp;&nbsp;&nbsp;&nbsp;"answer":&nbsp;"They&nbsp;are&nbsp;both&nbsp;the&nbsp;same&nbsp;weight.",\n&nbsp;&nbsp;&nbsp;&nbsp;"justification":&nbsp;"Both&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;and&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers&nbsp;weigh&nbsp;one&nbsp;pound.&nbsp;The&nbsp;difference&nbsp;lies&nbsp;in&nbsp;the&nbsp;volume&nbsp;and&nbsp;density&nbsp;of&nbsp;the&nbsp;materials,&nbsp;not&nbsp;the&nbsp;weight."&nbsp;\n}'),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'parsed':&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'answer':&nbsp;'They&nbsp;are&nbsp;both&nbsp;the&nbsp;same&nbsp;weight.',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'justification':&nbsp;'Both&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;and&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers&nbsp;weigh&nbsp;one&nbsp;pound.&nbsp;The&nbsp;difference&nbsp;lies&nbsp;in&nbsp;the&nbsp;volume&nbsp;and&nbsp;density&nbsp;of&nbsp;the&nbsp;materials,&nbsp;not&nbsp;the&nbsp;weight.'<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'parsing_error':&nbsp;None<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;}</tt></dd></dl>

<hr>
Class methods inherited from <a href="langchain_openai.chat_models.base.html#ChatOpenAI">langchain_openai.chat_models.base.ChatOpenAI</a>:<br>
<dl><dt><a name="ChatOpenAI-get_lc_namespace"><strong>get_lc_namespace</strong></a>() -&gt; 'List[str]'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Get&nbsp;the&nbsp;namespace&nbsp;of&nbsp;the&nbsp;langchain&nbsp;object.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-is_lc_serializable"><strong>is_lc_serializable</strong></a>() -&gt; 'bool'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Return&nbsp;whether&nbsp;this&nbsp;model&nbsp;can&nbsp;be&nbsp;serialized&nbsp;by&nbsp;Langchain.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_openai.chat_models.base.html#ChatOpenAI">langchain_openai.chat_models.base.ChatOpenAI</a>:<br>
<dl><dt><strong>lc_attributes</strong></dt>
<dd><tt>List&nbsp;of&nbsp;attribute&nbsp;names&nbsp;that&nbsp;should&nbsp;be&nbsp;included&nbsp;in&nbsp;the&nbsp;serialized&nbsp;kwargs.<br>
&nbsp;<br>
These&nbsp;attributes&nbsp;must&nbsp;be&nbsp;accepted&nbsp;by&nbsp;the&nbsp;constructor.<br>
Default&nbsp;is&nbsp;an&nbsp;empty&nbsp;dictionary.</tt></dd>
</dl>
<dl><dt><strong>lc_secrets</strong></dt>
<dd><tt>A&nbsp;map&nbsp;of&nbsp;constructor&nbsp;argument&nbsp;names&nbsp;to&nbsp;secret&nbsp;ids.<br>
&nbsp;<br>
For&nbsp;example,<br>
&nbsp;&nbsp;&nbsp;&nbsp;{"openai_api_key":&nbsp;"OPENAI_API_KEY"}</tt></dd>
</dl>
<hr>
Methods inherited from <a href="langchain_openai.chat_models.base.html#BaseChatOpenAI">langchain_openai.chat_models.base.BaseChatOpenAI</a>:<br>
<dl><dt><a name="ChatOpenAI-bind_functions"><strong>bind_functions</strong></a>(self, functions: 'Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]]', function_call: "Optional[Union[_FunctionCall, str, Literal['auto', 'none']]]" = None, **kwargs: 'Any') -&gt; 'Runnable[LanguageModelInput, BaseMessage]'</dt><dd><tt>..&nbsp;deprecated::&nbsp;0.2.1&nbsp;Use&nbsp;:meth:`~langchain_openai.chat_models.base.<a href="#ChatOpenAI">ChatOpenAI</a>.bind_tools`&nbsp;instead.&nbsp;It&nbsp;will&nbsp;not&nbsp;be&nbsp;removed&nbsp;until&nbsp;langchain-openai==1.0.0.<br>
&nbsp;<br>
Bind&nbsp;functions&nbsp;(and&nbsp;other&nbsp;objects)&nbsp;to&nbsp;this&nbsp;chat&nbsp;model.<br>
&nbsp;<br>
Assumes&nbsp;model&nbsp;is&nbsp;compatible&nbsp;with&nbsp;<a href="#OpenAI">OpenAI</a>&nbsp;function-calling&nbsp;API.<br>
&nbsp;<br>
NOTE:&nbsp;Using&nbsp;bind_tools&nbsp;is&nbsp;recommended&nbsp;instead,&nbsp;as&nbsp;the&nbsp;`functions`&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;`function_call`&nbsp;request&nbsp;parameters&nbsp;are&nbsp;officially&nbsp;marked&nbsp;as&nbsp;deprecated&nbsp;by<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#OpenAI">OpenAI</a>.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;functions:&nbsp;A&nbsp;list&nbsp;of&nbsp;function&nbsp;definitions&nbsp;to&nbsp;bind&nbsp;to&nbsp;this&nbsp;chat&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Can&nbsp;be&nbsp;&nbsp;a&nbsp;dictionary,&nbsp;pydantic&nbsp;model,&nbsp;or&nbsp;callable.&nbsp;Pydantic<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;models&nbsp;and&nbsp;callables&nbsp;will&nbsp;be&nbsp;automatically&nbsp;converted&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;their&nbsp;schema&nbsp;dictionary&nbsp;representation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;function_call:&nbsp;Which&nbsp;function&nbsp;to&nbsp;require&nbsp;the&nbsp;model&nbsp;to&nbsp;call.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Must&nbsp;be&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;single&nbsp;provided&nbsp;function&nbsp;or<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"auto"&nbsp;to&nbsp;automatically&nbsp;determine&nbsp;which&nbsp;function&nbsp;to&nbsp;call<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(if&nbsp;any).<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Any&nbsp;additional&nbsp;parameters&nbsp;to&nbsp;pass&nbsp;to&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`~langchain.runnable.Runnable`&nbsp;constructor.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-bind_tools"><strong>bind_tools</strong></a>(self, tools: 'Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]]', *, tool_choice: "Optional[Union[dict, str, Literal['auto', 'none', 'required', 'any'], bool]]" = None, strict: 'Optional[bool]' = None, parallel_tool_calls: 'Optional[bool]' = None, **kwargs: 'Any') -&gt; 'Runnable[LanguageModelInput, BaseMessage]'</dt><dd><tt>Bind&nbsp;tool-like&nbsp;objects&nbsp;to&nbsp;this&nbsp;chat&nbsp;model.<br>
&nbsp;<br>
Assumes&nbsp;model&nbsp;is&nbsp;compatible&nbsp;with&nbsp;<a href="#OpenAI">OpenAI</a>&nbsp;tool-calling&nbsp;API.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tools:&nbsp;A&nbsp;list&nbsp;of&nbsp;tool&nbsp;definitions&nbsp;to&nbsp;bind&nbsp;to&nbsp;this&nbsp;chat&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Supports&nbsp;any&nbsp;tool&nbsp;definition&nbsp;handled&nbsp;by<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:meth:`langchain_core.utils.function_calling.convert_to_openai_tool`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;tool_choice:&nbsp;Which&nbsp;tool&nbsp;to&nbsp;require&nbsp;the&nbsp;model&nbsp;to&nbsp;call.&nbsp;Options&nbsp;are:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;str&nbsp;of&nbsp;the&nbsp;form&nbsp;``"&lt;&lt;tool_name&gt;&gt;"``:&nbsp;calls&nbsp;&lt;&lt;tool_name&gt;&gt;&nbsp;tool.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``"auto"``:&nbsp;automatically&nbsp;selects&nbsp;a&nbsp;tool&nbsp;(including&nbsp;no&nbsp;tool).<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``"none"``:&nbsp;does&nbsp;not&nbsp;call&nbsp;a&nbsp;tool.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``"any"``&nbsp;or&nbsp;``"required"``&nbsp;or&nbsp;``True``:&nbsp;force&nbsp;at&nbsp;least&nbsp;one&nbsp;tool&nbsp;to&nbsp;be&nbsp;called.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;dict&nbsp;of&nbsp;the&nbsp;form&nbsp;``{"type":&nbsp;"function",&nbsp;"function":&nbsp;{"name":&nbsp;&lt;&lt;tool_name&gt;&gt;}}``:&nbsp;calls&nbsp;&lt;&lt;tool_name&gt;&gt;&nbsp;tool.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``False``&nbsp;or&nbsp;``None``:&nbsp;no&nbsp;effect,&nbsp;default&nbsp;<a href="#OpenAI">OpenAI</a>&nbsp;behavior.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;If&nbsp;True,&nbsp;model&nbsp;output&nbsp;is&nbsp;guaranteed&nbsp;to&nbsp;exactly&nbsp;match&nbsp;the&nbsp;JSON&nbsp;Schema<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;provided&nbsp;in&nbsp;the&nbsp;tool&nbsp;definition.&nbsp;If&nbsp;True,&nbsp;the&nbsp;input&nbsp;schema&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;validated&nbsp;according&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://platform.openai.com/docs/guides/structured-outputs/supported-schemas">https://platform.openai.com/docs/guides/structured-outputs/supported-schemas</a>.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;False,&nbsp;input&nbsp;schema&nbsp;will&nbsp;not&nbsp;be&nbsp;validated&nbsp;and&nbsp;model&nbsp;output&nbsp;will&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;validated.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;None,&nbsp;``strict``&nbsp;argument&nbsp;will&nbsp;not&nbsp;be&nbsp;passed&nbsp;to&nbsp;the&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;parallel_tool_calls:&nbsp;Set&nbsp;to&nbsp;``False``&nbsp;to&nbsp;disable&nbsp;parallel&nbsp;tool&nbsp;use.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;``None``&nbsp;(no&nbsp;specification,&nbsp;which&nbsp;allows&nbsp;parallel&nbsp;tool&nbsp;use).<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Any&nbsp;additional&nbsp;parameters&nbsp;are&nbsp;passed&nbsp;directly&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:meth:`~langchain_openai.chat_models.base.<a href="#ChatOpenAI">ChatOpenAI</a>.bind`.<br>
&nbsp;<br>
..&nbsp;versionchanged::&nbsp;0.1.21<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Support&nbsp;for&nbsp;``strict``&nbsp;argument&nbsp;added.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_num_tokens_from_messages"><strong>get_num_tokens_from_messages</strong></a>(self, messages: 'List[BaseMessage]', tools: 'Optional[Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]]]' = None) -&gt; 'int'</dt><dd><tt>Calculate&nbsp;num&nbsp;tokens&nbsp;for&nbsp;gpt-3.5-turbo&nbsp;and&nbsp;gpt-4&nbsp;with&nbsp;tiktoken&nbsp;package.<br>
&nbsp;<br>
**Requirements**:&nbsp;You&nbsp;must&nbsp;have&nbsp;the&nbsp;``pillow``&nbsp;installed&nbsp;if&nbsp;you&nbsp;want&nbsp;to&nbsp;count<br>
image&nbsp;tokens&nbsp;if&nbsp;you&nbsp;are&nbsp;specifying&nbsp;the&nbsp;image&nbsp;as&nbsp;a&nbsp;base64&nbsp;string,&nbsp;and&nbsp;you&nbsp;must<br>
have&nbsp;both&nbsp;``pillow``&nbsp;and&nbsp;``httpx``&nbsp;installed&nbsp;if&nbsp;you&nbsp;are&nbsp;specifying&nbsp;the&nbsp;image<br>
as&nbsp;a&nbsp;URL.&nbsp;If&nbsp;these&nbsp;aren't&nbsp;installed&nbsp;image&nbsp;inputs&nbsp;will&nbsp;be&nbsp;ignored&nbsp;in&nbsp;token<br>
counting.<br>
&nbsp;<br>
<a href="#OpenAI">OpenAI</a>&nbsp;reference:&nbsp;<a href="https://github.com/openai/openai-cookbook/blob/">https://github.com/openai/openai-cookbook/blob/</a><br>
main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;messages:&nbsp;The&nbsp;message&nbsp;inputs&nbsp;to&nbsp;tokenize.<br>
&nbsp;&nbsp;&nbsp;&nbsp;tools:&nbsp;If&nbsp;provided,&nbsp;sequence&nbsp;of&nbsp;dict,&nbsp;BaseModel,&nbsp;function,&nbsp;or&nbsp;BaseTools<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;be&nbsp;converted&nbsp;to&nbsp;tool&nbsp;schemas.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_token_ids"><strong>get_token_ids</strong></a>(self, text: 'str') -&gt; 'List[int]'</dt><dd><tt>Get&nbsp;the&nbsp;tokens&nbsp;present&nbsp;in&nbsp;the&nbsp;text&nbsp;with&nbsp;tiktoken&nbsp;package.</tt></dd></dl>

<hr>
Class methods inherited from <a href="langchain_openai.chat_models.base.html#BaseChatOpenAI">langchain_openai.chat_models.base.BaseChatOpenAI</a>:<br>
<dl><dt><a name="ChatOpenAI-build_extra"><strong>build_extra</strong></a>(values: 'Dict[str, Any]') -&gt; 'Any'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Build&nbsp;extra&nbsp;kwargs&nbsp;from&nbsp;additional&nbsp;params&nbsp;that&nbsp;were&nbsp;passed&nbsp;in.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-validate_temperature"><strong>validate_temperature</strong></a>(values: 'Dict[str, Any]') -&gt; 'Any'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Currently&nbsp;o1&nbsp;models&nbsp;only&nbsp;allow&nbsp;temperature=1.</tt></dd></dl>

<hr>
Methods inherited from <a href="langchain_core.language_models.chat_models.html#BaseChatModel">langchain_core.language_models.chat_models.BaseChatModel</a>:<br>
<dl><dt><a name="ChatOpenAI-__call__"><strong>__call__</strong></a>(self, messages: 'list[BaseMessage]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>..&nbsp;deprecated::&nbsp;0.1.7&nbsp;Use&nbsp;:meth:`~invoke`&nbsp;instead.&nbsp;It&nbsp;will&nbsp;not&nbsp;be&nbsp;removed&nbsp;until&nbsp;langchain-core==1.0.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-agenerate"><strong>agenerate</strong></a>(self, messages: 'list[list[BaseMessage]]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[list[str]]' = None, metadata: 'Optional[dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Asynchronously&nbsp;pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;to&nbsp;a&nbsp;model&nbsp;and&nbsp;return&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;messages:&nbsp;List&nbsp;of&nbsp;list&nbsp;of&nbsp;messages.<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-agenerate_prompt"><strong>agenerate_prompt</strong></a>(self, prompts: 'list[PromptValue]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Asynchronously&nbsp;pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;and&nbsp;return&nbsp;model&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompts:&nbsp;List&nbsp;of&nbsp;PromptValues.&nbsp;A&nbsp;PromptValue&nbsp;is&nbsp;an&nbsp;object&nbsp;that&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;converted&nbsp;to&nbsp;match&nbsp;the&nbsp;format&nbsp;of&nbsp;any&nbsp;language&nbsp;model&nbsp;(string&nbsp;for&nbsp;pure<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text&nbsp;generation&nbsp;models&nbsp;and&nbsp;BaseMessages&nbsp;for&nbsp;chat&nbsp;models).<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-ainvoke"><strong>ainvoke</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;ainvoke,&nbsp;calls&nbsp;invoke&nbsp;from&nbsp;a&nbsp;thread.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;allows&nbsp;usage&nbsp;of&nbsp;async&nbsp;code&nbsp;even&nbsp;if<br>
the&nbsp;Runnable&nbsp;did&nbsp;not&nbsp;implement&nbsp;a&nbsp;native&nbsp;async&nbsp;version&nbsp;of&nbsp;invoke.<br>
&nbsp;<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;run&nbsp;asynchronously.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-apredict"><strong>apredict</strong></a>(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>..&nbsp;deprecated::&nbsp;0.1.7&nbsp;Use&nbsp;:meth:`~ainvoke`&nbsp;instead.&nbsp;It&nbsp;will&nbsp;not&nbsp;be&nbsp;removed&nbsp;until&nbsp;langchain-core==1.0.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-apredict_messages"><strong>apredict_messages</strong></a>(self, messages: 'list[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>..&nbsp;deprecated::&nbsp;0.1.7&nbsp;Use&nbsp;:meth:`~ainvoke`&nbsp;instead.&nbsp;It&nbsp;will&nbsp;not&nbsp;be&nbsp;removed&nbsp;until&nbsp;langchain-core==1.0.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-astream"><strong>astream</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -&gt; 'AsyncIterator[BaseMessageChunk]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;astream,&nbsp;which&nbsp;calls&nbsp;ainvoke.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;support&nbsp;streaming&nbsp;output.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-call_as_llm"><strong>call_as_llm</strong></a>(self, message: 'str', stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>..&nbsp;deprecated::&nbsp;0.1.7&nbsp;Use&nbsp;:meth:`~invoke`&nbsp;instead.&nbsp;It&nbsp;will&nbsp;not&nbsp;be&nbsp;removed&nbsp;until&nbsp;langchain-core==1.0.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-dict"><strong>dict</strong></a>(self, **kwargs: 'Any') -&gt; 'dict'</dt><dd><tt>Return&nbsp;a&nbsp;dictionary&nbsp;of&nbsp;the&nbsp;LLM.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-generate"><strong>generate</strong></a>(self, messages: 'list[list[BaseMessage]]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[list[str]]' = None, metadata: 'Optional[dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;to&nbsp;the&nbsp;model&nbsp;and&nbsp;return&nbsp;model&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;messages:&nbsp;List&nbsp;of&nbsp;list&nbsp;of&nbsp;messages.<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-generate_prompt"><strong>generate_prompt</strong></a>(self, prompts: 'list[PromptValue]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;to&nbsp;the&nbsp;model&nbsp;and&nbsp;return&nbsp;model&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompts:&nbsp;List&nbsp;of&nbsp;PromptValues.&nbsp;A&nbsp;PromptValue&nbsp;is&nbsp;an&nbsp;object&nbsp;that&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;converted&nbsp;to&nbsp;match&nbsp;the&nbsp;format&nbsp;of&nbsp;any&nbsp;language&nbsp;model&nbsp;(string&nbsp;for&nbsp;pure<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text&nbsp;generation&nbsp;models&nbsp;and&nbsp;BaseMessages&nbsp;for&nbsp;chat&nbsp;models).<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-invoke"><strong>invoke</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>Transform&nbsp;a&nbsp;single&nbsp;input&nbsp;into&nbsp;an&nbsp;output.&nbsp;Override&nbsp;to&nbsp;implement.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;invoking&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;config&nbsp;supports&nbsp;standard&nbsp;keys&nbsp;like&nbsp;'tags',&nbsp;'metadata'&nbsp;for&nbsp;tracing<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;purposes,&nbsp;'max_concurrency'&nbsp;for&nbsp;controlling&nbsp;how&nbsp;much&nbsp;work&nbsp;to&nbsp;do<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;parallel,&nbsp;and&nbsp;other&nbsp;keys.&nbsp;Please&nbsp;refer&nbsp;to&nbsp;the&nbsp;RunnableConfig<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;more&nbsp;details.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-predict"><strong>predict</strong></a>(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>..&nbsp;deprecated::&nbsp;0.1.7&nbsp;Use&nbsp;:meth:`~invoke`&nbsp;instead.&nbsp;It&nbsp;will&nbsp;not&nbsp;be&nbsp;removed&nbsp;until&nbsp;langchain-core==1.0.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-predict_messages"><strong>predict_messages</strong></a>(self, messages: 'list[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>..&nbsp;deprecated::&nbsp;0.1.7&nbsp;Use&nbsp;:meth:`~invoke`&nbsp;instead.&nbsp;It&nbsp;will&nbsp;not&nbsp;be&nbsp;removed&nbsp;until&nbsp;langchain-core==1.0.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-stream"><strong>stream</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -&gt; 'Iterator[BaseMessageChunk]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;stream,&nbsp;which&nbsp;calls&nbsp;invoke.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;support&nbsp;streaming&nbsp;output.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<hr>
Class methods inherited from <a href="langchain_core.language_models.chat_models.html#BaseChatModel">langchain_core.language_models.chat_models.BaseChatModel</a>:<br>
<dl><dt><a name="ChatOpenAI-raise_deprecation"><strong>raise_deprecation</strong></a>(values: 'dict') -&gt; 'Any'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Raise&nbsp;deprecation&nbsp;warning&nbsp;if&nbsp;callback_manager&nbsp;is&nbsp;used.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;values&nbsp;(Dict):&nbsp;Values&nbsp;to&nbsp;validate.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Dict:&nbsp;Validated&nbsp;values.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;DeprecationWarning:&nbsp;If&nbsp;callback_manager&nbsp;is&nbsp;used.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_core.language_models.chat_models.html#BaseChatModel">langchain_core.language_models.chat_models.BaseChatModel</a>:<br>
<dl><dt><strong>OutputType</strong></dt>
<dd><tt>Get&nbsp;the&nbsp;output&nbsp;type&nbsp;for&nbsp;this&nbsp;runnable.</tt></dd>
</dl>
<hr>
Methods inherited from <a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a>:<br>
<dl><dt><a name="ChatOpenAI-get_num_tokens"><strong>get_num_tokens</strong></a>(self, text: 'str') -&gt; 'int'</dt><dd><tt>Get&nbsp;the&nbsp;number&nbsp;of&nbsp;tokens&nbsp;present&nbsp;in&nbsp;the&nbsp;text.<br>
&nbsp;<br>
Useful&nbsp;for&nbsp;checking&nbsp;if&nbsp;an&nbsp;input&nbsp;fits&nbsp;in&nbsp;a&nbsp;model's&nbsp;context&nbsp;window.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;text:&nbsp;The&nbsp;string&nbsp;input&nbsp;to&nbsp;tokenize.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;integer&nbsp;number&nbsp;of&nbsp;tokens&nbsp;in&nbsp;the&nbsp;text.</tt></dd></dl>

<hr>
Class methods inherited from <a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a>:<br>
<dl><dt><a name="ChatOpenAI-set_verbose"><strong>set_verbose</strong></a>(verbose: 'Optional[bool]') -&gt; 'bool'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>If&nbsp;verbose&nbsp;is&nbsp;None,&nbsp;set&nbsp;it.<br>
&nbsp;<br>
This&nbsp;allows&nbsp;users&nbsp;to&nbsp;pass&nbsp;in&nbsp;None&nbsp;as&nbsp;verbose&nbsp;to&nbsp;access&nbsp;the&nbsp;global&nbsp;setting.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;verbose:&nbsp;The&nbsp;verbosity&nbsp;setting&nbsp;to&nbsp;use.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;verbosity&nbsp;setting&nbsp;to&nbsp;use.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a>:<br>
<dl><dt><strong>InputType</strong></dt>
<dd><tt>Get&nbsp;the&nbsp;input&nbsp;type&nbsp;for&nbsp;this&nbsp;runnable.</tt></dd>
</dl>
<hr>
Methods inherited from <a href="langchain_core.runnables.base.html#RunnableSerializable">langchain_core.runnables.base.RunnableSerializable</a>:<br>
<dl><dt><a name="ChatOpenAI-configurable_alternatives"><strong>configurable_alternatives</strong></a>(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -&gt; 'RunnableSerializable[Input, Output]'</dt><dd><tt>Configure&nbsp;alternatives&nbsp;for&nbsp;Runnables&nbsp;that&nbsp;can&nbsp;be&nbsp;set&nbsp;at&nbsp;runtime.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;which:&nbsp;The&nbsp;ConfigurableField&nbsp;instance&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;to&nbsp;select&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;alternative.<br>
&nbsp;&nbsp;&nbsp;&nbsp;default_key:&nbsp;The&nbsp;default&nbsp;key&nbsp;to&nbsp;use&nbsp;if&nbsp;no&nbsp;alternative&nbsp;is&nbsp;selected.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;"default".<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix_keys:&nbsp;Whether&nbsp;to&nbsp;prefix&nbsp;the&nbsp;keys&nbsp;with&nbsp;the&nbsp;ConfigurableField&nbsp;id.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;False.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;A&nbsp;dictionary&nbsp;of&nbsp;keys&nbsp;to&nbsp;Runnable&nbsp;instances&nbsp;or&nbsp;callables&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;Runnable&nbsp;instances.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;alternatives&nbsp;configured.<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_anthropic&nbsp;import&nbsp;ChatAnthropic<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables.utils&nbsp;import&nbsp;ConfigurableField<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_openai&nbsp;import&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a><br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;ChatAnthropic(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model_name="claude-3-sonnet-20240229"<br>
&nbsp;&nbsp;&nbsp;&nbsp;).<a href="#ChatOpenAI-configurable_alternatives">configurable_alternatives</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ConfigurableField(id="llm"),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;default_key="anthropic",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;openai=<a href="#ChatOpenAI">ChatOpenAI</a>()<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;uses&nbsp;the&nbsp;default&nbsp;model&nbsp;ChatAnthropic<br>
&nbsp;&nbsp;&nbsp;&nbsp;print(model.<a href="#ChatOpenAI-invoke">invoke</a>("which&nbsp;organization&nbsp;created&nbsp;you?").content)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;uses&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a><br>
&nbsp;&nbsp;&nbsp;&nbsp;print(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model.<a href="#ChatOpenAI-with_config">with_config</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;configurable={"llm":&nbsp;"openai"}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;).<a href="#ChatOpenAI-invoke">invoke</a>("which&nbsp;organization&nbsp;created&nbsp;you?").content<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-configurable_fields"><strong>configurable_fields</strong></a>(self, **kwargs: 'AnyConfigurableField') -&gt; 'RunnableSerializable[Input, Output]'</dt><dd><tt>Configure&nbsp;particular&nbsp;Runnable&nbsp;fields&nbsp;at&nbsp;runtime.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;A&nbsp;dictionary&nbsp;of&nbsp;ConfigurableField&nbsp;instances&nbsp;to&nbsp;configure.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;fields&nbsp;configured.<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;ConfigurableField<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_openai&nbsp;import&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a><br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a>(max_tokens=20).<a href="#ChatOpenAI-configurable_fields">configurable_fields</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_tokens=ConfigurableField(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;id="output_token_number",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name="Max&nbsp;tokens&nbsp;in&nbsp;the&nbsp;output",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;description="The&nbsp;maximum&nbsp;number&nbsp;of&nbsp;tokens&nbsp;in&nbsp;the&nbsp;output",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;max_tokens&nbsp;=&nbsp;20<br>
&nbsp;&nbsp;&nbsp;&nbsp;print(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"max_tokens_20:&nbsp;",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model.<a href="#ChatOpenAI-invoke">invoke</a>("tell&nbsp;me&nbsp;something&nbsp;about&nbsp;chess").content<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;max_tokens&nbsp;=&nbsp;200<br>
&nbsp;&nbsp;&nbsp;&nbsp;print("max_tokens_200:&nbsp;",&nbsp;model.<a href="#ChatOpenAI-with_config">with_config</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;configurable={"output_token_number":&nbsp;200}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;).<a href="#ChatOpenAI-invoke">invoke</a>("tell&nbsp;me&nbsp;something&nbsp;about&nbsp;chess").content<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-to_json"><strong>to_json</strong></a>(self) -&gt; 'Union[SerializedConstructor, SerializedNotImplemented]'</dt><dd><tt>Serialize&nbsp;the&nbsp;Runnable&nbsp;to&nbsp;JSON.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;JSON-serializable&nbsp;representation&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<hr>
Data and other attributes inherited from <a href="langchain_core.runnables.base.html#RunnableSerializable">langchain_core.runnables.base.RunnableSerializable</a>:<br>
<dl><dt><strong>__orig_bases__</strong> = (&lt;class 'langchain_core.load.serializable.Serializable'&gt;, langchain_core.runnables.base.Runnable[-Input, +Output])</dl>

<hr>
Methods inherited from <a href="langchain_core.load.serializable.html#Serializable">langchain_core.load.serializable.Serializable</a>:<br>
<dl><dt><a name="ChatOpenAI-__repr_args__"><strong>__repr_args__</strong></a>(self) -&gt; Any</dt></dl>

<dl><dt><a name="ChatOpenAI-to_json_not_implemented"><strong>to_json_not_implemented</strong></a>(self) -&gt; langchain_core.load.serializable.SerializedNotImplemented</dt></dl>

<hr>
Class methods inherited from <a href="langchain_core.load.serializable.html#Serializable">langchain_core.load.serializable.Serializable</a>:<br>
<dl><dt><a name="ChatOpenAI-lc_id"><strong>lc_id</strong></a>() -&gt; list[str]<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>A&nbsp;unique&nbsp;identifier&nbsp;for&nbsp;this&nbsp;class&nbsp;for&nbsp;serialization&nbsp;purposes.<br>
&nbsp;<br>
The&nbsp;unique&nbsp;identifier&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;strings&nbsp;that&nbsp;describes&nbsp;the&nbsp;path<br>
to&nbsp;the&nbsp;object.<br>
For&nbsp;example,&nbsp;for&nbsp;the&nbsp;class&nbsp;`langchain.llms.openai.<a href="#OpenAI">OpenAI</a>`,&nbsp;the&nbsp;id&nbsp;is<br>
["langchain",&nbsp;"llms",&nbsp;"openai",&nbsp;"<a href="#OpenAI">OpenAI</a>"].</tt></dd></dl>

<hr>
Methods inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><a name="ChatOpenAI-__copy__"><strong>__copy__</strong></a>(self) -&gt; 'Self'</dt><dd><tt>Returns&nbsp;a&nbsp;shallow&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__deepcopy__"><strong>__deepcopy__</strong></a>(self, memo: 'dict[int, Any] | None' = None) -&gt; 'Self'</dt><dd><tt>Returns&nbsp;a&nbsp;deep&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__delattr__"><strong>__delattr__</strong></a>(self, item: 'str') -&gt; 'Any'</dt><dd><tt>Implement&nbsp;delattr(self,&nbsp;name).</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__eq__"><strong>__eq__</strong></a>(self, other: 'Any') -&gt; 'bool'</dt><dd><tt>Return&nbsp;self==value.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__getattr__"><strong>__getattr__</strong></a>(self, item: 'str') -&gt; 'Any'</dt></dl>

<dl><dt><a name="ChatOpenAI-__getstate__"><strong>__getstate__</strong></a>(self) -&gt; 'dict[Any, Any]'</dt></dl>

<dl><dt><a name="ChatOpenAI-__iter__"><strong>__iter__</strong></a>(self) -&gt; 'TupleGenerator'</dt><dd><tt>So&nbsp;`<a href="#ChatOpenAI-dict">dict</a>(model)`&nbsp;works.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__pretty__"><strong>__pretty__</strong></a>(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -&gt; 'typing.Generator[Any, None, None]'</dt><dd><tt>Used&nbsp;by&nbsp;devtools&nbsp;(<a href="https://python-devtools.helpmanual.io/">https://python-devtools.helpmanual.io/</a>)&nbsp;to&nbsp;pretty&nbsp;print&nbsp;objects.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__replace__"><strong>__replace__</strong></a>(self, **changes: 'Any') -&gt; 'Self'</dt><dd><tt>#&nbsp;Because&nbsp;we&nbsp;make&nbsp;use&nbsp;of&nbsp;`@dataclass_transform()`,&nbsp;`__replace__`&nbsp;is&nbsp;already&nbsp;synthesized&nbsp;by<br>
#&nbsp;type&nbsp;checkers,&nbsp;so&nbsp;we&nbsp;define&nbsp;the&nbsp;implementation&nbsp;in&nbsp;this&nbsp;`if&nbsp;not&nbsp;TYPE_CHECKING:`&nbsp;block:</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__repr__"><strong>__repr__</strong></a>(self) -&gt; 'str'</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__repr_name__"><strong>__repr_name__</strong></a>(self) -&gt; 'str'</dt><dd><tt>Name&nbsp;of&nbsp;the&nbsp;instance's&nbsp;class,&nbsp;used&nbsp;in&nbsp;__repr__.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__repr_recursion__"><strong>__repr_recursion__</strong></a>(self, object: 'Any') -&gt; 'str'</dt><dd><tt>Returns&nbsp;the&nbsp;string&nbsp;representation&nbsp;of&nbsp;a&nbsp;recursive&nbsp;object.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__repr_str__"><strong>__repr_str__</strong></a>(self, join_str: 'str') -&gt; 'str'</dt></dl>

<dl><dt><a name="ChatOpenAI-__rich_repr__"><strong>__rich_repr__</strong></a>(self) -&gt; 'RichReprResult'</dt><dd><tt>Used&nbsp;by&nbsp;Rich&nbsp;(<a href="https://rich.readthedocs.io/en/stable/pretty.html">https://rich.readthedocs.io/en/stable/pretty.html</a>)&nbsp;to&nbsp;pretty&nbsp;print&nbsp;objects.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__setattr__"><strong>__setattr__</strong></a>(self, name: 'str', value: 'Any') -&gt; 'None'</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__setstate__"><strong>__setstate__</strong></a>(self, state: 'dict[Any, Any]') -&gt; 'None'</dt></dl>

<dl><dt><a name="ChatOpenAI-__str__"><strong>__str__</strong></a>(self) -&gt; 'str'</dt><dd><tt>Return&nbsp;str(self).</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-copy"><strong>copy</strong></a>(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -&gt; 'Self'</dt><dd><tt>Returns&nbsp;a&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.<br>
&nbsp;<br>
!!!&nbsp;warning&nbsp;"Deprecated"<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;is&nbsp;now&nbsp;deprecated;&nbsp;use&nbsp;`model_copy`&nbsp;instead.<br>
&nbsp;<br>
If&nbsp;you&nbsp;need&nbsp;`include`&nbsp;or&nbsp;`exclude`,&nbsp;use:<br>
&nbsp;<br>
```python&nbsp;{test="skip"&nbsp;lint="skip"}<br>
data&nbsp;=&nbsp;self.<a href="#ChatOpenAI-model_dump">model_dump</a>(include=include,&nbsp;exclude=exclude,&nbsp;round_trip=True)<br>
data&nbsp;=&nbsp;{**data,&nbsp;**(update&nbsp;or&nbsp;{})}<br>
copied&nbsp;=&nbsp;self.<a href="#ChatOpenAI-model_validate">model_validate</a>(data)<br>
```<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;Optional&nbsp;set&nbsp;or&nbsp;mapping&nbsp;specifying&nbsp;which&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;copied&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude:&nbsp;Optional&nbsp;set&nbsp;or&nbsp;mapping&nbsp;specifying&nbsp;which&nbsp;fields&nbsp;to&nbsp;exclude&nbsp;in&nbsp;the&nbsp;copied&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;update:&nbsp;Optional&nbsp;dictionary&nbsp;of&nbsp;field-value&nbsp;pairs&nbsp;to&nbsp;override&nbsp;field&nbsp;values&nbsp;in&nbsp;the&nbsp;copied&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;deep:&nbsp;If&nbsp;True,&nbsp;the&nbsp;values&nbsp;of&nbsp;fields&nbsp;that&nbsp;are&nbsp;Pydantic&nbsp;models&nbsp;will&nbsp;be&nbsp;deep-copied.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;copy&nbsp;of&nbsp;the&nbsp;model&nbsp;with&nbsp;included,&nbsp;excluded&nbsp;and&nbsp;updated&nbsp;fields&nbsp;as&nbsp;specified.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-json"><strong>json</strong></a>(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -&gt; 'str'</dt></dl>

<dl><dt><a name="ChatOpenAI-model_copy"><strong>model_copy</strong></a>(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -&gt; 'Self'</dt><dd><tt>Usage&nbsp;docs:&nbsp;<a href="https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy">https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy</a><br>
&nbsp;<br>
Returns&nbsp;a&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;update:&nbsp;Values&nbsp;to&nbsp;change/add&nbsp;in&nbsp;the&nbsp;new&nbsp;model.&nbsp;Note:&nbsp;the&nbsp;data&nbsp;is&nbsp;not&nbsp;validated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;before&nbsp;creating&nbsp;the&nbsp;new&nbsp;model.&nbsp;You&nbsp;should&nbsp;trust&nbsp;this&nbsp;data.<br>
&nbsp;&nbsp;&nbsp;&nbsp;deep:&nbsp;Set&nbsp;to&nbsp;`True`&nbsp;to&nbsp;make&nbsp;a&nbsp;deep&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;New&nbsp;model&nbsp;instance.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-model_dump"><strong>model_dump</strong></a>(self, *, mode: "Literal['json', 'python'] | str" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: "bool | Literal['none', 'warn', 'error']" = True, serialize_as_any: 'bool' = False) -&gt; 'dict[str, Any]'</dt><dd><tt>Usage&nbsp;docs:&nbsp;<a href="https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump">https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump</a><br>
&nbsp;<br>
Generate&nbsp;a&nbsp;dictionary&nbsp;representation&nbsp;of&nbsp;the&nbsp;model,&nbsp;optionally&nbsp;specifying&nbsp;which&nbsp;fields&nbsp;to&nbsp;include&nbsp;or&nbsp;exclude.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode:&nbsp;The&nbsp;mode&nbsp;in&nbsp;which&nbsp;`to_python`&nbsp;should&nbsp;run.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;mode&nbsp;is&nbsp;'json',&nbsp;the&nbsp;output&nbsp;will&nbsp;only&nbsp;contain&nbsp;JSON&nbsp;serializable&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;mode&nbsp;is&nbsp;'python',&nbsp;the&nbsp;output&nbsp;may&nbsp;contain&nbsp;non-JSON-serializable&nbsp;Python&nbsp;objects.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;A&nbsp;set&nbsp;of&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;output.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude:&nbsp;A&nbsp;set&nbsp;of&nbsp;fields&nbsp;to&nbsp;exclude&nbsp;from&nbsp;the&nbsp;output.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Additional&nbsp;context&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;serializer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;by_alias:&nbsp;Whether&nbsp;to&nbsp;use&nbsp;the&nbsp;field's&nbsp;alias&nbsp;in&nbsp;the&nbsp;dictionary&nbsp;key&nbsp;if&nbsp;defined.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_unset:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;have&nbsp;not&nbsp;been&nbsp;explicitly&nbsp;set.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_defaults:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;are&nbsp;set&nbsp;to&nbsp;their&nbsp;default&nbsp;value.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_none:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;have&nbsp;a&nbsp;value&nbsp;of&nbsp;`None`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;round_trip:&nbsp;If&nbsp;True,&nbsp;dumped&nbsp;values&nbsp;should&nbsp;be&nbsp;valid&nbsp;as&nbsp;input&nbsp;for&nbsp;non-idempotent&nbsp;types&nbsp;such&nbsp;as&nbsp;Json[T].<br>
&nbsp;&nbsp;&nbsp;&nbsp;warnings:&nbsp;How&nbsp;to&nbsp;handle&nbsp;serialization&nbsp;errors.&nbsp;False/"none"&nbsp;ignores&nbsp;them,&nbsp;True/"warn"&nbsp;logs&nbsp;errors,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"error"&nbsp;raises&nbsp;a&nbsp;[`PydanticSerializationError`][pydantic_core.PydanticSerializationError].<br>
&nbsp;&nbsp;&nbsp;&nbsp;serialize_as_any:&nbsp;Whether&nbsp;to&nbsp;serialize&nbsp;fields&nbsp;with&nbsp;duck-typing&nbsp;serialization&nbsp;behavior.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;dictionary&nbsp;representation&nbsp;of&nbsp;the&nbsp;model.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-model_dump_json"><strong>model_dump_json</strong></a>(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: "bool | Literal['none', 'warn', 'error']" = True, serialize_as_any: 'bool' = False) -&gt; 'str'</dt><dd><tt>Usage&nbsp;docs:&nbsp;<a href="https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json">https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json</a><br>
&nbsp;<br>
Generates&nbsp;a&nbsp;JSON&nbsp;representation&nbsp;of&nbsp;the&nbsp;model&nbsp;using&nbsp;Pydantic's&nbsp;`to_json`&nbsp;method.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;indent:&nbsp;Indentation&nbsp;to&nbsp;use&nbsp;in&nbsp;the&nbsp;JSON&nbsp;output.&nbsp;If&nbsp;None&nbsp;is&nbsp;passed,&nbsp;the&nbsp;output&nbsp;will&nbsp;be&nbsp;compact.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;Field(s)&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;JSON&nbsp;output.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude:&nbsp;Field(s)&nbsp;to&nbsp;exclude&nbsp;from&nbsp;the&nbsp;JSON&nbsp;output.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Additional&nbsp;context&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;serializer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;by_alias:&nbsp;Whether&nbsp;to&nbsp;serialize&nbsp;using&nbsp;field&nbsp;aliases.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_unset:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;have&nbsp;not&nbsp;been&nbsp;explicitly&nbsp;set.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_defaults:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;are&nbsp;set&nbsp;to&nbsp;their&nbsp;default&nbsp;value.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_none:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;have&nbsp;a&nbsp;value&nbsp;of&nbsp;`None`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;round_trip:&nbsp;If&nbsp;True,&nbsp;dumped&nbsp;values&nbsp;should&nbsp;be&nbsp;valid&nbsp;as&nbsp;input&nbsp;for&nbsp;non-idempotent&nbsp;types&nbsp;such&nbsp;as&nbsp;Json[T].<br>
&nbsp;&nbsp;&nbsp;&nbsp;warnings:&nbsp;How&nbsp;to&nbsp;handle&nbsp;serialization&nbsp;errors.&nbsp;False/"none"&nbsp;ignores&nbsp;them,&nbsp;True/"warn"&nbsp;logs&nbsp;errors,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"error"&nbsp;raises&nbsp;a&nbsp;[`PydanticSerializationError`][pydantic_core.PydanticSerializationError].<br>
&nbsp;&nbsp;&nbsp;&nbsp;serialize_as_any:&nbsp;Whether&nbsp;to&nbsp;serialize&nbsp;fields&nbsp;with&nbsp;duck-typing&nbsp;serialization&nbsp;behavior.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;JSON&nbsp;string&nbsp;representation&nbsp;of&nbsp;the&nbsp;model.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-model_post_init"><strong>model_post_init</strong></a>(self, _BaseModel__context: 'Any') -&gt; 'None'</dt><dd><tt>Override&nbsp;this&nbsp;method&nbsp;to&nbsp;perform&nbsp;additional&nbsp;initialization&nbsp;after&nbsp;`__init__`&nbsp;and&nbsp;`model_construct`.<br>
This&nbsp;is&nbsp;useful&nbsp;if&nbsp;you&nbsp;want&nbsp;to&nbsp;do&nbsp;some&nbsp;validation&nbsp;that&nbsp;requires&nbsp;the&nbsp;entire&nbsp;model&nbsp;to&nbsp;be&nbsp;initialized.</tt></dd></dl>

<hr>
Class methods inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><a name="ChatOpenAI-__class_getitem__"><strong>__class_getitem__</strong></a>(typevar_values: 'type[Any] | tuple[type[Any], ...]') -&gt; 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatOpenAI-__get_pydantic_core_schema__"><strong>__get_pydantic_core_schema__</strong></a>(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -&gt; 'CoreSchema'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Hook&nbsp;into&nbsp;generating&nbsp;the&nbsp;model's&nbsp;CoreSchema.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;source:&nbsp;The&nbsp;class&nbsp;we&nbsp;are&nbsp;generating&nbsp;a&nbsp;schema&nbsp;for.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;will&nbsp;generally&nbsp;be&nbsp;the&nbsp;same&nbsp;as&nbsp;the&nbsp;`cls`&nbsp;argument&nbsp;if&nbsp;this&nbsp;is&nbsp;a&nbsp;classmethod.<br>
&nbsp;&nbsp;&nbsp;&nbsp;handler:&nbsp;A&nbsp;callable&nbsp;that&nbsp;calls&nbsp;into&nbsp;Pydantic's&nbsp;internal&nbsp;CoreSchema&nbsp;generation&nbsp;logic.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;`pydantic-core`&nbsp;`CoreSchema`.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__get_pydantic_json_schema__"><strong>__get_pydantic_json_schema__</strong></a>(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -&gt; 'JsonSchemaValue'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Hook&nbsp;into&nbsp;generating&nbsp;the&nbsp;model's&nbsp;JSON&nbsp;schema.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;core_schema:&nbsp;A&nbsp;`pydantic-core`&nbsp;CoreSchema.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You&nbsp;can&nbsp;ignore&nbsp;this&nbsp;argument&nbsp;and&nbsp;call&nbsp;the&nbsp;handler&nbsp;with&nbsp;a&nbsp;new&nbsp;CoreSchema,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;wrap&nbsp;this&nbsp;CoreSchema&nbsp;(`{'type':&nbsp;'nullable',&nbsp;'schema':&nbsp;current_schema}`),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;just&nbsp;call&nbsp;the&nbsp;handler&nbsp;with&nbsp;the&nbsp;original&nbsp;schema.<br>
&nbsp;&nbsp;&nbsp;&nbsp;handler:&nbsp;Call&nbsp;into&nbsp;Pydantic's&nbsp;internal&nbsp;JSON&nbsp;schema&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;will&nbsp;raise&nbsp;a&nbsp;`pydantic.errors.PydanticInvalidForJsonSchema`&nbsp;if&nbsp;JSON&nbsp;schema<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;generation&nbsp;fails.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Since&nbsp;this&nbsp;gets&nbsp;called&nbsp;by&nbsp;`BaseModel.model_json_schema`&nbsp;you&nbsp;can&nbsp;override&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`schema_generator`&nbsp;argument&nbsp;to&nbsp;that&nbsp;function&nbsp;to&nbsp;change&nbsp;JSON&nbsp;schema&nbsp;generation&nbsp;globally<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;a&nbsp;type.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;JSON&nbsp;schema,&nbsp;as&nbsp;a&nbsp;Python&nbsp;object.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__pydantic_init_subclass__"><strong>__pydantic_init_subclass__</strong></a>(**kwargs: 'Any') -&gt; 'None'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>This&nbsp;is&nbsp;intended&nbsp;to&nbsp;behave&nbsp;just&nbsp;like&nbsp;`__init_subclass__`,&nbsp;but&nbsp;is&nbsp;called&nbsp;by&nbsp;`ModelMetaclass`<br>
only&nbsp;after&nbsp;the&nbsp;class&nbsp;is&nbsp;actually&nbsp;fully&nbsp;initialized.&nbsp;In&nbsp;particular,&nbsp;attributes&nbsp;like&nbsp;`model_fields`&nbsp;will<br>
be&nbsp;present&nbsp;when&nbsp;this&nbsp;is&nbsp;called.<br>
&nbsp;<br>
This&nbsp;is&nbsp;necessary&nbsp;because&nbsp;`__init_subclass__`&nbsp;will&nbsp;always&nbsp;be&nbsp;called&nbsp;by&nbsp;`type.__new__`,<br>
and&nbsp;it&nbsp;would&nbsp;require&nbsp;a&nbsp;prohibitively&nbsp;large&nbsp;refactor&nbsp;to&nbsp;the&nbsp;`ModelMetaclass`&nbsp;to&nbsp;ensure&nbsp;that<br>
`type.__new__`&nbsp;was&nbsp;called&nbsp;in&nbsp;such&nbsp;a&nbsp;manner&nbsp;that&nbsp;the&nbsp;class&nbsp;would&nbsp;already&nbsp;be&nbsp;sufficiently&nbsp;initialized.<br>
&nbsp;<br>
This&nbsp;will&nbsp;receive&nbsp;the&nbsp;same&nbsp;`kwargs`&nbsp;that&nbsp;would&nbsp;be&nbsp;passed&nbsp;to&nbsp;the&nbsp;standard&nbsp;`__init_subclass__`,&nbsp;namely,<br>
any&nbsp;kwargs&nbsp;passed&nbsp;to&nbsp;the&nbsp;class&nbsp;definition&nbsp;that&nbsp;aren't&nbsp;used&nbsp;internally&nbsp;by&nbsp;pydantic.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Any&nbsp;keyword&nbsp;arguments&nbsp;passed&nbsp;to&nbsp;the&nbsp;class&nbsp;definition&nbsp;that&nbsp;aren't&nbsp;used&nbsp;internally<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;pydantic.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-construct"><strong>construct</strong></a>(_fields_set: 'set[str] | None' = None, **values: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatOpenAI-from_orm"><strong>from_orm</strong></a>(obj: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatOpenAI-model_construct"><strong>model_construct</strong></a>(_fields_set: 'set[str] | None' = None, **values: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Creates&nbsp;a&nbsp;new&nbsp;instance&nbsp;of&nbsp;the&nbsp;`Model`&nbsp;class&nbsp;with&nbsp;validated&nbsp;data.<br>
&nbsp;<br>
Creates&nbsp;a&nbsp;new&nbsp;model&nbsp;setting&nbsp;`__dict__`&nbsp;and&nbsp;`__pydantic_fields_set__`&nbsp;from&nbsp;trusted&nbsp;or&nbsp;pre-validated&nbsp;data.<br>
Default&nbsp;values&nbsp;are&nbsp;respected,&nbsp;but&nbsp;no&nbsp;other&nbsp;validation&nbsp;is&nbsp;performed.<br>
&nbsp;<br>
!!!&nbsp;note<br>
&nbsp;&nbsp;&nbsp;&nbsp;`<a href="#ChatOpenAI-model_construct">model_construct</a>()`&nbsp;generally&nbsp;respects&nbsp;the&nbsp;`model_config.extra`&nbsp;setting&nbsp;on&nbsp;the&nbsp;provided&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;That&nbsp;is,&nbsp;if&nbsp;`model_config.extra&nbsp;==&nbsp;'allow'`,&nbsp;then&nbsp;all&nbsp;extra&nbsp;passed&nbsp;values&nbsp;are&nbsp;added&nbsp;to&nbsp;the&nbsp;model&nbsp;instance's&nbsp;`__dict__`<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;`__pydantic_extra__`&nbsp;fields.&nbsp;If&nbsp;`model_config.extra&nbsp;==&nbsp;'ignore'`&nbsp;(the&nbsp;default),&nbsp;then&nbsp;all&nbsp;extra&nbsp;passed&nbsp;values&nbsp;are&nbsp;ignored.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Because&nbsp;no&nbsp;validation&nbsp;is&nbsp;performed&nbsp;with&nbsp;a&nbsp;call&nbsp;to&nbsp;`<a href="#ChatOpenAI-model_construct">model_construct</a>()`,&nbsp;having&nbsp;`model_config.extra&nbsp;==&nbsp;'forbid'`&nbsp;does&nbsp;not&nbsp;result&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;an&nbsp;error&nbsp;if&nbsp;extra&nbsp;values&nbsp;are&nbsp;passed,&nbsp;but&nbsp;they&nbsp;will&nbsp;be&nbsp;ignored.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;_fields_set:&nbsp;A&nbsp;set&nbsp;of&nbsp;field&nbsp;names&nbsp;that&nbsp;were&nbsp;originally&nbsp;explicitly&nbsp;set&nbsp;during&nbsp;instantiation.&nbsp;If&nbsp;provided,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;is&nbsp;directly&nbsp;used&nbsp;for&nbsp;the&nbsp;[`model_fields_set`][pydantic.BaseModel.model_fields_set]&nbsp;attribute.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Otherwise,&nbsp;the&nbsp;field&nbsp;names&nbsp;from&nbsp;the&nbsp;`values`&nbsp;argument&nbsp;will&nbsp;be&nbsp;used.<br>
&nbsp;&nbsp;&nbsp;&nbsp;values:&nbsp;Trusted&nbsp;or&nbsp;pre-validated&nbsp;data&nbsp;dictionary.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;instance&nbsp;of&nbsp;the&nbsp;`Model`&nbsp;class&nbsp;with&nbsp;validated&nbsp;data.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-model_json_schema"><strong>model_json_schema</strong></a>(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = &lt;class 'pydantic.json_schema.GenerateJsonSchema'&gt;, mode: 'JsonSchemaMode' = 'validation') -&gt; 'dict[str, Any]'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Generates&nbsp;a&nbsp;JSON&nbsp;schema&nbsp;for&nbsp;a&nbsp;model&nbsp;class.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;by_alias:&nbsp;Whether&nbsp;to&nbsp;use&nbsp;attribute&nbsp;aliases&nbsp;or&nbsp;not.<br>
&nbsp;&nbsp;&nbsp;&nbsp;ref_template:&nbsp;The&nbsp;reference&nbsp;template.<br>
&nbsp;&nbsp;&nbsp;&nbsp;schema_generator:&nbsp;To&nbsp;override&nbsp;the&nbsp;logic&nbsp;used&nbsp;to&nbsp;generate&nbsp;the&nbsp;JSON&nbsp;schema,&nbsp;as&nbsp;a&nbsp;subclass&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`GenerateJsonSchema`&nbsp;with&nbsp;your&nbsp;desired&nbsp;modifications<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode:&nbsp;The&nbsp;mode&nbsp;in&nbsp;which&nbsp;to&nbsp;generate&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;JSON&nbsp;schema&nbsp;for&nbsp;the&nbsp;given&nbsp;model&nbsp;class.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-model_parametrized_name"><strong>model_parametrized_name</strong></a>(params: 'tuple[type[Any], ...]') -&gt; 'str'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Compute&nbsp;the&nbsp;class&nbsp;name&nbsp;for&nbsp;parametrizations&nbsp;of&nbsp;generic&nbsp;classes.<br>
&nbsp;<br>
This&nbsp;method&nbsp;can&nbsp;be&nbsp;overridden&nbsp;to&nbsp;achieve&nbsp;a&nbsp;custom&nbsp;naming&nbsp;scheme&nbsp;for&nbsp;generic&nbsp;BaseModels.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;params:&nbsp;Tuple&nbsp;of&nbsp;types&nbsp;of&nbsp;the&nbsp;class.&nbsp;Given&nbsp;a&nbsp;generic&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`Model`&nbsp;with&nbsp;2&nbsp;type&nbsp;variables&nbsp;and&nbsp;a&nbsp;concrete&nbsp;model&nbsp;`Model[str,&nbsp;int]`,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;value&nbsp;`(str,&nbsp;int)`&nbsp;would&nbsp;be&nbsp;passed&nbsp;to&nbsp;`params`.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;String&nbsp;representing&nbsp;the&nbsp;new&nbsp;class&nbsp;where&nbsp;`params`&nbsp;are&nbsp;passed&nbsp;to&nbsp;`cls`&nbsp;as&nbsp;type&nbsp;variables.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;TypeError:&nbsp;Raised&nbsp;when&nbsp;trying&nbsp;to&nbsp;generate&nbsp;concrete&nbsp;names&nbsp;for&nbsp;non-generic&nbsp;models.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-model_rebuild"><strong>model_rebuild</strong></a>(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -&gt; 'bool | None'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Try&nbsp;to&nbsp;rebuild&nbsp;the&nbsp;pydantic-core&nbsp;schema&nbsp;for&nbsp;the&nbsp;model.<br>
&nbsp;<br>
This&nbsp;may&nbsp;be&nbsp;necessary&nbsp;when&nbsp;one&nbsp;of&nbsp;the&nbsp;annotations&nbsp;is&nbsp;a&nbsp;ForwardRef&nbsp;which&nbsp;could&nbsp;not&nbsp;be&nbsp;resolved&nbsp;during<br>
the&nbsp;initial&nbsp;attempt&nbsp;to&nbsp;build&nbsp;the&nbsp;schema,&nbsp;and&nbsp;automatic&nbsp;rebuilding&nbsp;fails.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;force:&nbsp;Whether&nbsp;to&nbsp;force&nbsp;the&nbsp;rebuilding&nbsp;of&nbsp;the&nbsp;model&nbsp;schema,&nbsp;defaults&nbsp;to&nbsp;`False`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;raise_errors:&nbsp;Whether&nbsp;to&nbsp;raise&nbsp;errors,&nbsp;defaults&nbsp;to&nbsp;`True`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;_parent_namespace_depth:&nbsp;The&nbsp;depth&nbsp;level&nbsp;of&nbsp;the&nbsp;parent&nbsp;namespace,&nbsp;defaults&nbsp;to&nbsp;2.<br>
&nbsp;&nbsp;&nbsp;&nbsp;_types_namespace:&nbsp;The&nbsp;types&nbsp;namespace,&nbsp;defaults&nbsp;to&nbsp;`None`.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Returns&nbsp;`None`&nbsp;if&nbsp;the&nbsp;schema&nbsp;is&nbsp;already&nbsp;"complete"&nbsp;and&nbsp;rebuilding&nbsp;was&nbsp;not&nbsp;required.<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;rebuilding&nbsp;_was_&nbsp;required,&nbsp;returns&nbsp;`True`&nbsp;if&nbsp;rebuilding&nbsp;was&nbsp;successful,&nbsp;otherwise&nbsp;`False`.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-model_validate"><strong>model_validate</strong></a>(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Validate&nbsp;a&nbsp;pydantic&nbsp;model&nbsp;instance.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;obj:&nbsp;The&nbsp;object&nbsp;to&nbsp;validate.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;Whether&nbsp;to&nbsp;enforce&nbsp;types&nbsp;strictly.<br>
&nbsp;&nbsp;&nbsp;&nbsp;from_attributes:&nbsp;Whether&nbsp;to&nbsp;extract&nbsp;data&nbsp;from&nbsp;object&nbsp;attributes.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Additional&nbsp;context&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;validator.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ValidationError:&nbsp;If&nbsp;the&nbsp;object&nbsp;could&nbsp;not&nbsp;be&nbsp;validated.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;validated&nbsp;model&nbsp;instance.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-model_validate_json"><strong>model_validate_json</strong></a>(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Usage&nbsp;docs:&nbsp;<a href="https://docs.pydantic.dev/2.10/concepts/json/#json-parsing">https://docs.pydantic.dev/2.10/concepts/json/#json-parsing</a><br>
&nbsp;<br>
Validate&nbsp;the&nbsp;given&nbsp;JSON&nbsp;data&nbsp;against&nbsp;the&nbsp;Pydantic&nbsp;model.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;json_data:&nbsp;The&nbsp;JSON&nbsp;data&nbsp;to&nbsp;validate.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;Whether&nbsp;to&nbsp;enforce&nbsp;types&nbsp;strictly.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Extra&nbsp;variables&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;validator.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;validated&nbsp;Pydantic&nbsp;model.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ValidationError:&nbsp;If&nbsp;`json_data`&nbsp;is&nbsp;not&nbsp;a&nbsp;JSON&nbsp;string&nbsp;or&nbsp;the&nbsp;object&nbsp;could&nbsp;not&nbsp;be&nbsp;validated.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-model_validate_strings"><strong>model_validate_strings</strong></a>(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Validate&nbsp;the&nbsp;given&nbsp;object&nbsp;with&nbsp;string&nbsp;data&nbsp;against&nbsp;the&nbsp;Pydantic&nbsp;model.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;obj:&nbsp;The&nbsp;object&nbsp;containing&nbsp;string&nbsp;data&nbsp;to&nbsp;validate.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;Whether&nbsp;to&nbsp;enforce&nbsp;types&nbsp;strictly.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Extra&nbsp;variables&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;validator.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;validated&nbsp;Pydantic&nbsp;model.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-parse_file"><strong>parse_file</strong></a>(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatOpenAI-parse_obj"><strong>parse_obj</strong></a>(obj: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatOpenAI-parse_raw"><strong>parse_raw</strong></a>(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatOpenAI-schema"><strong>schema</strong></a>(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -&gt; 'Dict[str, Any]'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatOpenAI-schema_json"><strong>schema_json</strong></a>(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -&gt; 'str'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatOpenAI-update_forward_refs"><strong>update_forward_refs</strong></a>(**localns: 'Any') -&gt; 'None'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatOpenAI-validate"><strong>validate</strong></a>(value: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<hr>
Readonly properties inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><strong>__fields_set__</strong></dt>
</dl>
<dl><dt><strong>model_computed_fields</strong></dt>
<dd><tt>Get&nbsp;metadata&nbsp;about&nbsp;the&nbsp;computed&nbsp;fields&nbsp;defined&nbsp;on&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Deprecation&nbsp;warning:&nbsp;you&nbsp;should&nbsp;be&nbsp;getting&nbsp;this&nbsp;information&nbsp;from&nbsp;the&nbsp;model&nbsp;class,&nbsp;not&nbsp;from&nbsp;an&nbsp;instance.<br>
In&nbsp;V3,&nbsp;this&nbsp;property&nbsp;will&nbsp;be&nbsp;removed&nbsp;from&nbsp;the&nbsp;`BaseModel`&nbsp;class.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;mapping&nbsp;of&nbsp;computed&nbsp;field&nbsp;names&nbsp;to&nbsp;[`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo]&nbsp;objects.</tt></dd>
</dl>
<dl><dt><strong>model_extra</strong></dt>
<dd><tt>Get&nbsp;extra&nbsp;fields&nbsp;set&nbsp;during&nbsp;validation.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;dictionary&nbsp;of&nbsp;extra&nbsp;fields,&nbsp;or&nbsp;`None`&nbsp;if&nbsp;`config.extra`&nbsp;is&nbsp;not&nbsp;set&nbsp;to&nbsp;`"allow"`.</tt></dd>
</dl>
<dl><dt><strong>model_fields</strong></dt>
<dd><tt>Get&nbsp;metadata&nbsp;about&nbsp;the&nbsp;fields&nbsp;defined&nbsp;on&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Deprecation&nbsp;warning:&nbsp;you&nbsp;should&nbsp;be&nbsp;getting&nbsp;this&nbsp;information&nbsp;from&nbsp;the&nbsp;model&nbsp;class,&nbsp;not&nbsp;from&nbsp;an&nbsp;instance.<br>
In&nbsp;V3,&nbsp;this&nbsp;property&nbsp;will&nbsp;be&nbsp;removed&nbsp;from&nbsp;the&nbsp;`BaseModel`&nbsp;class.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;mapping&nbsp;of&nbsp;field&nbsp;names&nbsp;to&nbsp;[`FieldInfo`][pydantic.fields.FieldInfo]&nbsp;objects.</tt></dd>
</dl>
<dl><dt><strong>model_fields_set</strong></dt>
<dd><tt>Returns&nbsp;the&nbsp;set&nbsp;of&nbsp;fields&nbsp;that&nbsp;have&nbsp;been&nbsp;explicitly&nbsp;set&nbsp;on&nbsp;this&nbsp;model&nbsp;instance.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;set&nbsp;of&nbsp;strings&nbsp;representing&nbsp;the&nbsp;fields&nbsp;that&nbsp;have&nbsp;been&nbsp;set,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i.e.&nbsp;that&nbsp;were&nbsp;not&nbsp;filled&nbsp;from&nbsp;defaults.</tt></dd>
</dl>
<hr>
Data descriptors inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__pydantic_extra__</strong></dt>
</dl>
<dl><dt><strong>__pydantic_fields_set__</strong></dt>
</dl>
<dl><dt><strong>__pydantic_private__</strong></dt>
</dl>
<hr>
Data and other attributes inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><strong>__hash__</strong> = None</dl>

<dl><dt><strong>__pydantic_root_model__</strong> = False</dl>

<hr>
Methods inherited from <a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a>:<br>
<dl><dt><a name="ChatOpenAI-__or__"><strong>__or__</strong></a>(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -&gt; 'RunnableSerializable[Input, Other]'</dt><dd><tt>Compose&nbsp;this&nbsp;Runnable&nbsp;with&nbsp;another&nbsp;object&nbsp;to&nbsp;create&nbsp;a&nbsp;RunnableSequence.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-__ror__"><strong>__ror__</strong></a>(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -&gt; 'RunnableSerializable[Other, Output]'</dt><dd><tt>Compose&nbsp;this&nbsp;Runnable&nbsp;with&nbsp;another&nbsp;object&nbsp;to&nbsp;create&nbsp;a&nbsp;RunnableSequence.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-abatch"><strong>abatch</strong></a>(self, inputs: 'list[Input]', config: 'Optional[Union[RunnableConfig, list[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -&gt; 'list[Output]'</dt><dd><tt>Default&nbsp;implementation&nbsp;runs&nbsp;ainvoke&nbsp;in&nbsp;parallel&nbsp;using&nbsp;asyncio.gather.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;of&nbsp;batch&nbsp;works&nbsp;well&nbsp;for&nbsp;IO&nbsp;bound&nbsp;runnables.<br>
&nbsp;<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;batch&nbsp;more&nbsp;efficiently;<br>
e.g.,&nbsp;if&nbsp;the&nbsp;underlying&nbsp;Runnable&nbsp;uses&nbsp;an&nbsp;API&nbsp;which&nbsp;supports&nbsp;a&nbsp;batch&nbsp;mode.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;inputs:&nbsp;A&nbsp;list&nbsp;of&nbsp;inputs&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;invoking&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;config&nbsp;supports&nbsp;standard&nbsp;keys&nbsp;like&nbsp;'tags',&nbsp;'metadata'&nbsp;for&nbsp;tracing<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;purposes,&nbsp;'max_concurrency'&nbsp;for&nbsp;controlling&nbsp;how&nbsp;much&nbsp;work&nbsp;to&nbsp;do<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;parallel,&nbsp;and&nbsp;other&nbsp;keys.&nbsp;Please&nbsp;refer&nbsp;to&nbsp;the&nbsp;RunnableConfig<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;more&nbsp;details.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;return_exceptions:&nbsp;Whether&nbsp;to&nbsp;return&nbsp;exceptions&nbsp;instead&nbsp;of&nbsp;raising&nbsp;them.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;False.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;list&nbsp;of&nbsp;outputs&nbsp;from&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-abatch_as_completed"><strong>abatch_as_completed</strong></a>(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -&gt; 'AsyncIterator[tuple[int, Union[Output, Exception]]]'</dt><dd><tt>Run&nbsp;ainvoke&nbsp;in&nbsp;parallel&nbsp;on&nbsp;a&nbsp;list&nbsp;of&nbsp;inputs,<br>
yielding&nbsp;results&nbsp;as&nbsp;they&nbsp;complete.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;inputs:&nbsp;A&nbsp;list&nbsp;of&nbsp;inputs&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;invoking&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;config&nbsp;supports&nbsp;standard&nbsp;keys&nbsp;like&nbsp;'tags',&nbsp;'metadata'&nbsp;for&nbsp;tracing<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;purposes,&nbsp;'max_concurrency'&nbsp;for&nbsp;controlling&nbsp;how&nbsp;much&nbsp;work&nbsp;to&nbsp;do<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;parallel,&nbsp;and&nbsp;other&nbsp;keys.&nbsp;Please&nbsp;refer&nbsp;to&nbsp;the&nbsp;RunnableConfig<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;more&nbsp;details.&nbsp;Defaults&nbsp;to&nbsp;None.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;return_exceptions:&nbsp;Whether&nbsp;to&nbsp;return&nbsp;exceptions&nbsp;instead&nbsp;of&nbsp;raising&nbsp;them.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;False.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;tuple&nbsp;of&nbsp;the&nbsp;index&nbsp;of&nbsp;the&nbsp;input&nbsp;and&nbsp;the&nbsp;output&nbsp;from&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-as_tool"><strong>as_tool</strong></a>(self, args_schema: 'Optional[type[BaseModel]]' = None, *, name: 'Optional[str]' = None, description: 'Optional[str]' = None, arg_types: 'Optional[dict[str, type]]' = None) -&gt; 'BaseTool'</dt><dd><tt>..&nbsp;beta::<br>
&nbsp;&nbsp;&nbsp;This&nbsp;API&nbsp;is&nbsp;in&nbsp;beta&nbsp;and&nbsp;may&nbsp;change&nbsp;in&nbsp;the&nbsp;future.<br>
&nbsp;<br>
Create&nbsp;a&nbsp;BaseTool&nbsp;from&nbsp;a&nbsp;Runnable.<br>
&nbsp;<br>
``as_tool``&nbsp;will&nbsp;instantiate&nbsp;a&nbsp;BaseTool&nbsp;with&nbsp;a&nbsp;name,&nbsp;description,&nbsp;and<br>
``args_schema``&nbsp;from&nbsp;a&nbsp;Runnable.&nbsp;Where&nbsp;possible,&nbsp;schemas&nbsp;are&nbsp;inferred<br>
from&nbsp;``runnable.get_input_schema``.&nbsp;Alternatively&nbsp;(e.g.,&nbsp;if&nbsp;the<br>
Runnable&nbsp;takes&nbsp;a&nbsp;dict&nbsp;as&nbsp;input&nbsp;and&nbsp;the&nbsp;specific&nbsp;dict&nbsp;keys&nbsp;are&nbsp;not&nbsp;typed),<br>
the&nbsp;schema&nbsp;can&nbsp;be&nbsp;specified&nbsp;directly&nbsp;with&nbsp;``args_schema``.&nbsp;You&nbsp;can&nbsp;also<br>
pass&nbsp;``arg_types``&nbsp;to&nbsp;just&nbsp;specify&nbsp;the&nbsp;required&nbsp;arguments&nbsp;and&nbsp;their&nbsp;types.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;args_schema:&nbsp;The&nbsp;schema&nbsp;for&nbsp;the&nbsp;tool.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;name:&nbsp;The&nbsp;name&nbsp;of&nbsp;the&nbsp;tool.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;description:&nbsp;The&nbsp;description&nbsp;of&nbsp;the&nbsp;tool.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;arg_types:&nbsp;A&nbsp;dictionary&nbsp;of&nbsp;argument&nbsp;names&nbsp;to&nbsp;types.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;BaseTool&nbsp;instance.<br>
&nbsp;<br>
Typed&nbsp;dict&nbsp;input:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing&nbsp;import&nbsp;List<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing_extensions&nbsp;import&nbsp;TypedDict<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;Args(TypedDict):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b:&nbsp;List[int]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;f(x:&nbsp;Args)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;str(x["a"]&nbsp;*&nbsp;max(x["b"]))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(f)<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool&nbsp;=&nbsp;runnable.<a href="#ChatOpenAI-as_tool">as_tool</a>()<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool.<a href="#ChatOpenAI-invoke">invoke</a>({"a":&nbsp;3,&nbsp;"b":&nbsp;[1,&nbsp;2]})<br>
&nbsp;<br>
``dict``&nbsp;input,&nbsp;specifying&nbsp;schema&nbsp;via&nbsp;``args_schema``:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing&nbsp;import&nbsp;Any,&nbsp;Dict,&nbsp;List<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;pydantic&nbsp;import&nbsp;BaseModel,&nbsp;Field<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;f(x:&nbsp;Dict[str,&nbsp;Any])&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;str(x["a"]&nbsp;*&nbsp;max(x["b"]))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;FSchema(BaseModel):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Apply&nbsp;a&nbsp;function&nbsp;to&nbsp;an&nbsp;integer&nbsp;and&nbsp;list&nbsp;of&nbsp;integers."""<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a:&nbsp;int&nbsp;=&nbsp;Field(...,&nbsp;description="Integer")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b:&nbsp;List[int]&nbsp;=&nbsp;Field(...,&nbsp;description="List&nbsp;of&nbsp;ints")<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(f)<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool&nbsp;=&nbsp;runnable.<a href="#ChatOpenAI-as_tool">as_tool</a>(FSchema)<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool.<a href="#ChatOpenAI-invoke">invoke</a>({"a":&nbsp;3,&nbsp;"b":&nbsp;[1,&nbsp;2]})<br>
&nbsp;<br>
``dict``&nbsp;input,&nbsp;specifying&nbsp;schema&nbsp;via&nbsp;``arg_types``:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing&nbsp;import&nbsp;Any,&nbsp;Dict,&nbsp;List<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;f(x:&nbsp;Dict[str,&nbsp;Any])&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;str(x["a"]&nbsp;*&nbsp;max(x["b"]))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(f)<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool&nbsp;=&nbsp;runnable.<a href="#ChatOpenAI-as_tool">as_tool</a>(arg_types={"a":&nbsp;int,&nbsp;"b":&nbsp;List[int]})<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool.<a href="#ChatOpenAI-invoke">invoke</a>({"a":&nbsp;3,&nbsp;"b":&nbsp;[1,&nbsp;2]})<br>
&nbsp;<br>
String&nbsp;input:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;f(x:&nbsp;str)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;+&nbsp;"a"<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;g(x:&nbsp;str)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;+&nbsp;"z"<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(f)&nbsp;|&nbsp;g<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool&nbsp;=&nbsp;runnable.<a href="#ChatOpenAI-as_tool">as_tool</a>()<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool.<a href="#ChatOpenAI-invoke">invoke</a>("b")<br>
&nbsp;<br>
..&nbsp;versionadded::&nbsp;0.2.14</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-assign"><strong>assign</strong></a>(self, **kwargs: 'Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any], Mapping[str, Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any]]]]') -&gt; 'RunnableSerializable[Any, Any]'</dt><dd><tt>Assigns&nbsp;new&nbsp;fields&nbsp;to&nbsp;the&nbsp;dict&nbsp;output&nbsp;of&nbsp;this&nbsp;Runnable.<br>
Returns&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_community.llms.fake&nbsp;import&nbsp;FakeStreamingListLLM<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.output_parsers&nbsp;import&nbsp;StrOutputParser<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.prompts&nbsp;import&nbsp;SystemMessagePromptTemplate<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;Runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;operator&nbsp;import&nbsp;itemgetter<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;=&nbsp;(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SystemMessagePromptTemplate.from_template("You&nbsp;are&nbsp;a&nbsp;nice&nbsp;assistant.")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+&nbsp;"{question}"<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;llm&nbsp;=&nbsp;FakeStreamingListLLM(responses=["foo-lish"])<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain:&nbsp;Runnable&nbsp;=&nbsp;prompt&nbsp;|&nbsp;llm&nbsp;|&nbsp;{"str":&nbsp;StrOutputParser()}<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain_with_assign&nbsp;=&nbsp;chain.<a href="#ChatOpenAI-assign">assign</a>(hello=itemgetter("str")&nbsp;|&nbsp;llm)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;print(chain_with_assign.input_schema.<a href="#ChatOpenAI-model_json_schema">model_json_schema</a>())<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;{'title':&nbsp;'PromptInput',&nbsp;'type':&nbsp;'object',&nbsp;'properties':<br>
&nbsp;&nbsp;&nbsp;&nbsp;{'question':&nbsp;{'title':&nbsp;'Question',&nbsp;'type':&nbsp;'string'}}}<br>
&nbsp;&nbsp;&nbsp;&nbsp;print(chain_with_assign.output_schema.<a href="#ChatOpenAI-model_json_schema">model_json_schema</a>())<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;{'title':&nbsp;'RunnableSequenceOutput',&nbsp;'type':&nbsp;'object',&nbsp;'properties':<br>
&nbsp;&nbsp;&nbsp;&nbsp;{'str':&nbsp;{'title':&nbsp;'Str',<br>
&nbsp;&nbsp;&nbsp;&nbsp;'type':&nbsp;'string'},&nbsp;'hello':&nbsp;{'title':&nbsp;'Hello',&nbsp;'type':&nbsp;'string'}}}</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-astream_events"><strong>astream_events</strong></a>(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: "Literal['v1', 'v2']" = 'v2', include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'AsyncIterator[StreamEvent]'</dt><dd><tt>Generate&nbsp;a&nbsp;stream&nbsp;of&nbsp;events.<br>
&nbsp;<br>
Use&nbsp;to&nbsp;create&nbsp;an&nbsp;iterator&nbsp;over&nbsp;StreamEvents&nbsp;that&nbsp;provide&nbsp;real-time&nbsp;information<br>
about&nbsp;the&nbsp;progress&nbsp;of&nbsp;the&nbsp;Runnable,&nbsp;including&nbsp;StreamEvents&nbsp;from&nbsp;intermediate<br>
results.<br>
&nbsp;<br>
A&nbsp;StreamEvent&nbsp;is&nbsp;a&nbsp;dictionary&nbsp;with&nbsp;the&nbsp;following&nbsp;schema:<br>
&nbsp;<br>
-&nbsp;``event``:&nbsp;**str**&nbsp;-&nbsp;Event&nbsp;names&nbsp;are&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;format:&nbsp;on_[runnable_type]_(start|stream|end).<br>
-&nbsp;``name``:&nbsp;**str**&nbsp;-&nbsp;The&nbsp;name&nbsp;of&nbsp;the&nbsp;Runnable&nbsp;that&nbsp;generated&nbsp;the&nbsp;event.<br>
-&nbsp;``run_id``:&nbsp;**str**&nbsp;-&nbsp;randomly&nbsp;generated&nbsp;ID&nbsp;associated&nbsp;with&nbsp;the&nbsp;given&nbsp;execution&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;Runnable&nbsp;that&nbsp;emitted&nbsp;the&nbsp;event.<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;child&nbsp;Runnable&nbsp;that&nbsp;gets&nbsp;invoked&nbsp;as&nbsp;part&nbsp;of&nbsp;the&nbsp;execution&nbsp;of&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;parent&nbsp;Runnable&nbsp;is&nbsp;assigned&nbsp;its&nbsp;own&nbsp;unique&nbsp;ID.<br>
-&nbsp;``parent_ids``:&nbsp;**List[str]**&nbsp;-&nbsp;The&nbsp;IDs&nbsp;of&nbsp;the&nbsp;parent&nbsp;runnables&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;generated&nbsp;the&nbsp;event.&nbsp;The&nbsp;root&nbsp;Runnable&nbsp;will&nbsp;have&nbsp;an&nbsp;empty&nbsp;list.<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;order&nbsp;of&nbsp;the&nbsp;parent&nbsp;IDs&nbsp;is&nbsp;from&nbsp;the&nbsp;root&nbsp;to&nbsp;the&nbsp;immediate&nbsp;parent.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Only&nbsp;available&nbsp;for&nbsp;v2&nbsp;version&nbsp;of&nbsp;the&nbsp;API.&nbsp;The&nbsp;v1&nbsp;version&nbsp;of&nbsp;the&nbsp;API<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;return&nbsp;an&nbsp;empty&nbsp;list.<br>
-&nbsp;``tags``:&nbsp;**Optional[List[str]]**&nbsp;-&nbsp;The&nbsp;tags&nbsp;of&nbsp;the&nbsp;Runnable&nbsp;that&nbsp;generated<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;event.<br>
-&nbsp;``metadata``:&nbsp;**Optional[Dict[str,&nbsp;Any]]**&nbsp;-&nbsp;The&nbsp;metadata&nbsp;of&nbsp;the&nbsp;Runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;generated&nbsp;the&nbsp;event.<br>
-&nbsp;``data``:&nbsp;**Dict[str,&nbsp;Any]**<br>
&nbsp;<br>
&nbsp;<br>
Below&nbsp;is&nbsp;a&nbsp;table&nbsp;that&nbsp;illustrates&nbsp;some&nbsp;events&nbsp;that&nbsp;might&nbsp;be&nbsp;emitted&nbsp;by&nbsp;various<br>
chains.&nbsp;Metadata&nbsp;fields&nbsp;have&nbsp;been&nbsp;omitted&nbsp;from&nbsp;the&nbsp;table&nbsp;for&nbsp;brevity.<br>
Chain&nbsp;definitions&nbsp;have&nbsp;been&nbsp;included&nbsp;after&nbsp;the&nbsp;table.<br>
&nbsp;<br>
**ATTENTION**&nbsp;This&nbsp;reference&nbsp;table&nbsp;is&nbsp;for&nbsp;the&nbsp;V2&nbsp;version&nbsp;of&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;event&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;chunk&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;input&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;output&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+======================+==================+=================================+===============================================+=================================================+<br>
|&nbsp;on_chat_model_start&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"messages":&nbsp;[[SystemMessage,&nbsp;HumanMessage]]}&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_chat_model_stream&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;AIMessageChunk(content="hello")&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_chat_model_end&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"messages":&nbsp;[[SystemMessage,&nbsp;HumanMessage]]}&nbsp;|&nbsp;AIMessageChunk(content="hello&nbsp;world")&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_llm_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{'input':&nbsp;'hello'}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_llm_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;'Hello'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_llm_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;'Hello&nbsp;human!'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_chain_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_chain_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;"hello&nbsp;world!,&nbsp;goodbye&nbsp;world!"&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_chain_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[Document(...)]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;"hello&nbsp;world!,&nbsp;goodbye&nbsp;world!"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_tool_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_tool_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_retriever_start&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"query":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_retriever_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"query":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[Document(...),&nbsp;..]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_prompt_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[template_name]&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"question":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_prompt_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[template_name]&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"question":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;ChatPromptValue(messages:&nbsp;[SystemMessage,&nbsp;...])&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
&nbsp;<br>
In&nbsp;addition&nbsp;to&nbsp;the&nbsp;standard&nbsp;events,&nbsp;users&nbsp;can&nbsp;also&nbsp;dispatch&nbsp;custom&nbsp;events&nbsp;(see&nbsp;example&nbsp;below).<br>
&nbsp;<br>
Custom&nbsp;events&nbsp;will&nbsp;be&nbsp;only&nbsp;be&nbsp;surfaced&nbsp;with&nbsp;in&nbsp;the&nbsp;`v2`&nbsp;version&nbsp;of&nbsp;the&nbsp;API!<br>
&nbsp;<br>
A&nbsp;custom&nbsp;event&nbsp;has&nbsp;following&nbsp;format:<br>
&nbsp;<br>
+-----------+------+-----------------------------------------------------------------------------------------------------------+<br>
|&nbsp;Attribute&nbsp;|&nbsp;Type&nbsp;|&nbsp;Description&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+===========+======+===========================================================================================================+<br>
|&nbsp;name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;str&nbsp;&nbsp;|&nbsp;A&nbsp;user&nbsp;defined&nbsp;name&nbsp;for&nbsp;the&nbsp;event.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+-----------+------+-----------------------------------------------------------------------------------------------------------+<br>
|&nbsp;data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;Any&nbsp;&nbsp;|&nbsp;The&nbsp;data&nbsp;associated&nbsp;with&nbsp;the&nbsp;event.&nbsp;This&nbsp;can&nbsp;be&nbsp;anything,&nbsp;though&nbsp;we&nbsp;suggest&nbsp;making&nbsp;it&nbsp;JSON&nbsp;serializable.&nbsp;&nbsp;|<br>
+-----------+------+-----------------------------------------------------------------------------------------------------------+<br>
&nbsp;<br>
Here&nbsp;are&nbsp;declarations&nbsp;associated&nbsp;with&nbsp;the&nbsp;standard&nbsp;events&nbsp;shown&nbsp;above:<br>
&nbsp;<br>
`format_docs`:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;format_docs(docs:&nbsp;List[Document])&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'''Format&nbsp;the&nbsp;docs.'''<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;",&nbsp;".join([doc.page_content&nbsp;for&nbsp;doc&nbsp;in&nbsp;docs])<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;format_docs&nbsp;=&nbsp;RunnableLambda(format_docs)<br>
&nbsp;<br>
`some_tool`:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;@tool<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;some_tool(x:&nbsp;int,&nbsp;y:&nbsp;str)&nbsp;-&gt;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'''Some_tool.'''<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;{"x":&nbsp;x,&nbsp;"y":&nbsp;y}<br>
&nbsp;<br>
`prompt`:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;template&nbsp;=&nbsp;ChatPromptTemplate.from_messages(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[("system",&nbsp;"You&nbsp;are&nbsp;Cat&nbsp;Agent&nbsp;007"),&nbsp;("human",&nbsp;"{question}")]<br>
&nbsp;&nbsp;&nbsp;&nbsp;).<a href="#ChatOpenAI-with_config">with_config</a>({"run_name":&nbsp;"my_template",&nbsp;"tags":&nbsp;["my_template"]})<br>
&nbsp;<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;reverse(s:&nbsp;str)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;s[::-1]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;RunnableLambda(func=reverse)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;events&nbsp;=&nbsp;[<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;event&nbsp;async&nbsp;for&nbsp;event&nbsp;in&nbsp;chain.<a href="#ChatOpenAI-astream_events">astream_events</a>("hello",&nbsp;version="v2")<br>
&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;will&nbsp;produce&nbsp;the&nbsp;following&nbsp;events&nbsp;(run_id,&nbsp;and&nbsp;parent_ids<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;has&nbsp;been&nbsp;omitted&nbsp;for&nbsp;brevity):<br>
&nbsp;&nbsp;&nbsp;&nbsp;[<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"input":&nbsp;"hello"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_start",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"chunk":&nbsp;"olleh"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_stream",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"output":&nbsp;"olleh"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_end",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;<br>
&nbsp;<br>
Example:&nbsp;Dispatch&nbsp;Custom&nbsp;Event<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.callbacks.manager&nbsp;import&nbsp;(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;adispatch_custom_event,<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda,&nbsp;RunnableConfig<br>
&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;asyncio<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;slow_thing(some_input:&nbsp;str,&nbsp;config:&nbsp;RunnableConfig)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Do&nbsp;something&nbsp;that&nbsp;takes&nbsp;a&nbsp;long&nbsp;time."""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(1)&nbsp;#&nbsp;Placeholder&nbsp;for&nbsp;some&nbsp;slow&nbsp;operation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;adispatch_custom_event(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"progress_event",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{"message":&nbsp;"Finished&nbsp;step&nbsp;1&nbsp;of&nbsp;3"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;config=config&nbsp;#&nbsp;Must&nbsp;be&nbsp;included&nbsp;for&nbsp;python&nbsp;&lt;&nbsp;3.10<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(1)&nbsp;#&nbsp;Placeholder&nbsp;for&nbsp;some&nbsp;slow&nbsp;operation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;adispatch_custom_event(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"progress_event",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{"message":&nbsp;"Finished&nbsp;step&nbsp;2&nbsp;of&nbsp;3"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;config=config&nbsp;#&nbsp;Must&nbsp;be&nbsp;included&nbsp;for&nbsp;python&nbsp;&lt;&nbsp;3.10<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(1)&nbsp;#&nbsp;Placeholder&nbsp;for&nbsp;some&nbsp;slow&nbsp;operation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;"Done"<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;slow_thing&nbsp;=&nbsp;RunnableLambda(slow_thing)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;for&nbsp;event&nbsp;in&nbsp;slow_thing.<a href="#ChatOpenAI-astream_events">astream_events</a>("some_input",&nbsp;version="v2"):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(event)<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;version:&nbsp;The&nbsp;version&nbsp;of&nbsp;the&nbsp;schema&nbsp;to&nbsp;use&nbsp;either&nbsp;`v2`&nbsp;or&nbsp;`v1`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Users&nbsp;should&nbsp;use&nbsp;`v2`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`v1`&nbsp;is&nbsp;for&nbsp;backwards&nbsp;compatibility&nbsp;and&nbsp;will&nbsp;be&nbsp;deprecated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;0.4.0.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;No&nbsp;default&nbsp;will&nbsp;be&nbsp;assigned&nbsp;until&nbsp;the&nbsp;API&nbsp;is&nbsp;stabilized.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;custom&nbsp;events&nbsp;will&nbsp;only&nbsp;be&nbsp;surfaced&nbsp;in&nbsp;`v2`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_names:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_types:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_tags:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_names:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_types:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_tags:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;These&nbsp;will&nbsp;be&nbsp;passed&nbsp;to&nbsp;astream_log&nbsp;as&nbsp;this&nbsp;implementation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;astream_events&nbsp;is&nbsp;built&nbsp;on&nbsp;top&nbsp;of&nbsp;astream_log.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;async&nbsp;stream&nbsp;of&nbsp;StreamEvents.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;NotImplementedError:&nbsp;If&nbsp;the&nbsp;version&nbsp;is&nbsp;not&nbsp;`v1`&nbsp;or&nbsp;`v2`.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-astream_log"><strong>astream_log</strong></a>(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'</dt><dd><tt>Stream&nbsp;all&nbsp;output&nbsp;from&nbsp;a&nbsp;Runnable,&nbsp;as&nbsp;reported&nbsp;to&nbsp;the&nbsp;callback&nbsp;system.<br>
This&nbsp;includes&nbsp;all&nbsp;inner&nbsp;runs&nbsp;of&nbsp;LLMs,&nbsp;Retrievers,&nbsp;Tools,&nbsp;etc.<br>
&nbsp;<br>
Output&nbsp;is&nbsp;streamed&nbsp;as&nbsp;Log&nbsp;objects,&nbsp;which&nbsp;include&nbsp;a&nbsp;list&nbsp;of<br>
Jsonpatch&nbsp;ops&nbsp;that&nbsp;describe&nbsp;how&nbsp;the&nbsp;state&nbsp;of&nbsp;the&nbsp;run&nbsp;has&nbsp;changed&nbsp;in&nbsp;each<br>
step,&nbsp;and&nbsp;the&nbsp;final&nbsp;state&nbsp;of&nbsp;the&nbsp;run.<br>
&nbsp;<br>
The&nbsp;Jsonpatch&nbsp;ops&nbsp;can&nbsp;be&nbsp;applied&nbsp;in&nbsp;order&nbsp;to&nbsp;construct&nbsp;state.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;diff:&nbsp;Whether&nbsp;to&nbsp;yield&nbsp;diffs&nbsp;between&nbsp;each&nbsp;step&nbsp;or&nbsp;the&nbsp;current&nbsp;state.<br>
&nbsp;&nbsp;&nbsp;&nbsp;with_streamed_output_list:&nbsp;Whether&nbsp;to&nbsp;yield&nbsp;the&nbsp;streamed_output&nbsp;list.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_names:&nbsp;Only&nbsp;include&nbsp;logs&nbsp;with&nbsp;these&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_types:&nbsp;Only&nbsp;include&nbsp;logs&nbsp;with&nbsp;these&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_tags:&nbsp;Only&nbsp;include&nbsp;logs&nbsp;with&nbsp;these&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_names:&nbsp;Exclude&nbsp;logs&nbsp;with&nbsp;these&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_types:&nbsp;Exclude&nbsp;logs&nbsp;with&nbsp;these&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_tags:&nbsp;Exclude&nbsp;logs&nbsp;with&nbsp;these&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;RunLogPatch&nbsp;or&nbsp;RunLog&nbsp;object.</tt></dd></dl>

<dl><dt>async <a name="ChatOpenAI-atransform"><strong>atransform</strong></a>(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -&gt; 'AsyncIterator[Output]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;atransform,&nbsp;which&nbsp;buffers&nbsp;input&nbsp;and&nbsp;calls&nbsp;astream.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;start&nbsp;producing&nbsp;output&nbsp;while<br>
input&nbsp;is&nbsp;still&nbsp;being&nbsp;generated.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;An&nbsp;async&nbsp;iterator&nbsp;of&nbsp;inputs&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-batch"><strong>batch</strong></a>(self, inputs: 'list[Input]', config: 'Optional[Union[RunnableConfig, list[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -&gt; 'list[Output]'</dt><dd><tt>Default&nbsp;implementation&nbsp;runs&nbsp;invoke&nbsp;in&nbsp;parallel&nbsp;using&nbsp;a&nbsp;thread&nbsp;pool&nbsp;executor.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;of&nbsp;batch&nbsp;works&nbsp;well&nbsp;for&nbsp;IO&nbsp;bound&nbsp;runnables.<br>
&nbsp;<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;batch&nbsp;more&nbsp;efficiently;<br>
e.g.,&nbsp;if&nbsp;the&nbsp;underlying&nbsp;Runnable&nbsp;uses&nbsp;an&nbsp;API&nbsp;which&nbsp;supports&nbsp;a&nbsp;batch&nbsp;mode.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-batch_as_completed"><strong>batch_as_completed</strong></a>(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -&gt; 'Iterator[tuple[int, Union[Output, Exception]]]'</dt><dd><tt>Run&nbsp;invoke&nbsp;in&nbsp;parallel&nbsp;on&nbsp;a&nbsp;list&nbsp;of&nbsp;inputs,<br>
yielding&nbsp;results&nbsp;as&nbsp;they&nbsp;complete.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-bind"><strong>bind</strong></a>(self, **kwargs: 'Any') -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;arguments&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
Useful&nbsp;when&nbsp;a&nbsp;Runnable&nbsp;in&nbsp;a&nbsp;chain&nbsp;requires&nbsp;an&nbsp;argument&nbsp;that&nbsp;is&nbsp;not<br>
in&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;previous&nbsp;Runnable&nbsp;or&nbsp;included&nbsp;in&nbsp;the&nbsp;user&nbsp;input.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;The&nbsp;arguments&nbsp;to&nbsp;bind&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;arguments&nbsp;bound.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_community.chat_models&nbsp;import&nbsp;ChatOllama<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.output_parsers&nbsp;import&nbsp;StrOutputParser<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;llm&nbsp;=&nbsp;ChatOllama(model='llama2')<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Without&nbsp;bind.<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;llm<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;StrOutputParser()<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain.<a href="#ChatOpenAI-invoke">invoke</a>("Repeat&nbsp;quoted&nbsp;words&nbsp;exactly:&nbsp;'One&nbsp;two&nbsp;three&nbsp;four&nbsp;five.'")<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Output&nbsp;is&nbsp;'One&nbsp;two&nbsp;three&nbsp;four&nbsp;five.'<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;With&nbsp;bind.<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;llm.<a href="#ChatOpenAI-bind">bind</a>(stop=["three"])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;StrOutputParser()<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain.<a href="#ChatOpenAI-invoke">invoke</a>("Repeat&nbsp;quoted&nbsp;words&nbsp;exactly:&nbsp;'One&nbsp;two&nbsp;three&nbsp;four&nbsp;five.'")<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Output&nbsp;is&nbsp;'One&nbsp;two'</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-config_schema"><strong>config_schema</strong></a>(self, *, include: 'Optional[Sequence[str]]' = None) -&gt; 'type[BaseModel]'</dt><dd><tt>The&nbsp;type&nbsp;of&nbsp;config&nbsp;this&nbsp;Runnable&nbsp;accepts&nbsp;specified&nbsp;as&nbsp;a&nbsp;pydantic&nbsp;model.<br>
&nbsp;<br>
To&nbsp;mark&nbsp;a&nbsp;field&nbsp;as&nbsp;configurable,&nbsp;see&nbsp;the&nbsp;`configurable_fields`<br>
and&nbsp;`configurable_alternatives`&nbsp;methods.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;A&nbsp;list&nbsp;of&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;config&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;config.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_config_jsonschema"><strong>get_config_jsonschema</strong></a>(self, *, include: 'Optional[Sequence[str]]' = None) -&gt; 'dict[str, Any]'</dt><dd><tt>Get&nbsp;a&nbsp;JSON&nbsp;schema&nbsp;that&nbsp;represents&nbsp;the&nbsp;config&nbsp;of&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;A&nbsp;list&nbsp;of&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;config&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;JSON&nbsp;schema&nbsp;that&nbsp;represents&nbsp;the&nbsp;config&nbsp;of&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
..&nbsp;versionadded::&nbsp;0.3.0</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_graph"><strong>get_graph</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'Graph'</dt><dd><tt>Return&nbsp;a&nbsp;graph&nbsp;representation&nbsp;of&nbsp;this&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_input_jsonschema"><strong>get_input_jsonschema</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'dict[str, Any]'</dt><dd><tt>Get&nbsp;a&nbsp;JSON&nbsp;schema&nbsp;that&nbsp;represents&nbsp;the&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;generating&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;JSON&nbsp;schema&nbsp;that&nbsp;represents&nbsp;the&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;add_one(x:&nbsp;int)&nbsp;-&gt;&nbsp;int:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;+&nbsp;1<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(add_one)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(runnable.<a href="#ChatOpenAI-get_input_jsonschema">get_input_jsonschema</a>())<br>
&nbsp;<br>
..&nbsp;versionadded::&nbsp;0.3.0</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_input_schema"><strong>get_input_schema</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'type[BaseModel]'</dt><dd><tt>Get&nbsp;a&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Runnables&nbsp;that&nbsp;leverage&nbsp;the&nbsp;configurable_fields&nbsp;and&nbsp;configurable_alternatives<br>
methods&nbsp;will&nbsp;have&nbsp;a&nbsp;dynamic&nbsp;input&nbsp;schema&nbsp;that&nbsp;depends&nbsp;on&nbsp;which<br>
configuration&nbsp;the&nbsp;Runnable&nbsp;is&nbsp;invoked&nbsp;with.<br>
&nbsp;<br>
This&nbsp;method&nbsp;allows&nbsp;to&nbsp;get&nbsp;an&nbsp;input&nbsp;schema&nbsp;for&nbsp;a&nbsp;specific&nbsp;configuration.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;generating&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;input.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_name"><strong>get_name</strong></a>(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -&gt; 'str'</dt><dd><tt>Get&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_output_jsonschema"><strong>get_output_jsonschema</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'dict[str, Any]'</dt><dd><tt>Get&nbsp;a&nbsp;JSON&nbsp;schema&nbsp;that&nbsp;represents&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;generating&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;JSON&nbsp;schema&nbsp;that&nbsp;represents&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;add_one(x:&nbsp;int)&nbsp;-&gt;&nbsp;int:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;+&nbsp;1<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(add_one)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(runnable.<a href="#ChatOpenAI-get_output_jsonschema">get_output_jsonschema</a>())<br>
&nbsp;<br>
..&nbsp;versionadded::&nbsp;0.3.0</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_output_schema"><strong>get_output_schema</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'type[BaseModel]'</dt><dd><tt>Get&nbsp;a&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;output&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Runnables&nbsp;that&nbsp;leverage&nbsp;the&nbsp;configurable_fields&nbsp;and&nbsp;configurable_alternatives<br>
methods&nbsp;will&nbsp;have&nbsp;a&nbsp;dynamic&nbsp;output&nbsp;schema&nbsp;that&nbsp;depends&nbsp;on&nbsp;which<br>
configuration&nbsp;the&nbsp;Runnable&nbsp;is&nbsp;invoked&nbsp;with.<br>
&nbsp;<br>
This&nbsp;method&nbsp;allows&nbsp;to&nbsp;get&nbsp;an&nbsp;output&nbsp;schema&nbsp;for&nbsp;a&nbsp;specific&nbsp;configuration.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;generating&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;output.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-get_prompts"><strong>get_prompts</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'list[BasePromptTemplate]'</dt><dd><tt>Return&nbsp;a&nbsp;list&nbsp;of&nbsp;prompts&nbsp;used&nbsp;by&nbsp;this&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-map"><strong>map</strong></a>(self) -&gt; 'Runnable[list[Input], list[Output]]'</dt><dd><tt>Return&nbsp;a&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;maps&nbsp;a&nbsp;list&nbsp;of&nbsp;inputs&nbsp;to&nbsp;a&nbsp;list&nbsp;of&nbsp;outputs,<br>
by&nbsp;calling&nbsp;<a href="#ChatOpenAI-invoke">invoke</a>()&nbsp;with&nbsp;each&nbsp;input.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;maps&nbsp;a&nbsp;list&nbsp;of&nbsp;inputs&nbsp;to&nbsp;a&nbsp;list&nbsp;of&nbsp;outputs.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;_lambda(x:&nbsp;int)&nbsp;-&gt;&nbsp;int:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;+&nbsp;1<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(_lambda)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(runnable.<a href="#ChatOpenAI-map">map</a>().<a href="#ChatOpenAI-invoke">invoke</a>([1,&nbsp;2,&nbsp;3]))&nbsp;#&nbsp;[2,&nbsp;3,&nbsp;4]</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-pick"><strong>pick</strong></a>(self, keys: 'Union[str, list[str]]') -&gt; 'RunnableSerializable[Any, Any]'</dt><dd><tt>Pick&nbsp;keys&nbsp;from&nbsp;the&nbsp;output&nbsp;dict&nbsp;of&nbsp;this&nbsp;Runnable.<br>
&nbsp;<br>
Pick&nbsp;single&nbsp;key:<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;json<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda,&nbsp;RunnableMap<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as_str&nbsp;=&nbsp;RunnableLambda(str)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as_json&nbsp;=&nbsp;RunnableLambda(json.loads)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;RunnableMap(str=as_str,&nbsp;json=as_json)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chain.<a href="#ChatOpenAI-invoke">invoke</a>("[1,&nbsp;2,&nbsp;3]")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;{"str":&nbsp;"[1,&nbsp;2,&nbsp;3]",&nbsp;"json":&nbsp;[1,&nbsp;2,&nbsp;3]}<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;json_only_chain&nbsp;=&nbsp;chain.<a href="#ChatOpenAI-pick">pick</a>("json")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;json_only_chain.<a href="#ChatOpenAI-invoke">invoke</a>("[1,&nbsp;2,&nbsp;3]")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;[1,&nbsp;2,&nbsp;3]<br>
&nbsp;<br>
Pick&nbsp;list&nbsp;of&nbsp;keys:<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing&nbsp;import&nbsp;Any<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;json<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda,&nbsp;RunnableMap<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as_str&nbsp;=&nbsp;RunnableLambda(str)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as_json&nbsp;=&nbsp;RunnableLambda(json.loads)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;as_bytes(x:&nbsp;Any)&nbsp;-&gt;&nbsp;bytes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;bytes(x,&nbsp;"utf-8")<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;RunnableMap(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;str=as_str,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;json=as_json,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bytes=RunnableLambda(as_bytes)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chain.<a href="#ChatOpenAI-invoke">invoke</a>("[1,&nbsp;2,&nbsp;3]")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;{"str":&nbsp;"[1,&nbsp;2,&nbsp;3]",&nbsp;"json":&nbsp;[1,&nbsp;2,&nbsp;3],&nbsp;"bytes":&nbsp;b"[1,&nbsp;2,&nbsp;3]"}<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;json_and_bytes_chain&nbsp;=&nbsp;chain.<a href="#ChatOpenAI-pick">pick</a>(["json",&nbsp;"bytes"])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;json_and_bytes_chain.<a href="#ChatOpenAI-invoke">invoke</a>("[1,&nbsp;2,&nbsp;3]")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;{"json":&nbsp;[1,&nbsp;2,&nbsp;3],&nbsp;"bytes":&nbsp;b"[1,&nbsp;2,&nbsp;3]"}</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-pipe"><strong>pipe</strong></a>(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -&gt; 'RunnableSerializable[Input, Other]'</dt><dd><tt>Compose&nbsp;this&nbsp;Runnable&nbsp;with&nbsp;Runnable-like&nbsp;objects&nbsp;to&nbsp;make&nbsp;a&nbsp;RunnableSequence.<br>
&nbsp;<br>
Equivalent&nbsp;to&nbsp;`RunnableSequence(self,&nbsp;*others)`&nbsp;or&nbsp;`self&nbsp;|&nbsp;others[0]&nbsp;|&nbsp;...`<br>
&nbsp;<br>
Example:<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;add_one(x:&nbsp;int)&nbsp;-&gt;&nbsp;int:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;+&nbsp;1<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;mul_two(x:&nbsp;int)&nbsp;-&gt;&nbsp;int:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;*&nbsp;2<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable_1&nbsp;=&nbsp;RunnableLambda(add_one)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable_2&nbsp;=&nbsp;RunnableLambda(mul_two)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sequence&nbsp;=&nbsp;runnable_1.<a href="#ChatOpenAI-pipe">pipe</a>(runnable_2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Or&nbsp;equivalently:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;sequence&nbsp;=&nbsp;runnable_1&nbsp;|&nbsp;runnable_2<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;sequence&nbsp;=&nbsp;RunnableSequence(first=runnable_1,&nbsp;last=runnable_2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sequence.<a href="#ChatOpenAI-invoke">invoke</a>(1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;sequence.<a href="#ChatOpenAI-ainvoke">ainvoke</a>(1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;4<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sequence.<a href="#ChatOpenAI-batch">batch</a>([1,&nbsp;2,&nbsp;3])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;sequence.<a href="#ChatOpenAI-abatch">abatch</a>([1,&nbsp;2,&nbsp;3])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;[4,&nbsp;6,&nbsp;8]</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-transform"><strong>transform</strong></a>(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -&gt; 'Iterator[Output]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;transform,&nbsp;which&nbsp;buffers&nbsp;input&nbsp;and&nbsp;calls&nbsp;astream.<br>
&nbsp;<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;start&nbsp;producing&nbsp;output&nbsp;while<br>
input&nbsp;is&nbsp;still&nbsp;being&nbsp;generated.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;An&nbsp;iterator&nbsp;of&nbsp;inputs&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-with_alisteners"><strong>with_alisteners</strong></a>(self, *, on_start: 'Optional[AsyncListener]' = None, on_end: 'Optional[AsyncListener]' = None, on_error: 'Optional[AsyncListener]' = None) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;async&nbsp;lifecycle&nbsp;listeners&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
on_start:&nbsp;Asynchronously&nbsp;called&nbsp;before&nbsp;the&nbsp;Runnable&nbsp;starts&nbsp;running.<br>
on_end:&nbsp;Asynchronously&nbsp;called&nbsp;after&nbsp;the&nbsp;Runnable&nbsp;finishes&nbsp;running.<br>
on_error:&nbsp;Asynchronously&nbsp;called&nbsp;if&nbsp;the&nbsp;Runnable&nbsp;throws&nbsp;an&nbsp;error.<br>
&nbsp;<br>
The&nbsp;Run&nbsp;object&nbsp;contains&nbsp;information&nbsp;about&nbsp;the&nbsp;run,&nbsp;including&nbsp;its&nbsp;id,<br>
type,&nbsp;input,&nbsp;output,&nbsp;error,&nbsp;start_time,&nbsp;end_time,&nbsp;and&nbsp;any&nbsp;tags&nbsp;or&nbsp;metadata<br>
added&nbsp;to&nbsp;the&nbsp;run.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_start:&nbsp;Asynchronously&nbsp;called&nbsp;before&nbsp;the&nbsp;Runnable&nbsp;starts&nbsp;running.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_end:&nbsp;Asynchronously&nbsp;called&nbsp;after&nbsp;the&nbsp;Runnable&nbsp;finishes&nbsp;running.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_error:&nbsp;Asynchronously&nbsp;called&nbsp;if&nbsp;the&nbsp;Runnable&nbsp;throws&nbsp;an&nbsp;error.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;listeners&nbsp;bound.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda,&nbsp;Runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;datetime&nbsp;import&nbsp;datetime,&nbsp;timezone<br>
&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;time<br>
&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;asyncio<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;format_t(timestamp:&nbsp;float)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;datetime.fromtimestamp(timestamp,&nbsp;tz=timezone.utc).isoformat()<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;test_runnable(time_to_sleep&nbsp;:&nbsp;int):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"Runnable[{time_to_sleep}s]:&nbsp;starts&nbsp;at&nbsp;{format_t(time.time())}")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(time_to_sleep)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"Runnable[{time_to_sleep}s]:&nbsp;ends&nbsp;at&nbsp;{format_t(time.time())}")<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;fn_start(run_obj&nbsp;:&nbsp;Runnable):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"on&nbsp;start&nbsp;callback&nbsp;starts&nbsp;at&nbsp;{format_t(time.time())}")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"on&nbsp;start&nbsp;callback&nbsp;ends&nbsp;at&nbsp;{format_t(time.time())}")<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;fn_end(run_obj&nbsp;:&nbsp;Runnable):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"on&nbsp;end&nbsp;callback&nbsp;starts&nbsp;at&nbsp;{format_t(time.time())}")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"on&nbsp;end&nbsp;callback&nbsp;ends&nbsp;at&nbsp;{format_t(time.time())}")<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(test_runnable).<a href="#ChatOpenAI-with_alisteners">with_alisteners</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on_start=fn_start,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on_end=fn_end<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;concurrent_runs():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.gather(runnable.<a href="#ChatOpenAI-ainvoke">ainvoke</a>(2),&nbsp;runnable.<a href="#ChatOpenAI-ainvoke">ainvoke</a>(3))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;asyncio.run(concurrent_runs())<br>
&nbsp;&nbsp;&nbsp;&nbsp;Result:<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;start&nbsp;callback&nbsp;starts&nbsp;at&nbsp;2025-03-01T07:05:22.875378+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;start&nbsp;callback&nbsp;starts&nbsp;at&nbsp;2025-03-01T07:05:22.875495+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;start&nbsp;callback&nbsp;ends&nbsp;at&nbsp;2025-03-01T07:05:25.878862+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;start&nbsp;callback&nbsp;ends&nbsp;at&nbsp;2025-03-01T07:05:25.878947+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;Runnable[2s]:&nbsp;starts&nbsp;at&nbsp;2025-03-01T07:05:25.879392+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;Runnable[3s]:&nbsp;starts&nbsp;at&nbsp;2025-03-01T07:05:25.879804+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;Runnable[2s]:&nbsp;ends&nbsp;at&nbsp;2025-03-01T07:05:27.881998+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;end&nbsp;callback&nbsp;starts&nbsp;at&nbsp;2025-03-01T07:05:27.882360+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;Runnable[3s]:&nbsp;ends&nbsp;at&nbsp;2025-03-01T07:05:28.881737+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;end&nbsp;callback&nbsp;starts&nbsp;at&nbsp;2025-03-01T07:05:28.882428+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;end&nbsp;callback&nbsp;ends&nbsp;at&nbsp;2025-03-01T07:05:29.883893+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;end&nbsp;callback&nbsp;ends&nbsp;at&nbsp;2025-03-01T07:05:30.884831+00:00</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-with_config"><strong>with_config</strong></a>(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;config&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;bind&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;config&nbsp;bound.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-with_fallbacks"><strong>with_fallbacks</strong></a>(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'tuple[type[BaseException], ...]' = (&lt;class 'Exception'&gt;,), exception_key: 'Optional[str]' = None) -&gt; 'RunnableWithFallbacksT[Input, Output]'</dt><dd><tt>Add&nbsp;fallbacks&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
The&nbsp;new&nbsp;Runnable&nbsp;will&nbsp;try&nbsp;the&nbsp;original&nbsp;Runnable,&nbsp;and&nbsp;then&nbsp;each&nbsp;fallback<br>
in&nbsp;order,&nbsp;upon&nbsp;failures.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fallbacks:&nbsp;A&nbsp;sequence&nbsp;of&nbsp;runnables&nbsp;to&nbsp;try&nbsp;if&nbsp;the&nbsp;original&nbsp;Runnable&nbsp;fails.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exceptions_to_handle:&nbsp;A&nbsp;tuple&nbsp;of&nbsp;exception&nbsp;types&nbsp;to&nbsp;handle.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;(Exception,).<br>
&nbsp;&nbsp;&nbsp;&nbsp;exception_key:&nbsp;If&nbsp;string&nbsp;is&nbsp;specified&nbsp;then&nbsp;handled&nbsp;exceptions&nbsp;will&nbsp;be&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;fallbacks&nbsp;as&nbsp;part&nbsp;of&nbsp;the&nbsp;input&nbsp;under&nbsp;the&nbsp;specified&nbsp;key.&nbsp;If&nbsp;None,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exceptions&nbsp;will&nbsp;not&nbsp;be&nbsp;passed&nbsp;to&nbsp;fallbacks.&nbsp;If&nbsp;used,&nbsp;the&nbsp;base&nbsp;Runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;its&nbsp;fallbacks&nbsp;must&nbsp;accept&nbsp;a&nbsp;dictionary&nbsp;as&nbsp;input.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;will&nbsp;try&nbsp;the&nbsp;original&nbsp;Runnable,&nbsp;and&nbsp;then&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;fallback&nbsp;in&nbsp;order,&nbsp;upon&nbsp;failures.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing&nbsp;import&nbsp;Iterator<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableGenerator<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;_generate_immediate_error(input:&nbsp;Iterator)&nbsp;-&gt;&nbsp;Iterator[str]:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise&nbsp;ValueError()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yield&nbsp;""<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;_generate(input:&nbsp;Iterator)&nbsp;-&gt;&nbsp;Iterator[str]:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yield&nbsp;from&nbsp;"foo&nbsp;bar"<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableGenerator(_generate_immediate_error).<a href="#ChatOpenAI-with_fallbacks">with_fallbacks</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[RunnableGenerator(_generate)]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(''.join(runnable.<a href="#ChatOpenAI-stream">stream</a>({})))&nbsp;#foo&nbsp;bar<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fallbacks:&nbsp;A&nbsp;sequence&nbsp;of&nbsp;runnables&nbsp;to&nbsp;try&nbsp;if&nbsp;the&nbsp;original&nbsp;Runnable&nbsp;fails.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exceptions_to_handle:&nbsp;A&nbsp;tuple&nbsp;of&nbsp;exception&nbsp;types&nbsp;to&nbsp;handle.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exception_key:&nbsp;If&nbsp;string&nbsp;is&nbsp;specified&nbsp;then&nbsp;handled&nbsp;exceptions&nbsp;will&nbsp;be&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;fallbacks&nbsp;as&nbsp;part&nbsp;of&nbsp;the&nbsp;input&nbsp;under&nbsp;the&nbsp;specified&nbsp;key.&nbsp;If&nbsp;None,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exceptions&nbsp;will&nbsp;not&nbsp;be&nbsp;passed&nbsp;to&nbsp;fallbacks.&nbsp;If&nbsp;used,&nbsp;the&nbsp;base&nbsp;Runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;its&nbsp;fallbacks&nbsp;must&nbsp;accept&nbsp;a&nbsp;dictionary&nbsp;as&nbsp;input.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;will&nbsp;try&nbsp;the&nbsp;original&nbsp;Runnable,&nbsp;and&nbsp;then&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;fallback&nbsp;in&nbsp;order,&nbsp;upon&nbsp;failures.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-with_listeners"><strong>with_listeners</strong></a>(self, *, on_start: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_end: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_error: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;lifecycle&nbsp;listeners&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
on_start:&nbsp;Called&nbsp;before&nbsp;the&nbsp;Runnable&nbsp;starts&nbsp;running,&nbsp;with&nbsp;the&nbsp;Run&nbsp;object.<br>
on_end:&nbsp;Called&nbsp;after&nbsp;the&nbsp;Runnable&nbsp;finishes&nbsp;running,&nbsp;with&nbsp;the&nbsp;Run&nbsp;object.<br>
on_error:&nbsp;Called&nbsp;if&nbsp;the&nbsp;Runnable&nbsp;throws&nbsp;an&nbsp;error,&nbsp;with&nbsp;the&nbsp;Run&nbsp;object.<br>
&nbsp;<br>
The&nbsp;Run&nbsp;object&nbsp;contains&nbsp;information&nbsp;about&nbsp;the&nbsp;run,&nbsp;including&nbsp;its&nbsp;id,<br>
type,&nbsp;input,&nbsp;output,&nbsp;error,&nbsp;start_time,&nbsp;end_time,&nbsp;and&nbsp;any&nbsp;tags&nbsp;or&nbsp;metadata<br>
added&nbsp;to&nbsp;the&nbsp;run.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_start:&nbsp;Called&nbsp;before&nbsp;the&nbsp;Runnable&nbsp;starts&nbsp;running.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_end:&nbsp;Called&nbsp;after&nbsp;the&nbsp;Runnable&nbsp;finishes&nbsp;running.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_error:&nbsp;Called&nbsp;if&nbsp;the&nbsp;Runnable&nbsp;throws&nbsp;an&nbsp;error.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;listeners&nbsp;bound.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.tracers.schemas&nbsp;import&nbsp;Run<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;time<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;test_runnable(time_to_sleep&nbsp;:&nbsp;int):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(time_to_sleep)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;fn_start(run_obj:&nbsp;Run):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("start_time:",&nbsp;run_obj.start_time)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;fn_end(run_obj:&nbsp;Run):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("end_time:",&nbsp;run_obj.end_time)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;RunnableLambda(test_runnable).<a href="#ChatOpenAI-with_listeners">with_listeners</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on_start=fn_start,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on_end=fn_end<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain.<a href="#ChatOpenAI-invoke">invoke</a>(2)</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-with_retry"><strong>with_retry</strong></a>(self, *, retry_if_exception_type: 'tuple[type[BaseException], ...]' = (&lt;class 'Exception'&gt;,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Create&nbsp;a&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;retries&nbsp;the&nbsp;original&nbsp;Runnable&nbsp;on&nbsp;exceptions.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;retry_if_exception_type:&nbsp;A&nbsp;tuple&nbsp;of&nbsp;exception&nbsp;types&nbsp;to&nbsp;retry&nbsp;on.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;(Exception,).<br>
&nbsp;&nbsp;&nbsp;&nbsp;wait_exponential_jitter:&nbsp;Whether&nbsp;to&nbsp;add&nbsp;jitter&nbsp;to&nbsp;the&nbsp;wait<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;time&nbsp;between&nbsp;retries.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop_after_attempt:&nbsp;The&nbsp;maximum&nbsp;number&nbsp;of&nbsp;attempts&nbsp;to&nbsp;make&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;giving&nbsp;up.&nbsp;Defaults&nbsp;to&nbsp;3.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;retries&nbsp;the&nbsp;original&nbsp;Runnable&nbsp;on&nbsp;exceptions.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;count&nbsp;=&nbsp;0<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;_lambda(x:&nbsp;int)&nbsp;-&gt;&nbsp;None:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;global&nbsp;count<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;count&nbsp;=&nbsp;count&nbsp;+&nbsp;1<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;x&nbsp;==&nbsp;1:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise&nbsp;ValueError("x&nbsp;is&nbsp;1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(_lambda)<br>
&nbsp;&nbsp;&nbsp;&nbsp;try:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable.<a href="#ChatOpenAI-with_retry">with_retry</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stop_after_attempt=2,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;retry_if_exception_type=(ValueError,),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;).<a href="#ChatOpenAI-invoke">invoke</a>(1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;except&nbsp;ValueError:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;assert&nbsp;(count&nbsp;==&nbsp;2)<br>
&nbsp;<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;retry_if_exception_type:&nbsp;A&nbsp;tuple&nbsp;of&nbsp;exception&nbsp;types&nbsp;to&nbsp;retry&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;wait_exponential_jitter:&nbsp;Whether&nbsp;to&nbsp;add&nbsp;jitter&nbsp;to&nbsp;the&nbsp;wait&nbsp;time<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;between&nbsp;retries<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop_after_attempt:&nbsp;The&nbsp;maximum&nbsp;number&nbsp;of&nbsp;attempts&nbsp;to&nbsp;make&nbsp;before&nbsp;giving&nbsp;up<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;retries&nbsp;the&nbsp;original&nbsp;Runnable&nbsp;on&nbsp;exceptions.</tt></dd></dl>

<dl><dt><a name="ChatOpenAI-with_types"><strong>with_types</strong></a>(self, *, input_type: 'Optional[type[Input]]' = None, output_type: 'Optional[type[Output]]' = None) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_type:&nbsp;The&nbsp;input&nbsp;type&nbsp;to&nbsp;bind&nbsp;to&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_type:&nbsp;The&nbsp;output&nbsp;type&nbsp;to&nbsp;bind&nbsp;to&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;types&nbsp;bound.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a>:<br>
<dl><dt><strong>config_specs</strong></dt>
<dd><tt>List&nbsp;configurable&nbsp;fields&nbsp;for&nbsp;this&nbsp;Runnable.</tt></dd>
</dl>
<dl><dt><strong>input_schema</strong></dt>
<dd><tt>The&nbsp;type&nbsp;of&nbsp;input&nbsp;this&nbsp;Runnable&nbsp;accepts&nbsp;specified&nbsp;as&nbsp;a&nbsp;pydantic&nbsp;model.</tt></dd>
</dl>
<dl><dt><strong>output_schema</strong></dt>
<dd><tt>The&nbsp;type&nbsp;of&nbsp;output&nbsp;this&nbsp;Runnable&nbsp;produces&nbsp;specified&nbsp;as&nbsp;a&nbsp;pydantic&nbsp;model.</tt></dd>
</dl>
<hr>
Class methods inherited from <a href="typing.html#Generic">typing.Generic</a>:<br>
<dl><dt><a name="ChatOpenAI-__init_subclass__"><strong>__init_subclass__</strong></a>(*args, **kwargs)<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>This&nbsp;method&nbsp;is&nbsp;called&nbsp;when&nbsp;a&nbsp;class&nbsp;is&nbsp;subclassed.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;does&nbsp;nothing.&nbsp;It&nbsp;may&nbsp;be<br>
overridden&nbsp;to&nbsp;extend&nbsp;subclasses.</tt></dd></dl>

</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="OpenAI">class <strong>OpenAI</strong></a>(<a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>, <a href="langchain_openai.llms.base.html#OpenAI">langchain_openai.llms.base.OpenAI</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt><a href="#OpenAI">OpenAI</a>(*args,&nbsp;name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;cache:&nbsp;Union[langchain_core.caches.BaseCache,&nbsp;bool,&nbsp;NoneType]&nbsp;=&nbsp;None,&nbsp;verbose:&nbsp;bool&nbsp;=&nbsp;&amp;lt;factory&amp;gt;,&nbsp;callbacks:&nbsp;Union[list[langchain_core.callbacks.base.BaseCallbackHandler],&nbsp;langchain_core.callbacks.base.BaseCallbackManager,&nbsp;NoneType]&nbsp;=&nbsp;None,&nbsp;tags:&nbsp;Optional[list[str]]&nbsp;=&nbsp;None,&nbsp;metadata:&nbsp;Optional[dict[str,&nbsp;Any]]&nbsp;=&nbsp;None,&nbsp;custom_get_token_ids:&nbsp;Optional[Callable[[str],&nbsp;list[int]]]&nbsp;=&nbsp;None,&nbsp;callback_manager:&nbsp;Optional[langchain_core.callbacks.base.BaseCallbackManager]&nbsp;=&nbsp;None,&nbsp;client:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;async_client:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;model_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;temperature:&nbsp;float&nbsp;=&nbsp;0.7,&nbsp;max_tokens:&nbsp;int&nbsp;=&nbsp;256,&nbsp;top_p:&nbsp;float&nbsp;=&nbsp;1,&nbsp;frequency_penalty:&nbsp;float&nbsp;=&nbsp;0,&nbsp;presence_penalty:&nbsp;float&nbsp;=&nbsp;0,&nbsp;n:&nbsp;int&nbsp;=&nbsp;1,&nbsp;best_of:&nbsp;int&nbsp;=&nbsp;1,&nbsp;model_kwargs:&nbsp;Dict[str,&nbsp;Any]&nbsp;=&nbsp;&amp;lt;factory&amp;gt;,&nbsp;api_key:&nbsp;Optional[pydantic.types.SecretStr]&nbsp;=&nbsp;&amp;lt;factory&amp;gt;,&nbsp;base_url:&nbsp;Optional[str]&nbsp;=&nbsp;&amp;lt;factory&amp;gt;,&nbsp;organization:&nbsp;Optional[str]&nbsp;=&nbsp;&amp;lt;factory&amp;gt;,&nbsp;openai_proxy:&nbsp;Optional[str]&nbsp;=&nbsp;&amp;lt;factory&amp;gt;,&nbsp;batch_size:&nbsp;int&nbsp;=&nbsp;20,&nbsp;timeout:&nbsp;Union[float,&nbsp;Tuple[float,&nbsp;float],&nbsp;Any,&nbsp;NoneType]&nbsp;=&nbsp;None,&nbsp;logit_bias:&nbsp;Optional[Dict[str,&nbsp;float]]&nbsp;=&nbsp;None,&nbsp;max_retries:&nbsp;int&nbsp;=&nbsp;2,&nbsp;seed:&nbsp;Optional[int]&nbsp;=&nbsp;None,&nbsp;logprobs:&nbsp;Optional[int]&nbsp;=&nbsp;None,&nbsp;streaming:&nbsp;bool&nbsp;=&nbsp;False,&nbsp;allowed_special:&nbsp;Union[Literal['all'],&nbsp;AbstractSet[str]]&nbsp;=&nbsp;set(),&nbsp;disallowed_special:&nbsp;Union[Literal['all'],&nbsp;Collection[str]]&nbsp;=&nbsp;'all',&nbsp;tiktoken_model_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;default_headers:&nbsp;Optional[Mapping[str,&nbsp;str]]&nbsp;=&nbsp;None,&nbsp;default_query:&nbsp;Optional[Mapping[str,&nbsp;object]]&nbsp;=&nbsp;None,&nbsp;http_client:&nbsp;Optional[Any]&nbsp;=&nbsp;None,&nbsp;http_async_client:&nbsp;Optional[Any]&nbsp;=&nbsp;None,&nbsp;extra_body:&nbsp;Optional[Mapping[str,&nbsp;Any]]&nbsp;=&nbsp;None,&nbsp;proxy_client:&nbsp;Optional[Any]&nbsp;=&nbsp;None,&nbsp;deployment_id:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;config_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;config_id:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;proxy_model_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;api_version:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;**kwargs)&nbsp;-&amp;gt;&nbsp;None<br>
&nbsp;<br>
<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="gen_ai_hub.proxy.langchain.openai.html#OpenAI">OpenAI</a></dd>
<dd><a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a></dd>
<dd><a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a></dd>
<dd><a href="langchain_openai.llms.base.html#OpenAI">langchain_openai.llms.base.OpenAI</a></dd>
<dd><a href="langchain_openai.llms.base.html#BaseOpenAI">langchain_openai.llms.base.BaseOpenAI</a></dd>
<dd><a href="langchain_core.language_models.llms.html#BaseLLM">langchain_core.language_models.llms.BaseLLM</a></dd>
<dd><a href="langchain_core.language_models.base.html#BaseLanguageModel[str]">langchain_core.language_models.base.BaseLanguageModel[str]</a></dd>
<dd><a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a></dd>
<dd><a href="langchain_core.runnables.base.html#RunnableSerializable[Union[PromptValue, str, Sequence[Union[BaseMessage, list[str], tuple[str, str], str, dict[str, Any]]]], ~LanguageModelOutputVar]">langchain_core.runnables.base.RunnableSerializable[Union[PromptValue, str, Sequence[Union[BaseMessage, list[str], tuple[str, str], str, dict[str, Any]]]], ~LanguageModelOutputVar]</a></dd>
<dd><a href="langchain_core.runnables.base.html#RunnableSerializable">langchain_core.runnables.base.RunnableSerializable</a></dd>
<dd><a href="langchain_core.load.serializable.html#Serializable">langchain_core.load.serializable.Serializable</a></dd>
<dd><a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a></dd>
<dd><a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a></dd>
<dd><a href="typing.html#Generic">typing.Generic</a></dd>
<dd><a href="abc.html#ABC">abc.ABC</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="OpenAI-__init__"><strong>__init__</strong></a>(self, *args, **kwargs)</dt><dd><tt>Create&nbsp;a&nbsp;new&nbsp;model&nbsp;by&nbsp;parsing&nbsp;and&nbsp;validating&nbsp;input&nbsp;data&nbsp;from&nbsp;keyword&nbsp;arguments.<br>
&nbsp;<br>
Raises&nbsp;[`ValidationError`][pydantic_core.ValidationError]&nbsp;if&nbsp;the&nbsp;input&nbsp;data&nbsp;cannot&nbsp;be<br>
validated&nbsp;to&nbsp;form&nbsp;a&nbsp;valid&nbsp;model.<br>
&nbsp;<br>
`self`&nbsp;is&nbsp;explicitly&nbsp;positional-only&nbsp;to&nbsp;allow&nbsp;`self`&nbsp;as&nbsp;a&nbsp;field&nbsp;name.</tt></dd></dl>

<hr>
Class methods defined here:<br>
<dl><dt><a name="OpenAI-validate_environment"><strong>validate_environment</strong></a>(values: Dict) -&gt; Dict<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Validates&nbsp;the&nbsp;environment.<br>
&nbsp;<br>
:param&nbsp;values:&nbsp;The&nbsp;input&nbsp;values<br>
:type&nbsp;values:&nbsp;Dict<br>
:return:&nbsp;The&nbsp;validated&nbsp;values<br>
:rtype:&nbsp;Dict</tt></dd></dl>

<hr>
Static methods defined here:<br>
<dl><dt><a name="OpenAI-__new__"><strong>__new__</strong></a>(cls, **data: Any)</dt><dd><tt>Initialize&nbsp;the&nbsp;<a href="#OpenAI">OpenAI</a>&nbsp;object.</tt></dd></dl>

<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>__abstractmethods__</strong> = frozenset()</dl>

<dl><dt><strong>__annotations__</strong> = {'model_name': typing.Optional[str], 'openai_api_version': typing.Optional[str]}</dl>

<dl><dt><strong>__class_vars__</strong> = set()</dl>

<dl><dt><strong>__parameters__</strong> = ()</dl>

<dl><dt><strong>__private_attributes__</strong> = {}</dl>

<dl><dt><strong>__pydantic_complete__</strong> = True</dl>

<dl><dt><strong>__pydantic_computed_fields__</strong> = {}</dl>

<dl><dt><strong>__pydantic_core_schema__</strong> = {'cls': &lt;class 'gen_ai_hub.proxy.langchain.openai.OpenAI'&gt;, 'config': {'extra_fields_behavior': 'allow', 'populate_by_name': True, 'title': 'OpenAI'}, 'custom_init': True, 'metadata': {'pydantic_js_functions': [&lt;bound method BaseModel.__get_pydantic_json_sche...lass 'gen_ai_hub.proxy.langchain.openai.OpenAI'&gt;&gt;]}, 'ref': 'gen_ai_hub.proxy.langchain.openai.OpenAI:140673046533696', 'root_model': False, 'schema': {'function': {'function': &lt;bound method OpenAI.validate_environment of &lt;class 'gen_ai_hub.proxy.langchain.openai.OpenAI'&gt;&gt;, 'type': 'no-info'}, 'schema': {'function': {'function': &lt;bound method BaseOpenAI.build_extra of &lt;class 'gen_ai_hub.proxy.langchain.openai.OpenAI'&gt;&gt;, 'type': 'no-info'}, 'schema': {'function': {'function': &lt;bound method BaseLLM.raise_deprecation of &lt;class 'gen_ai_hub.proxy.langchain.openai.OpenAI'&gt;&gt;, 'type': 'no-info'}, 'schema': {'computed_fields': [], 'fields': {'allowed_special': {...}, 'async_client': {...}, 'batch_size': {...}, 'best_of': {...}, 'cache': {...}, 'callback_manager': {...}, 'callbacks': {...}, 'client': {...}, 'config_id': {...}, 'config_name': {...}, ...}, 'model_name': 'OpenAI', 'type': 'model-fields'}, 'type': 'function-before'}, 'type': 'function-before'}, 'type': 'function-before'}, 'type': 'model'}</dl>

<dl><dt><strong>__pydantic_custom_init__</strong> = True</dl>

<dl><dt><strong>__pydantic_decorators__</strong> = DecoratorInfos(validators={}, field_validators={...coratorInfo(mode='before'))}, computed_fields={})</dl>

<dl><dt><strong>__pydantic_fields__</strong> = {'allowed_special': FieldInfo(annotation=Union[Literal['all'], AbstractSet[str]], required=False, default=set()), 'async_client': FieldInfo(annotation=Any, required=False, default=None, exclude=True), 'batch_size': FieldInfo(annotation=int, required=False, default=20), 'best_of': FieldInfo(annotation=int, required=False, default=1), 'cache': FieldInfo(annotation=Union[BaseCache, bool, NoneType], required=False, default=None, exclude=True), 'callback_manager': FieldInfo(annotation=Union[BaseCallbackManager, ...ype], required=False, default=None, exclude=True), 'callbacks': FieldInfo(annotation=Union[list[BaseCallbackHand...ype], required=False, default=None, exclude=True), 'client': FieldInfo(annotation=Any, required=False, default=None, exclude=True), 'config_id': FieldInfo(annotation=Union[str, NoneType], required=False, default=None), 'config_name': FieldInfo(annotation=Union[str, NoneType], required=False, default=None), ...}</dl>

<dl><dt><strong>__pydantic_generic_metadata__</strong> = {'args': (), 'origin': None, 'parameters': ()}</dl>

<dl><dt><strong>__pydantic_parent_namespace__</strong> = None</dl>

<dl><dt><strong>__pydantic_post_init__</strong> = None</dl>

<dl><dt><strong>__pydantic_serializer__</strong> = SchemaSerializer(serializer=Model(
    ModelSeri...        name: "OpenAI",
    },
), definitions=[])</dl>

<dl><dt><strong>__pydantic_validator__</strong> = SchemaValidator(title="OpenAI", validator=Model(...I",
    },
), definitions=[], cache_strings=True)</dl>

<dl><dt><strong>__signature__</strong> = &lt;Signature (*args, name: Optional[str] = None, c...version: Optional[str] = None, **kwargs) -&gt; None&gt;</dl>

<dl><dt><strong>model_config</strong> = {'arbitrary_types_allowed': True, 'extra': 'allow', 'populate_by_name': True, 'protected_namespaces': ()}</dl>

<hr>
Class methods inherited from <a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>:<br>
<dl><dt><a name="OpenAI-validate_clients"><strong>validate_clients</strong></a>(values: Dict) -&gt; Dict<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<hr>
Data descriptors inherited from <a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a>:<br>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Class methods inherited from <a href="langchain_openai.llms.base.html#OpenAI">langchain_openai.llms.base.OpenAI</a>:<br>
<dl><dt><a name="OpenAI-get_lc_namespace"><strong>get_lc_namespace</strong></a>() -&gt; 'List[str]'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Get&nbsp;the&nbsp;namespace&nbsp;of&nbsp;the&nbsp;langchain&nbsp;object.</tt></dd></dl>

<dl><dt><a name="OpenAI-is_lc_serializable"><strong>is_lc_serializable</strong></a>() -&gt; 'bool'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Return&nbsp;whether&nbsp;this&nbsp;model&nbsp;can&nbsp;be&nbsp;serialized&nbsp;by&nbsp;Langchain.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_openai.llms.base.html#OpenAI">langchain_openai.llms.base.OpenAI</a>:<br>
<dl><dt><strong>lc_attributes</strong></dt>
<dd><tt>List&nbsp;of&nbsp;attribute&nbsp;names&nbsp;that&nbsp;should&nbsp;be&nbsp;included&nbsp;in&nbsp;the&nbsp;serialized&nbsp;kwargs.<br>
&nbsp;<br>
These&nbsp;attributes&nbsp;must&nbsp;be&nbsp;accepted&nbsp;by&nbsp;the&nbsp;constructor.<br>
Default&nbsp;is&nbsp;an&nbsp;empty&nbsp;dictionary.</tt></dd>
</dl>
<dl><dt><strong>lc_secrets</strong></dt>
<dd><tt>A&nbsp;map&nbsp;of&nbsp;constructor&nbsp;argument&nbsp;names&nbsp;to&nbsp;secret&nbsp;ids.<br>
&nbsp;<br>
For&nbsp;example,<br>
&nbsp;&nbsp;&nbsp;&nbsp;{"openai_api_key":&nbsp;"OPENAI_API_KEY"}</tt></dd>
</dl>
<hr>
Methods inherited from <a href="langchain_openai.llms.base.html#BaseOpenAI">langchain_openai.llms.base.BaseOpenAI</a>:<br>
<dl><dt><a name="OpenAI-create_llm_result"><strong>create_llm_result</strong></a>(self, choices: 'Any', prompts: 'List[str]', params: 'Dict[str, Any]', token_usage: 'Dict[str, int]', *, system_fingerprint: 'Optional[str]' = None) -&gt; 'LLMResult'</dt><dd><tt>Create&nbsp;the&nbsp;LLMResult&nbsp;from&nbsp;the&nbsp;choices&nbsp;and&nbsp;prompts.</tt></dd></dl>

<dl><dt><a name="OpenAI-get_sub_prompts"><strong>get_sub_prompts</strong></a>(self, params: 'Dict[str, Any]', prompts: 'List[str]', stop: 'Optional[List[str]]' = None) -&gt; 'List[List[str]]'</dt><dd><tt>Get&nbsp;the&nbsp;sub&nbsp;prompts&nbsp;for&nbsp;llm&nbsp;call.</tt></dd></dl>

<dl><dt><a name="OpenAI-get_token_ids"><strong>get_token_ids</strong></a>(self, text: 'str') -&gt; 'List[int]'</dt><dd><tt>Get&nbsp;the&nbsp;token&nbsp;IDs&nbsp;using&nbsp;the&nbsp;tiktoken&nbsp;package.</tt></dd></dl>

<dl><dt><a name="OpenAI-max_tokens_for_prompt"><strong>max_tokens_for_prompt</strong></a>(self, prompt: 'str') -&gt; 'int'</dt><dd><tt>Calculate&nbsp;the&nbsp;maximum&nbsp;number&nbsp;of&nbsp;tokens&nbsp;possible&nbsp;to&nbsp;generate&nbsp;for&nbsp;a&nbsp;prompt.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompt:&nbsp;The&nbsp;prompt&nbsp;to&nbsp;pass&nbsp;into&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;maximum&nbsp;number&nbsp;of&nbsp;tokens&nbsp;to&nbsp;generate&nbsp;for&nbsp;a&nbsp;prompt.<br>
&nbsp;<br>
Example:<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_tokens&nbsp;=&nbsp;openai.<a href="#OpenAI-max_tokens_for_prompt">max_tokens_for_prompt</a>("Tell&nbsp;me&nbsp;a&nbsp;joke.")</tt></dd></dl>

<hr>
Class methods inherited from <a href="langchain_openai.llms.base.html#BaseOpenAI">langchain_openai.llms.base.BaseOpenAI</a>:<br>
<dl><dt><a name="OpenAI-build_extra"><strong>build_extra</strong></a>(values: 'Dict[str, Any]') -&gt; 'Any'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Build&nbsp;extra&nbsp;kwargs&nbsp;from&nbsp;additional&nbsp;params&nbsp;that&nbsp;were&nbsp;passed&nbsp;in.</tt></dd></dl>

<hr>
Static methods inherited from <a href="langchain_openai.llms.base.html#BaseOpenAI">langchain_openai.llms.base.BaseOpenAI</a>:<br>
<dl><dt><a name="OpenAI-modelname_to_contextsize"><strong>modelname_to_contextsize</strong></a>(modelname: 'str') -&gt; 'int'</dt><dd><tt>Calculate&nbsp;the&nbsp;maximum&nbsp;number&nbsp;of&nbsp;tokens&nbsp;possible&nbsp;to&nbsp;generate&nbsp;for&nbsp;a&nbsp;model.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;modelname:&nbsp;The&nbsp;modelname&nbsp;we&nbsp;want&nbsp;to&nbsp;know&nbsp;the&nbsp;context&nbsp;size&nbsp;for.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;maximum&nbsp;context&nbsp;size<br>
&nbsp;<br>
Example:<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_tokens&nbsp;=&nbsp;openai.<a href="#OpenAI-modelname_to_contextsize">modelname_to_contextsize</a>("gpt-3.5-turbo-instruct")</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_openai.llms.base.html#BaseOpenAI">langchain_openai.llms.base.BaseOpenAI</a>:<br>
<dl><dt><strong>max_context_size</strong></dt>
<dd><tt>Get&nbsp;max&nbsp;context&nbsp;size&nbsp;for&nbsp;this&nbsp;model.</tt></dd>
</dl>
<hr>
Methods inherited from <a href="langchain_core.language_models.llms.html#BaseLLM">langchain_core.language_models.llms.BaseLLM</a>:<br>
<dl><dt><a name="OpenAI-__call__"><strong>__call__</strong></a>(self, prompt: 'str', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[list[str]]' = None, metadata: 'Optional[dict[str, Any]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>..&nbsp;deprecated::&nbsp;0.1.7&nbsp;Use&nbsp;:meth:`~invoke`&nbsp;instead.&nbsp;It&nbsp;will&nbsp;not&nbsp;be&nbsp;removed&nbsp;until&nbsp;langchain-core==1.0.<br>
&nbsp;<br>
Check&nbsp;Cache&nbsp;and&nbsp;run&nbsp;the&nbsp;LLM&nbsp;on&nbsp;the&nbsp;given&nbsp;prompt&nbsp;and&nbsp;input.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompt:&nbsp;The&nbsp;prompt&nbsp;to&nbsp;generate&nbsp;from.<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;tags:&nbsp;List&nbsp;of&nbsp;tags&nbsp;to&nbsp;associate&nbsp;with&nbsp;the&nbsp;prompt.<br>
&nbsp;&nbsp;&nbsp;&nbsp;metadata:&nbsp;Metadata&nbsp;to&nbsp;associate&nbsp;with&nbsp;the&nbsp;prompt.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;generated&nbsp;text.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ValueError:&nbsp;If&nbsp;the&nbsp;prompt&nbsp;is&nbsp;not&nbsp;a&nbsp;string.</tt></dd></dl>

<dl><dt><a name="OpenAI-__str__"><strong>__str__</strong></a>(self) -&gt; 'str'</dt><dd><tt>Get&nbsp;a&nbsp;string&nbsp;representation&nbsp;of&nbsp;the&nbsp;object&nbsp;for&nbsp;printing.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-abatch"><strong>abatch</strong></a>(self, inputs: 'list[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, list[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -&gt; 'list[str]'</dt><dd><tt>Default&nbsp;implementation&nbsp;runs&nbsp;ainvoke&nbsp;in&nbsp;parallel&nbsp;using&nbsp;asyncio.gather.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;of&nbsp;batch&nbsp;works&nbsp;well&nbsp;for&nbsp;IO&nbsp;bound&nbsp;runnables.<br>
&nbsp;<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;batch&nbsp;more&nbsp;efficiently;<br>
e.g.,&nbsp;if&nbsp;the&nbsp;underlying&nbsp;Runnable&nbsp;uses&nbsp;an&nbsp;API&nbsp;which&nbsp;supports&nbsp;a&nbsp;batch&nbsp;mode.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;inputs:&nbsp;A&nbsp;list&nbsp;of&nbsp;inputs&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;invoking&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;config&nbsp;supports&nbsp;standard&nbsp;keys&nbsp;like&nbsp;'tags',&nbsp;'metadata'&nbsp;for&nbsp;tracing<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;purposes,&nbsp;'max_concurrency'&nbsp;for&nbsp;controlling&nbsp;how&nbsp;much&nbsp;work&nbsp;to&nbsp;do<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;parallel,&nbsp;and&nbsp;other&nbsp;keys.&nbsp;Please&nbsp;refer&nbsp;to&nbsp;the&nbsp;RunnableConfig<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;more&nbsp;details.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;return_exceptions:&nbsp;Whether&nbsp;to&nbsp;return&nbsp;exceptions&nbsp;instead&nbsp;of&nbsp;raising&nbsp;them.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;False.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;list&nbsp;of&nbsp;outputs&nbsp;from&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-agenerate"><strong>agenerate</strong></a>(self, prompts: 'list[str]', stop: 'Optional[list[str]]' = None, callbacks: 'Optional[Union[Callbacks, list[Callbacks]]]' = None, *, tags: 'Optional[Union[list[str], list[list[str]]]]' = None, metadata: 'Optional[Union[dict[str, Any], list[dict[str, Any]]]]' = None, run_name: 'Optional[Union[str, list[str]]]' = None, run_id: 'Optional[Union[uuid.UUID, list[Optional[uuid.UUID]]]]' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Asynchronously&nbsp;pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;to&nbsp;a&nbsp;model&nbsp;and&nbsp;return&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompts:&nbsp;List&nbsp;of&nbsp;string&nbsp;prompts.<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;tags:&nbsp;List&nbsp;of&nbsp;tags&nbsp;to&nbsp;associate&nbsp;with&nbsp;each&nbsp;prompt.&nbsp;If&nbsp;provided,&nbsp;the&nbsp;length<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;list&nbsp;must&nbsp;match&nbsp;the&nbsp;length&nbsp;of&nbsp;the&nbsp;prompts&nbsp;list.<br>
&nbsp;&nbsp;&nbsp;&nbsp;metadata:&nbsp;List&nbsp;of&nbsp;metadata&nbsp;dictionaries&nbsp;to&nbsp;associate&nbsp;with&nbsp;each&nbsp;prompt.&nbsp;If<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;provided,&nbsp;the&nbsp;length&nbsp;of&nbsp;the&nbsp;list&nbsp;must&nbsp;match&nbsp;the&nbsp;length&nbsp;of&nbsp;the&nbsp;prompts<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;list.<br>
&nbsp;&nbsp;&nbsp;&nbsp;run_name:&nbsp;List&nbsp;of&nbsp;run&nbsp;names&nbsp;to&nbsp;associate&nbsp;with&nbsp;each&nbsp;prompt.&nbsp;If&nbsp;provided,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;length&nbsp;of&nbsp;the&nbsp;list&nbsp;must&nbsp;match&nbsp;the&nbsp;length&nbsp;of&nbsp;the&nbsp;prompts&nbsp;list.<br>
&nbsp;&nbsp;&nbsp;&nbsp;run_id:&nbsp;List&nbsp;of&nbsp;run&nbsp;IDs&nbsp;to&nbsp;associate&nbsp;with&nbsp;each&nbsp;prompt.&nbsp;If&nbsp;provided,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;length&nbsp;of&nbsp;the&nbsp;list&nbsp;must&nbsp;match&nbsp;the&nbsp;length&nbsp;of&nbsp;the&nbsp;prompts&nbsp;list.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-agenerate_prompt"><strong>agenerate_prompt</strong></a>(self, prompts: 'list[PromptValue]', stop: 'Optional[list[str]]' = None, callbacks: 'Optional[Union[Callbacks, list[Callbacks]]]' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Asynchronously&nbsp;pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;and&nbsp;return&nbsp;model&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompts:&nbsp;List&nbsp;of&nbsp;PromptValues.&nbsp;A&nbsp;PromptValue&nbsp;is&nbsp;an&nbsp;object&nbsp;that&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;converted&nbsp;to&nbsp;match&nbsp;the&nbsp;format&nbsp;of&nbsp;any&nbsp;language&nbsp;model&nbsp;(string&nbsp;for&nbsp;pure<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text&nbsp;generation&nbsp;models&nbsp;and&nbsp;BaseMessages&nbsp;for&nbsp;chat&nbsp;models).<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-ainvoke"><strong>ainvoke</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;ainvoke,&nbsp;calls&nbsp;invoke&nbsp;from&nbsp;a&nbsp;thread.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;allows&nbsp;usage&nbsp;of&nbsp;async&nbsp;code&nbsp;even&nbsp;if<br>
the&nbsp;Runnable&nbsp;did&nbsp;not&nbsp;implement&nbsp;a&nbsp;native&nbsp;async&nbsp;version&nbsp;of&nbsp;invoke.<br>
&nbsp;<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;run&nbsp;asynchronously.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-apredict"><strong>apredict</strong></a>(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>..&nbsp;deprecated::&nbsp;0.1.7&nbsp;Use&nbsp;:meth:`~ainvoke`&nbsp;instead.&nbsp;It&nbsp;will&nbsp;not&nbsp;be&nbsp;removed&nbsp;until&nbsp;langchain-core==1.0.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-apredict_messages"><strong>apredict_messages</strong></a>(self, messages: 'list[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>..&nbsp;deprecated::&nbsp;0.1.7&nbsp;Use&nbsp;:meth:`~ainvoke`&nbsp;instead.&nbsp;It&nbsp;will&nbsp;not&nbsp;be&nbsp;removed&nbsp;until&nbsp;langchain-core==1.0.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-astream"><strong>astream</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -&gt; 'AsyncIterator[str]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;astream,&nbsp;which&nbsp;calls&nbsp;ainvoke.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;support&nbsp;streaming&nbsp;output.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="OpenAI-batch"><strong>batch</strong></a>(self, inputs: 'list[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, list[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -&gt; 'list[str]'</dt><dd><tt>Default&nbsp;implementation&nbsp;runs&nbsp;invoke&nbsp;in&nbsp;parallel&nbsp;using&nbsp;a&nbsp;thread&nbsp;pool&nbsp;executor.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;of&nbsp;batch&nbsp;works&nbsp;well&nbsp;for&nbsp;IO&nbsp;bound&nbsp;runnables.<br>
&nbsp;<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;batch&nbsp;more&nbsp;efficiently;<br>
e.g.,&nbsp;if&nbsp;the&nbsp;underlying&nbsp;Runnable&nbsp;uses&nbsp;an&nbsp;API&nbsp;which&nbsp;supports&nbsp;a&nbsp;batch&nbsp;mode.</tt></dd></dl>

<dl><dt><a name="OpenAI-dict"><strong>dict</strong></a>(self, **kwargs: 'Any') -&gt; 'dict'</dt><dd><tt>Return&nbsp;a&nbsp;dictionary&nbsp;of&nbsp;the&nbsp;LLM.</tt></dd></dl>

<dl><dt><a name="OpenAI-generate"><strong>generate</strong></a>(self, prompts: 'list[str]', stop: 'Optional[list[str]]' = None, callbacks: 'Optional[Union[Callbacks, list[Callbacks]]]' = None, *, tags: 'Optional[Union[list[str], list[list[str]]]]' = None, metadata: 'Optional[Union[dict[str, Any], list[dict[str, Any]]]]' = None, run_name: 'Optional[Union[str, list[str]]]' = None, run_id: 'Optional[Union[uuid.UUID, list[Optional[uuid.UUID]]]]' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;to&nbsp;a&nbsp;model&nbsp;and&nbsp;return&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompts:&nbsp;List&nbsp;of&nbsp;string&nbsp;prompts.<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;tags:&nbsp;List&nbsp;of&nbsp;tags&nbsp;to&nbsp;associate&nbsp;with&nbsp;each&nbsp;prompt.&nbsp;If&nbsp;provided,&nbsp;the&nbsp;length<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;list&nbsp;must&nbsp;match&nbsp;the&nbsp;length&nbsp;of&nbsp;the&nbsp;prompts&nbsp;list.<br>
&nbsp;&nbsp;&nbsp;&nbsp;metadata:&nbsp;List&nbsp;of&nbsp;metadata&nbsp;dictionaries&nbsp;to&nbsp;associate&nbsp;with&nbsp;each&nbsp;prompt.&nbsp;If<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;provided,&nbsp;the&nbsp;length&nbsp;of&nbsp;the&nbsp;list&nbsp;must&nbsp;match&nbsp;the&nbsp;length&nbsp;of&nbsp;the&nbsp;prompts<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;list.<br>
&nbsp;&nbsp;&nbsp;&nbsp;run_name:&nbsp;List&nbsp;of&nbsp;run&nbsp;names&nbsp;to&nbsp;associate&nbsp;with&nbsp;each&nbsp;prompt.&nbsp;If&nbsp;provided,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;length&nbsp;of&nbsp;the&nbsp;list&nbsp;must&nbsp;match&nbsp;the&nbsp;length&nbsp;of&nbsp;the&nbsp;prompts&nbsp;list.<br>
&nbsp;&nbsp;&nbsp;&nbsp;run_id:&nbsp;List&nbsp;of&nbsp;run&nbsp;IDs&nbsp;to&nbsp;associate&nbsp;with&nbsp;each&nbsp;prompt.&nbsp;If&nbsp;provided,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;length&nbsp;of&nbsp;the&nbsp;list&nbsp;must&nbsp;match&nbsp;the&nbsp;length&nbsp;of&nbsp;the&nbsp;prompts&nbsp;list.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt><a name="OpenAI-generate_prompt"><strong>generate_prompt</strong></a>(self, prompts: 'list[PromptValue]', stop: 'Optional[list[str]]' = None, callbacks: 'Optional[Union[Callbacks, list[Callbacks]]]' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;to&nbsp;the&nbsp;model&nbsp;and&nbsp;return&nbsp;model&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompts:&nbsp;List&nbsp;of&nbsp;PromptValues.&nbsp;A&nbsp;PromptValue&nbsp;is&nbsp;an&nbsp;object&nbsp;that&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;converted&nbsp;to&nbsp;match&nbsp;the&nbsp;format&nbsp;of&nbsp;any&nbsp;language&nbsp;model&nbsp;(string&nbsp;for&nbsp;pure<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text&nbsp;generation&nbsp;models&nbsp;and&nbsp;BaseMessages&nbsp;for&nbsp;chat&nbsp;models).<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt><a name="OpenAI-invoke"><strong>invoke</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>Transform&nbsp;a&nbsp;single&nbsp;input&nbsp;into&nbsp;an&nbsp;output.&nbsp;Override&nbsp;to&nbsp;implement.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;invoking&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;config&nbsp;supports&nbsp;standard&nbsp;keys&nbsp;like&nbsp;'tags',&nbsp;'metadata'&nbsp;for&nbsp;tracing<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;purposes,&nbsp;'max_concurrency'&nbsp;for&nbsp;controlling&nbsp;how&nbsp;much&nbsp;work&nbsp;to&nbsp;do<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;parallel,&nbsp;and&nbsp;other&nbsp;keys.&nbsp;Please&nbsp;refer&nbsp;to&nbsp;the&nbsp;RunnableConfig<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;more&nbsp;details.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="OpenAI-predict"><strong>predict</strong></a>(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>..&nbsp;deprecated::&nbsp;0.1.7&nbsp;Use&nbsp;:meth:`~invoke`&nbsp;instead.&nbsp;It&nbsp;will&nbsp;not&nbsp;be&nbsp;removed&nbsp;until&nbsp;langchain-core==1.0.</tt></dd></dl>

<dl><dt><a name="OpenAI-predict_messages"><strong>predict_messages</strong></a>(self, messages: 'list[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>..&nbsp;deprecated::&nbsp;0.1.7&nbsp;Use&nbsp;:meth:`~invoke`&nbsp;instead.&nbsp;It&nbsp;will&nbsp;not&nbsp;be&nbsp;removed&nbsp;until&nbsp;langchain-core==1.0.</tt></dd></dl>

<dl><dt><a name="OpenAI-save"><strong>save</strong></a>(self, file_path: 'Union[Path, str]') -&gt; 'None'</dt><dd><tt>Save&nbsp;the&nbsp;LLM.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;file_path:&nbsp;Path&nbsp;to&nbsp;file&nbsp;to&nbsp;save&nbsp;the&nbsp;LLM&nbsp;to.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ValueError:&nbsp;If&nbsp;the&nbsp;file&nbsp;path&nbsp;is&nbsp;not&nbsp;a&nbsp;string&nbsp;or&nbsp;Path&nbsp;object.<br>
&nbsp;<br>
Example:<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;llm.<a href="#OpenAI-save">save</a>(file_path="path/llm.yaml")</tt></dd></dl>

<dl><dt><a name="OpenAI-stream"><strong>stream</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -&gt; 'Iterator[str]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;stream,&nbsp;which&nbsp;calls&nbsp;invoke.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;support&nbsp;streaming&nbsp;output.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<hr>
Class methods inherited from <a href="langchain_core.language_models.llms.html#BaseLLM">langchain_core.language_models.llms.BaseLLM</a>:<br>
<dl><dt><a name="OpenAI-raise_deprecation"><strong>raise_deprecation</strong></a>(values: 'dict') -&gt; 'Any'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Raise&nbsp;deprecation&nbsp;warning&nbsp;if&nbsp;callback_manager&nbsp;is&nbsp;used.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_core.language_models.llms.html#BaseLLM">langchain_core.language_models.llms.BaseLLM</a>:<br>
<dl><dt><strong>OutputType</strong></dt>
<dd><tt>Get&nbsp;the&nbsp;input&nbsp;type&nbsp;for&nbsp;this&nbsp;runnable.</tt></dd>
</dl>
<hr>
Methods inherited from <a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a>:<br>
<dl><dt><a name="OpenAI-get_num_tokens"><strong>get_num_tokens</strong></a>(self, text: 'str') -&gt; 'int'</dt><dd><tt>Get&nbsp;the&nbsp;number&nbsp;of&nbsp;tokens&nbsp;present&nbsp;in&nbsp;the&nbsp;text.<br>
&nbsp;<br>
Useful&nbsp;for&nbsp;checking&nbsp;if&nbsp;an&nbsp;input&nbsp;fits&nbsp;in&nbsp;a&nbsp;model's&nbsp;context&nbsp;window.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;text:&nbsp;The&nbsp;string&nbsp;input&nbsp;to&nbsp;tokenize.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;integer&nbsp;number&nbsp;of&nbsp;tokens&nbsp;in&nbsp;the&nbsp;text.</tt></dd></dl>

<dl><dt><a name="OpenAI-get_num_tokens_from_messages"><strong>get_num_tokens_from_messages</strong></a>(self, messages: 'list[BaseMessage]', tools: 'Optional[Sequence]' = None) -&gt; 'int'</dt><dd><tt>Get&nbsp;the&nbsp;number&nbsp;of&nbsp;tokens&nbsp;in&nbsp;the&nbsp;messages.<br>
&nbsp;<br>
Useful&nbsp;for&nbsp;checking&nbsp;if&nbsp;an&nbsp;input&nbsp;fits&nbsp;in&nbsp;a&nbsp;model's&nbsp;context&nbsp;window.<br>
&nbsp;<br>
**Note**:&nbsp;the&nbsp;base&nbsp;implementation&nbsp;of&nbsp;get_num_tokens_from_messages&nbsp;ignores<br>
tool&nbsp;schemas.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;messages:&nbsp;The&nbsp;message&nbsp;inputs&nbsp;to&nbsp;tokenize.<br>
&nbsp;&nbsp;&nbsp;&nbsp;tools:&nbsp;If&nbsp;provided,&nbsp;sequence&nbsp;of&nbsp;dict,&nbsp;BaseModel,&nbsp;function,&nbsp;or&nbsp;BaseTools<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;be&nbsp;converted&nbsp;to&nbsp;tool&nbsp;schemas.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;sum&nbsp;of&nbsp;the&nbsp;number&nbsp;of&nbsp;tokens&nbsp;across&nbsp;the&nbsp;messages.</tt></dd></dl>

<dl><dt><a name="OpenAI-with_structured_output"><strong>with_structured_output</strong></a>(self, schema: 'Union[dict, type]', **kwargs: 'Any') -&gt; 'Runnable[LanguageModelInput, Union[dict, BaseModel]]'</dt><dd><tt>Not&nbsp;implemented&nbsp;on&nbsp;this&nbsp;class.</tt></dd></dl>

<hr>
Class methods inherited from <a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a>:<br>
<dl><dt><a name="OpenAI-set_verbose"><strong>set_verbose</strong></a>(verbose: 'Optional[bool]') -&gt; 'bool'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>If&nbsp;verbose&nbsp;is&nbsp;None,&nbsp;set&nbsp;it.<br>
&nbsp;<br>
This&nbsp;allows&nbsp;users&nbsp;to&nbsp;pass&nbsp;in&nbsp;None&nbsp;as&nbsp;verbose&nbsp;to&nbsp;access&nbsp;the&nbsp;global&nbsp;setting.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;verbose:&nbsp;The&nbsp;verbosity&nbsp;setting&nbsp;to&nbsp;use.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;verbosity&nbsp;setting&nbsp;to&nbsp;use.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a>:<br>
<dl><dt><strong>InputType</strong></dt>
<dd><tt>Get&nbsp;the&nbsp;input&nbsp;type&nbsp;for&nbsp;this&nbsp;runnable.</tt></dd>
</dl>
<hr>
Methods inherited from <a href="langchain_core.runnables.base.html#RunnableSerializable">langchain_core.runnables.base.RunnableSerializable</a>:<br>
<dl><dt><a name="OpenAI-configurable_alternatives"><strong>configurable_alternatives</strong></a>(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -&gt; 'RunnableSerializable[Input, Output]'</dt><dd><tt>Configure&nbsp;alternatives&nbsp;for&nbsp;Runnables&nbsp;that&nbsp;can&nbsp;be&nbsp;set&nbsp;at&nbsp;runtime.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;which:&nbsp;The&nbsp;ConfigurableField&nbsp;instance&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;to&nbsp;select&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;alternative.<br>
&nbsp;&nbsp;&nbsp;&nbsp;default_key:&nbsp;The&nbsp;default&nbsp;key&nbsp;to&nbsp;use&nbsp;if&nbsp;no&nbsp;alternative&nbsp;is&nbsp;selected.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;"default".<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix_keys:&nbsp;Whether&nbsp;to&nbsp;prefix&nbsp;the&nbsp;keys&nbsp;with&nbsp;the&nbsp;ConfigurableField&nbsp;id.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;False.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;A&nbsp;dictionary&nbsp;of&nbsp;keys&nbsp;to&nbsp;Runnable&nbsp;instances&nbsp;or&nbsp;callables&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;Runnable&nbsp;instances.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;alternatives&nbsp;configured.<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_anthropic&nbsp;import&nbsp;ChatAnthropic<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables.utils&nbsp;import&nbsp;ConfigurableField<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_openai&nbsp;import&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a><br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;ChatAnthropic(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model_name="claude-3-sonnet-20240229"<br>
&nbsp;&nbsp;&nbsp;&nbsp;).<a href="#OpenAI-configurable_alternatives">configurable_alternatives</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ConfigurableField(id="llm"),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;default_key="anthropic",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;openai=<a href="#ChatOpenAI">ChatOpenAI</a>()<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;uses&nbsp;the&nbsp;default&nbsp;model&nbsp;ChatAnthropic<br>
&nbsp;&nbsp;&nbsp;&nbsp;print(model.<a href="#OpenAI-invoke">invoke</a>("which&nbsp;organization&nbsp;created&nbsp;you?").content)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;uses&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a><br>
&nbsp;&nbsp;&nbsp;&nbsp;print(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model.<a href="#OpenAI-with_config">with_config</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;configurable={"llm":&nbsp;"openai"}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;).<a href="#OpenAI-invoke">invoke</a>("which&nbsp;organization&nbsp;created&nbsp;you?").content<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</tt></dd></dl>

<dl><dt><a name="OpenAI-configurable_fields"><strong>configurable_fields</strong></a>(self, **kwargs: 'AnyConfigurableField') -&gt; 'RunnableSerializable[Input, Output]'</dt><dd><tt>Configure&nbsp;particular&nbsp;Runnable&nbsp;fields&nbsp;at&nbsp;runtime.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;A&nbsp;dictionary&nbsp;of&nbsp;ConfigurableField&nbsp;instances&nbsp;to&nbsp;configure.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;fields&nbsp;configured.<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;ConfigurableField<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_openai&nbsp;import&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a><br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;<a href="#ChatOpenAI">ChatOpenAI</a>(max_tokens=20).<a href="#OpenAI-configurable_fields">configurable_fields</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_tokens=ConfigurableField(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;id="output_token_number",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name="Max&nbsp;tokens&nbsp;in&nbsp;the&nbsp;output",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;description="The&nbsp;maximum&nbsp;number&nbsp;of&nbsp;tokens&nbsp;in&nbsp;the&nbsp;output",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;max_tokens&nbsp;=&nbsp;20<br>
&nbsp;&nbsp;&nbsp;&nbsp;print(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"max_tokens_20:&nbsp;",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model.<a href="#OpenAI-invoke">invoke</a>("tell&nbsp;me&nbsp;something&nbsp;about&nbsp;chess").content<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;max_tokens&nbsp;=&nbsp;200<br>
&nbsp;&nbsp;&nbsp;&nbsp;print("max_tokens_200:&nbsp;",&nbsp;model.<a href="#OpenAI-with_config">with_config</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;configurable={"output_token_number":&nbsp;200}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;).<a href="#OpenAI-invoke">invoke</a>("tell&nbsp;me&nbsp;something&nbsp;about&nbsp;chess").content<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</tt></dd></dl>

<dl><dt><a name="OpenAI-to_json"><strong>to_json</strong></a>(self) -&gt; 'Union[SerializedConstructor, SerializedNotImplemented]'</dt><dd><tt>Serialize&nbsp;the&nbsp;Runnable&nbsp;to&nbsp;JSON.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;JSON-serializable&nbsp;representation&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<hr>
Data and other attributes inherited from <a href="langchain_core.runnables.base.html#RunnableSerializable">langchain_core.runnables.base.RunnableSerializable</a>:<br>
<dl><dt><strong>__orig_bases__</strong> = (&lt;class 'langchain_core.load.serializable.Serializable'&gt;, langchain_core.runnables.base.Runnable[-Input, +Output])</dl>

<hr>
Methods inherited from <a href="langchain_core.load.serializable.html#Serializable">langchain_core.load.serializable.Serializable</a>:<br>
<dl><dt><a name="OpenAI-__repr_args__"><strong>__repr_args__</strong></a>(self) -&gt; Any</dt></dl>

<dl><dt><a name="OpenAI-to_json_not_implemented"><strong>to_json_not_implemented</strong></a>(self) -&gt; langchain_core.load.serializable.SerializedNotImplemented</dt></dl>

<hr>
Class methods inherited from <a href="langchain_core.load.serializable.html#Serializable">langchain_core.load.serializable.Serializable</a>:<br>
<dl><dt><a name="OpenAI-lc_id"><strong>lc_id</strong></a>() -&gt; list[str]<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>A&nbsp;unique&nbsp;identifier&nbsp;for&nbsp;this&nbsp;class&nbsp;for&nbsp;serialization&nbsp;purposes.<br>
&nbsp;<br>
The&nbsp;unique&nbsp;identifier&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;strings&nbsp;that&nbsp;describes&nbsp;the&nbsp;path<br>
to&nbsp;the&nbsp;object.<br>
For&nbsp;example,&nbsp;for&nbsp;the&nbsp;class&nbsp;`langchain.llms.openai.<a href="#OpenAI">OpenAI</a>`,&nbsp;the&nbsp;id&nbsp;is<br>
["langchain",&nbsp;"llms",&nbsp;"openai",&nbsp;"<a href="#OpenAI">OpenAI</a>"].</tt></dd></dl>

<hr>
Methods inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><a name="OpenAI-__copy__"><strong>__copy__</strong></a>(self) -&gt; 'Self'</dt><dd><tt>Returns&nbsp;a&nbsp;shallow&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.</tt></dd></dl>

<dl><dt><a name="OpenAI-__deepcopy__"><strong>__deepcopy__</strong></a>(self, memo: 'dict[int, Any] | None' = None) -&gt; 'Self'</dt><dd><tt>Returns&nbsp;a&nbsp;deep&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.</tt></dd></dl>

<dl><dt><a name="OpenAI-__delattr__"><strong>__delattr__</strong></a>(self, item: 'str') -&gt; 'Any'</dt><dd><tt>Implement&nbsp;delattr(self,&nbsp;name).</tt></dd></dl>

<dl><dt><a name="OpenAI-__eq__"><strong>__eq__</strong></a>(self, other: 'Any') -&gt; 'bool'</dt><dd><tt>Return&nbsp;self==value.</tt></dd></dl>

<dl><dt><a name="OpenAI-__getattr__"><strong>__getattr__</strong></a>(self, item: 'str') -&gt; 'Any'</dt></dl>

<dl><dt><a name="OpenAI-__getstate__"><strong>__getstate__</strong></a>(self) -&gt; 'dict[Any, Any]'</dt></dl>

<dl><dt><a name="OpenAI-__iter__"><strong>__iter__</strong></a>(self) -&gt; 'TupleGenerator'</dt><dd><tt>So&nbsp;`<a href="#OpenAI-dict">dict</a>(model)`&nbsp;works.</tt></dd></dl>

<dl><dt><a name="OpenAI-__pretty__"><strong>__pretty__</strong></a>(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -&gt; 'typing.Generator[Any, None, None]'</dt><dd><tt>Used&nbsp;by&nbsp;devtools&nbsp;(<a href="https://python-devtools.helpmanual.io/">https://python-devtools.helpmanual.io/</a>)&nbsp;to&nbsp;pretty&nbsp;print&nbsp;objects.</tt></dd></dl>

<dl><dt><a name="OpenAI-__replace__"><strong>__replace__</strong></a>(self, **changes: 'Any') -&gt; 'Self'</dt><dd><tt>#&nbsp;Because&nbsp;we&nbsp;make&nbsp;use&nbsp;of&nbsp;`@dataclass_transform()`,&nbsp;`__replace__`&nbsp;is&nbsp;already&nbsp;synthesized&nbsp;by<br>
#&nbsp;type&nbsp;checkers,&nbsp;so&nbsp;we&nbsp;define&nbsp;the&nbsp;implementation&nbsp;in&nbsp;this&nbsp;`if&nbsp;not&nbsp;TYPE_CHECKING:`&nbsp;block:</tt></dd></dl>

<dl><dt><a name="OpenAI-__repr__"><strong>__repr__</strong></a>(self) -&gt; 'str'</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="OpenAI-__repr_name__"><strong>__repr_name__</strong></a>(self) -&gt; 'str'</dt><dd><tt>Name&nbsp;of&nbsp;the&nbsp;instance's&nbsp;class,&nbsp;used&nbsp;in&nbsp;__repr__.</tt></dd></dl>

<dl><dt><a name="OpenAI-__repr_recursion__"><strong>__repr_recursion__</strong></a>(self, object: 'Any') -&gt; 'str'</dt><dd><tt>Returns&nbsp;the&nbsp;string&nbsp;representation&nbsp;of&nbsp;a&nbsp;recursive&nbsp;object.</tt></dd></dl>

<dl><dt><a name="OpenAI-__repr_str__"><strong>__repr_str__</strong></a>(self, join_str: 'str') -&gt; 'str'</dt></dl>

<dl><dt><a name="OpenAI-__rich_repr__"><strong>__rich_repr__</strong></a>(self) -&gt; 'RichReprResult'</dt><dd><tt>Used&nbsp;by&nbsp;Rich&nbsp;(<a href="https://rich.readthedocs.io/en/stable/pretty.html">https://rich.readthedocs.io/en/stable/pretty.html</a>)&nbsp;to&nbsp;pretty&nbsp;print&nbsp;objects.</tt></dd></dl>

<dl><dt><a name="OpenAI-__setattr__"><strong>__setattr__</strong></a>(self, name: 'str', value: 'Any') -&gt; 'None'</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="OpenAI-__setstate__"><strong>__setstate__</strong></a>(self, state: 'dict[Any, Any]') -&gt; 'None'</dt></dl>

<dl><dt><a name="OpenAI-copy"><strong>copy</strong></a>(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -&gt; 'Self'</dt><dd><tt>Returns&nbsp;a&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.<br>
&nbsp;<br>
!!!&nbsp;warning&nbsp;"Deprecated"<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;is&nbsp;now&nbsp;deprecated;&nbsp;use&nbsp;`model_copy`&nbsp;instead.<br>
&nbsp;<br>
If&nbsp;you&nbsp;need&nbsp;`include`&nbsp;or&nbsp;`exclude`,&nbsp;use:<br>
&nbsp;<br>
```python&nbsp;{test="skip"&nbsp;lint="skip"}<br>
data&nbsp;=&nbsp;self.<a href="#OpenAI-model_dump">model_dump</a>(include=include,&nbsp;exclude=exclude,&nbsp;round_trip=True)<br>
data&nbsp;=&nbsp;{**data,&nbsp;**(update&nbsp;or&nbsp;{})}<br>
copied&nbsp;=&nbsp;self.<a href="#OpenAI-model_validate">model_validate</a>(data)<br>
```<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;Optional&nbsp;set&nbsp;or&nbsp;mapping&nbsp;specifying&nbsp;which&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;copied&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude:&nbsp;Optional&nbsp;set&nbsp;or&nbsp;mapping&nbsp;specifying&nbsp;which&nbsp;fields&nbsp;to&nbsp;exclude&nbsp;in&nbsp;the&nbsp;copied&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;update:&nbsp;Optional&nbsp;dictionary&nbsp;of&nbsp;field-value&nbsp;pairs&nbsp;to&nbsp;override&nbsp;field&nbsp;values&nbsp;in&nbsp;the&nbsp;copied&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;deep:&nbsp;If&nbsp;True,&nbsp;the&nbsp;values&nbsp;of&nbsp;fields&nbsp;that&nbsp;are&nbsp;Pydantic&nbsp;models&nbsp;will&nbsp;be&nbsp;deep-copied.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;copy&nbsp;of&nbsp;the&nbsp;model&nbsp;with&nbsp;included,&nbsp;excluded&nbsp;and&nbsp;updated&nbsp;fields&nbsp;as&nbsp;specified.</tt></dd></dl>

<dl><dt><a name="OpenAI-json"><strong>json</strong></a>(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -&gt; 'str'</dt></dl>

<dl><dt><a name="OpenAI-model_copy"><strong>model_copy</strong></a>(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -&gt; 'Self'</dt><dd><tt>Usage&nbsp;docs:&nbsp;<a href="https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy">https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy</a><br>
&nbsp;<br>
Returns&nbsp;a&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;update:&nbsp;Values&nbsp;to&nbsp;change/add&nbsp;in&nbsp;the&nbsp;new&nbsp;model.&nbsp;Note:&nbsp;the&nbsp;data&nbsp;is&nbsp;not&nbsp;validated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;before&nbsp;creating&nbsp;the&nbsp;new&nbsp;model.&nbsp;You&nbsp;should&nbsp;trust&nbsp;this&nbsp;data.<br>
&nbsp;&nbsp;&nbsp;&nbsp;deep:&nbsp;Set&nbsp;to&nbsp;`True`&nbsp;to&nbsp;make&nbsp;a&nbsp;deep&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;New&nbsp;model&nbsp;instance.</tt></dd></dl>

<dl><dt><a name="OpenAI-model_dump"><strong>model_dump</strong></a>(self, *, mode: "Literal['json', 'python'] | str" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: "bool | Literal['none', 'warn', 'error']" = True, serialize_as_any: 'bool' = False) -&gt; 'dict[str, Any]'</dt><dd><tt>Usage&nbsp;docs:&nbsp;<a href="https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump">https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump</a><br>
&nbsp;<br>
Generate&nbsp;a&nbsp;dictionary&nbsp;representation&nbsp;of&nbsp;the&nbsp;model,&nbsp;optionally&nbsp;specifying&nbsp;which&nbsp;fields&nbsp;to&nbsp;include&nbsp;or&nbsp;exclude.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode:&nbsp;The&nbsp;mode&nbsp;in&nbsp;which&nbsp;`to_python`&nbsp;should&nbsp;run.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;mode&nbsp;is&nbsp;'json',&nbsp;the&nbsp;output&nbsp;will&nbsp;only&nbsp;contain&nbsp;JSON&nbsp;serializable&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;mode&nbsp;is&nbsp;'python',&nbsp;the&nbsp;output&nbsp;may&nbsp;contain&nbsp;non-JSON-serializable&nbsp;Python&nbsp;objects.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;A&nbsp;set&nbsp;of&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;output.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude:&nbsp;A&nbsp;set&nbsp;of&nbsp;fields&nbsp;to&nbsp;exclude&nbsp;from&nbsp;the&nbsp;output.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Additional&nbsp;context&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;serializer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;by_alias:&nbsp;Whether&nbsp;to&nbsp;use&nbsp;the&nbsp;field's&nbsp;alias&nbsp;in&nbsp;the&nbsp;dictionary&nbsp;key&nbsp;if&nbsp;defined.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_unset:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;have&nbsp;not&nbsp;been&nbsp;explicitly&nbsp;set.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_defaults:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;are&nbsp;set&nbsp;to&nbsp;their&nbsp;default&nbsp;value.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_none:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;have&nbsp;a&nbsp;value&nbsp;of&nbsp;`None`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;round_trip:&nbsp;If&nbsp;True,&nbsp;dumped&nbsp;values&nbsp;should&nbsp;be&nbsp;valid&nbsp;as&nbsp;input&nbsp;for&nbsp;non-idempotent&nbsp;types&nbsp;such&nbsp;as&nbsp;Json[T].<br>
&nbsp;&nbsp;&nbsp;&nbsp;warnings:&nbsp;How&nbsp;to&nbsp;handle&nbsp;serialization&nbsp;errors.&nbsp;False/"none"&nbsp;ignores&nbsp;them,&nbsp;True/"warn"&nbsp;logs&nbsp;errors,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"error"&nbsp;raises&nbsp;a&nbsp;[`PydanticSerializationError`][pydantic_core.PydanticSerializationError].<br>
&nbsp;&nbsp;&nbsp;&nbsp;serialize_as_any:&nbsp;Whether&nbsp;to&nbsp;serialize&nbsp;fields&nbsp;with&nbsp;duck-typing&nbsp;serialization&nbsp;behavior.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;dictionary&nbsp;representation&nbsp;of&nbsp;the&nbsp;model.</tt></dd></dl>

<dl><dt><a name="OpenAI-model_dump_json"><strong>model_dump_json</strong></a>(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: "bool | Literal['none', 'warn', 'error']" = True, serialize_as_any: 'bool' = False) -&gt; 'str'</dt><dd><tt>Usage&nbsp;docs:&nbsp;<a href="https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json">https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json</a><br>
&nbsp;<br>
Generates&nbsp;a&nbsp;JSON&nbsp;representation&nbsp;of&nbsp;the&nbsp;model&nbsp;using&nbsp;Pydantic's&nbsp;`to_json`&nbsp;method.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;indent:&nbsp;Indentation&nbsp;to&nbsp;use&nbsp;in&nbsp;the&nbsp;JSON&nbsp;output.&nbsp;If&nbsp;None&nbsp;is&nbsp;passed,&nbsp;the&nbsp;output&nbsp;will&nbsp;be&nbsp;compact.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;Field(s)&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;JSON&nbsp;output.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude:&nbsp;Field(s)&nbsp;to&nbsp;exclude&nbsp;from&nbsp;the&nbsp;JSON&nbsp;output.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Additional&nbsp;context&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;serializer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;by_alias:&nbsp;Whether&nbsp;to&nbsp;serialize&nbsp;using&nbsp;field&nbsp;aliases.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_unset:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;have&nbsp;not&nbsp;been&nbsp;explicitly&nbsp;set.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_defaults:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;are&nbsp;set&nbsp;to&nbsp;their&nbsp;default&nbsp;value.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_none:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;have&nbsp;a&nbsp;value&nbsp;of&nbsp;`None`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;round_trip:&nbsp;If&nbsp;True,&nbsp;dumped&nbsp;values&nbsp;should&nbsp;be&nbsp;valid&nbsp;as&nbsp;input&nbsp;for&nbsp;non-idempotent&nbsp;types&nbsp;such&nbsp;as&nbsp;Json[T].<br>
&nbsp;&nbsp;&nbsp;&nbsp;warnings:&nbsp;How&nbsp;to&nbsp;handle&nbsp;serialization&nbsp;errors.&nbsp;False/"none"&nbsp;ignores&nbsp;them,&nbsp;True/"warn"&nbsp;logs&nbsp;errors,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"error"&nbsp;raises&nbsp;a&nbsp;[`PydanticSerializationError`][pydantic_core.PydanticSerializationError].<br>
&nbsp;&nbsp;&nbsp;&nbsp;serialize_as_any:&nbsp;Whether&nbsp;to&nbsp;serialize&nbsp;fields&nbsp;with&nbsp;duck-typing&nbsp;serialization&nbsp;behavior.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;JSON&nbsp;string&nbsp;representation&nbsp;of&nbsp;the&nbsp;model.</tt></dd></dl>

<dl><dt><a name="OpenAI-model_post_init"><strong>model_post_init</strong></a>(self, _BaseModel__context: 'Any') -&gt; 'None'</dt><dd><tt>Override&nbsp;this&nbsp;method&nbsp;to&nbsp;perform&nbsp;additional&nbsp;initialization&nbsp;after&nbsp;`__init__`&nbsp;and&nbsp;`model_construct`.<br>
This&nbsp;is&nbsp;useful&nbsp;if&nbsp;you&nbsp;want&nbsp;to&nbsp;do&nbsp;some&nbsp;validation&nbsp;that&nbsp;requires&nbsp;the&nbsp;entire&nbsp;model&nbsp;to&nbsp;be&nbsp;initialized.</tt></dd></dl>

<hr>
Class methods inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><a name="OpenAI-__class_getitem__"><strong>__class_getitem__</strong></a>(typevar_values: 'type[Any] | tuple[type[Any], ...]') -&gt; 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAI-__get_pydantic_core_schema__"><strong>__get_pydantic_core_schema__</strong></a>(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -&gt; 'CoreSchema'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Hook&nbsp;into&nbsp;generating&nbsp;the&nbsp;model's&nbsp;CoreSchema.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;source:&nbsp;The&nbsp;class&nbsp;we&nbsp;are&nbsp;generating&nbsp;a&nbsp;schema&nbsp;for.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;will&nbsp;generally&nbsp;be&nbsp;the&nbsp;same&nbsp;as&nbsp;the&nbsp;`cls`&nbsp;argument&nbsp;if&nbsp;this&nbsp;is&nbsp;a&nbsp;classmethod.<br>
&nbsp;&nbsp;&nbsp;&nbsp;handler:&nbsp;A&nbsp;callable&nbsp;that&nbsp;calls&nbsp;into&nbsp;Pydantic's&nbsp;internal&nbsp;CoreSchema&nbsp;generation&nbsp;logic.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;`pydantic-core`&nbsp;`CoreSchema`.</tt></dd></dl>

<dl><dt><a name="OpenAI-__get_pydantic_json_schema__"><strong>__get_pydantic_json_schema__</strong></a>(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -&gt; 'JsonSchemaValue'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Hook&nbsp;into&nbsp;generating&nbsp;the&nbsp;model's&nbsp;JSON&nbsp;schema.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;core_schema:&nbsp;A&nbsp;`pydantic-core`&nbsp;CoreSchema.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You&nbsp;can&nbsp;ignore&nbsp;this&nbsp;argument&nbsp;and&nbsp;call&nbsp;the&nbsp;handler&nbsp;with&nbsp;a&nbsp;new&nbsp;CoreSchema,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;wrap&nbsp;this&nbsp;CoreSchema&nbsp;(`{'type':&nbsp;'nullable',&nbsp;'schema':&nbsp;current_schema}`),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;just&nbsp;call&nbsp;the&nbsp;handler&nbsp;with&nbsp;the&nbsp;original&nbsp;schema.<br>
&nbsp;&nbsp;&nbsp;&nbsp;handler:&nbsp;Call&nbsp;into&nbsp;Pydantic's&nbsp;internal&nbsp;JSON&nbsp;schema&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;will&nbsp;raise&nbsp;a&nbsp;`pydantic.errors.PydanticInvalidForJsonSchema`&nbsp;if&nbsp;JSON&nbsp;schema<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;generation&nbsp;fails.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Since&nbsp;this&nbsp;gets&nbsp;called&nbsp;by&nbsp;`BaseModel.model_json_schema`&nbsp;you&nbsp;can&nbsp;override&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`schema_generator`&nbsp;argument&nbsp;to&nbsp;that&nbsp;function&nbsp;to&nbsp;change&nbsp;JSON&nbsp;schema&nbsp;generation&nbsp;globally<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;a&nbsp;type.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;JSON&nbsp;schema,&nbsp;as&nbsp;a&nbsp;Python&nbsp;object.</tt></dd></dl>

<dl><dt><a name="OpenAI-__pydantic_init_subclass__"><strong>__pydantic_init_subclass__</strong></a>(**kwargs: 'Any') -&gt; 'None'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>This&nbsp;is&nbsp;intended&nbsp;to&nbsp;behave&nbsp;just&nbsp;like&nbsp;`__init_subclass__`,&nbsp;but&nbsp;is&nbsp;called&nbsp;by&nbsp;`ModelMetaclass`<br>
only&nbsp;after&nbsp;the&nbsp;class&nbsp;is&nbsp;actually&nbsp;fully&nbsp;initialized.&nbsp;In&nbsp;particular,&nbsp;attributes&nbsp;like&nbsp;`model_fields`&nbsp;will<br>
be&nbsp;present&nbsp;when&nbsp;this&nbsp;is&nbsp;called.<br>
&nbsp;<br>
This&nbsp;is&nbsp;necessary&nbsp;because&nbsp;`__init_subclass__`&nbsp;will&nbsp;always&nbsp;be&nbsp;called&nbsp;by&nbsp;`type.__new__`,<br>
and&nbsp;it&nbsp;would&nbsp;require&nbsp;a&nbsp;prohibitively&nbsp;large&nbsp;refactor&nbsp;to&nbsp;the&nbsp;`ModelMetaclass`&nbsp;to&nbsp;ensure&nbsp;that<br>
`type.__new__`&nbsp;was&nbsp;called&nbsp;in&nbsp;such&nbsp;a&nbsp;manner&nbsp;that&nbsp;the&nbsp;class&nbsp;would&nbsp;already&nbsp;be&nbsp;sufficiently&nbsp;initialized.<br>
&nbsp;<br>
This&nbsp;will&nbsp;receive&nbsp;the&nbsp;same&nbsp;`kwargs`&nbsp;that&nbsp;would&nbsp;be&nbsp;passed&nbsp;to&nbsp;the&nbsp;standard&nbsp;`__init_subclass__`,&nbsp;namely,<br>
any&nbsp;kwargs&nbsp;passed&nbsp;to&nbsp;the&nbsp;class&nbsp;definition&nbsp;that&nbsp;aren't&nbsp;used&nbsp;internally&nbsp;by&nbsp;pydantic.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Any&nbsp;keyword&nbsp;arguments&nbsp;passed&nbsp;to&nbsp;the&nbsp;class&nbsp;definition&nbsp;that&nbsp;aren't&nbsp;used&nbsp;internally<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;pydantic.</tt></dd></dl>

<dl><dt><a name="OpenAI-construct"><strong>construct</strong></a>(_fields_set: 'set[str] | None' = None, **values: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAI-from_orm"><strong>from_orm</strong></a>(obj: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAI-model_construct"><strong>model_construct</strong></a>(_fields_set: 'set[str] | None' = None, **values: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Creates&nbsp;a&nbsp;new&nbsp;instance&nbsp;of&nbsp;the&nbsp;`Model`&nbsp;class&nbsp;with&nbsp;validated&nbsp;data.<br>
&nbsp;<br>
Creates&nbsp;a&nbsp;new&nbsp;model&nbsp;setting&nbsp;`__dict__`&nbsp;and&nbsp;`__pydantic_fields_set__`&nbsp;from&nbsp;trusted&nbsp;or&nbsp;pre-validated&nbsp;data.<br>
Default&nbsp;values&nbsp;are&nbsp;respected,&nbsp;but&nbsp;no&nbsp;other&nbsp;validation&nbsp;is&nbsp;performed.<br>
&nbsp;<br>
!!!&nbsp;note<br>
&nbsp;&nbsp;&nbsp;&nbsp;`<a href="#OpenAI-model_construct">model_construct</a>()`&nbsp;generally&nbsp;respects&nbsp;the&nbsp;`model_config.extra`&nbsp;setting&nbsp;on&nbsp;the&nbsp;provided&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;That&nbsp;is,&nbsp;if&nbsp;`model_config.extra&nbsp;==&nbsp;'allow'`,&nbsp;then&nbsp;all&nbsp;extra&nbsp;passed&nbsp;values&nbsp;are&nbsp;added&nbsp;to&nbsp;the&nbsp;model&nbsp;instance's&nbsp;`__dict__`<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;`__pydantic_extra__`&nbsp;fields.&nbsp;If&nbsp;`model_config.extra&nbsp;==&nbsp;'ignore'`&nbsp;(the&nbsp;default),&nbsp;then&nbsp;all&nbsp;extra&nbsp;passed&nbsp;values&nbsp;are&nbsp;ignored.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Because&nbsp;no&nbsp;validation&nbsp;is&nbsp;performed&nbsp;with&nbsp;a&nbsp;call&nbsp;to&nbsp;`<a href="#OpenAI-model_construct">model_construct</a>()`,&nbsp;having&nbsp;`model_config.extra&nbsp;==&nbsp;'forbid'`&nbsp;does&nbsp;not&nbsp;result&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;an&nbsp;error&nbsp;if&nbsp;extra&nbsp;values&nbsp;are&nbsp;passed,&nbsp;but&nbsp;they&nbsp;will&nbsp;be&nbsp;ignored.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;_fields_set:&nbsp;A&nbsp;set&nbsp;of&nbsp;field&nbsp;names&nbsp;that&nbsp;were&nbsp;originally&nbsp;explicitly&nbsp;set&nbsp;during&nbsp;instantiation.&nbsp;If&nbsp;provided,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;is&nbsp;directly&nbsp;used&nbsp;for&nbsp;the&nbsp;[`model_fields_set`][pydantic.BaseModel.model_fields_set]&nbsp;attribute.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Otherwise,&nbsp;the&nbsp;field&nbsp;names&nbsp;from&nbsp;the&nbsp;`values`&nbsp;argument&nbsp;will&nbsp;be&nbsp;used.<br>
&nbsp;&nbsp;&nbsp;&nbsp;values:&nbsp;Trusted&nbsp;or&nbsp;pre-validated&nbsp;data&nbsp;dictionary.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;instance&nbsp;of&nbsp;the&nbsp;`Model`&nbsp;class&nbsp;with&nbsp;validated&nbsp;data.</tt></dd></dl>

<dl><dt><a name="OpenAI-model_json_schema"><strong>model_json_schema</strong></a>(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = &lt;class 'pydantic.json_schema.GenerateJsonSchema'&gt;, mode: 'JsonSchemaMode' = 'validation') -&gt; 'dict[str, Any]'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Generates&nbsp;a&nbsp;JSON&nbsp;schema&nbsp;for&nbsp;a&nbsp;model&nbsp;class.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;by_alias:&nbsp;Whether&nbsp;to&nbsp;use&nbsp;attribute&nbsp;aliases&nbsp;or&nbsp;not.<br>
&nbsp;&nbsp;&nbsp;&nbsp;ref_template:&nbsp;The&nbsp;reference&nbsp;template.<br>
&nbsp;&nbsp;&nbsp;&nbsp;schema_generator:&nbsp;To&nbsp;override&nbsp;the&nbsp;logic&nbsp;used&nbsp;to&nbsp;generate&nbsp;the&nbsp;JSON&nbsp;schema,&nbsp;as&nbsp;a&nbsp;subclass&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`GenerateJsonSchema`&nbsp;with&nbsp;your&nbsp;desired&nbsp;modifications<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode:&nbsp;The&nbsp;mode&nbsp;in&nbsp;which&nbsp;to&nbsp;generate&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;JSON&nbsp;schema&nbsp;for&nbsp;the&nbsp;given&nbsp;model&nbsp;class.</tt></dd></dl>

<dl><dt><a name="OpenAI-model_parametrized_name"><strong>model_parametrized_name</strong></a>(params: 'tuple[type[Any], ...]') -&gt; 'str'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Compute&nbsp;the&nbsp;class&nbsp;name&nbsp;for&nbsp;parametrizations&nbsp;of&nbsp;generic&nbsp;classes.<br>
&nbsp;<br>
This&nbsp;method&nbsp;can&nbsp;be&nbsp;overridden&nbsp;to&nbsp;achieve&nbsp;a&nbsp;custom&nbsp;naming&nbsp;scheme&nbsp;for&nbsp;generic&nbsp;BaseModels.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;params:&nbsp;Tuple&nbsp;of&nbsp;types&nbsp;of&nbsp;the&nbsp;class.&nbsp;Given&nbsp;a&nbsp;generic&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`Model`&nbsp;with&nbsp;2&nbsp;type&nbsp;variables&nbsp;and&nbsp;a&nbsp;concrete&nbsp;model&nbsp;`Model[str,&nbsp;int]`,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;value&nbsp;`(str,&nbsp;int)`&nbsp;would&nbsp;be&nbsp;passed&nbsp;to&nbsp;`params`.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;String&nbsp;representing&nbsp;the&nbsp;new&nbsp;class&nbsp;where&nbsp;`params`&nbsp;are&nbsp;passed&nbsp;to&nbsp;`cls`&nbsp;as&nbsp;type&nbsp;variables.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;TypeError:&nbsp;Raised&nbsp;when&nbsp;trying&nbsp;to&nbsp;generate&nbsp;concrete&nbsp;names&nbsp;for&nbsp;non-generic&nbsp;models.</tt></dd></dl>

<dl><dt><a name="OpenAI-model_rebuild"><strong>model_rebuild</strong></a>(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -&gt; 'bool | None'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Try&nbsp;to&nbsp;rebuild&nbsp;the&nbsp;pydantic-core&nbsp;schema&nbsp;for&nbsp;the&nbsp;model.<br>
&nbsp;<br>
This&nbsp;may&nbsp;be&nbsp;necessary&nbsp;when&nbsp;one&nbsp;of&nbsp;the&nbsp;annotations&nbsp;is&nbsp;a&nbsp;ForwardRef&nbsp;which&nbsp;could&nbsp;not&nbsp;be&nbsp;resolved&nbsp;during<br>
the&nbsp;initial&nbsp;attempt&nbsp;to&nbsp;build&nbsp;the&nbsp;schema,&nbsp;and&nbsp;automatic&nbsp;rebuilding&nbsp;fails.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;force:&nbsp;Whether&nbsp;to&nbsp;force&nbsp;the&nbsp;rebuilding&nbsp;of&nbsp;the&nbsp;model&nbsp;schema,&nbsp;defaults&nbsp;to&nbsp;`False`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;raise_errors:&nbsp;Whether&nbsp;to&nbsp;raise&nbsp;errors,&nbsp;defaults&nbsp;to&nbsp;`True`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;_parent_namespace_depth:&nbsp;The&nbsp;depth&nbsp;level&nbsp;of&nbsp;the&nbsp;parent&nbsp;namespace,&nbsp;defaults&nbsp;to&nbsp;2.<br>
&nbsp;&nbsp;&nbsp;&nbsp;_types_namespace:&nbsp;The&nbsp;types&nbsp;namespace,&nbsp;defaults&nbsp;to&nbsp;`None`.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Returns&nbsp;`None`&nbsp;if&nbsp;the&nbsp;schema&nbsp;is&nbsp;already&nbsp;"complete"&nbsp;and&nbsp;rebuilding&nbsp;was&nbsp;not&nbsp;required.<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;rebuilding&nbsp;_was_&nbsp;required,&nbsp;returns&nbsp;`True`&nbsp;if&nbsp;rebuilding&nbsp;was&nbsp;successful,&nbsp;otherwise&nbsp;`False`.</tt></dd></dl>

<dl><dt><a name="OpenAI-model_validate"><strong>model_validate</strong></a>(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Validate&nbsp;a&nbsp;pydantic&nbsp;model&nbsp;instance.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;obj:&nbsp;The&nbsp;object&nbsp;to&nbsp;validate.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;Whether&nbsp;to&nbsp;enforce&nbsp;types&nbsp;strictly.<br>
&nbsp;&nbsp;&nbsp;&nbsp;from_attributes:&nbsp;Whether&nbsp;to&nbsp;extract&nbsp;data&nbsp;from&nbsp;object&nbsp;attributes.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Additional&nbsp;context&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;validator.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ValidationError:&nbsp;If&nbsp;the&nbsp;object&nbsp;could&nbsp;not&nbsp;be&nbsp;validated.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;validated&nbsp;model&nbsp;instance.</tt></dd></dl>

<dl><dt><a name="OpenAI-model_validate_json"><strong>model_validate_json</strong></a>(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Usage&nbsp;docs:&nbsp;<a href="https://docs.pydantic.dev/2.10/concepts/json/#json-parsing">https://docs.pydantic.dev/2.10/concepts/json/#json-parsing</a><br>
&nbsp;<br>
Validate&nbsp;the&nbsp;given&nbsp;JSON&nbsp;data&nbsp;against&nbsp;the&nbsp;Pydantic&nbsp;model.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;json_data:&nbsp;The&nbsp;JSON&nbsp;data&nbsp;to&nbsp;validate.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;Whether&nbsp;to&nbsp;enforce&nbsp;types&nbsp;strictly.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Extra&nbsp;variables&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;validator.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;validated&nbsp;Pydantic&nbsp;model.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ValidationError:&nbsp;If&nbsp;`json_data`&nbsp;is&nbsp;not&nbsp;a&nbsp;JSON&nbsp;string&nbsp;or&nbsp;the&nbsp;object&nbsp;could&nbsp;not&nbsp;be&nbsp;validated.</tt></dd></dl>

<dl><dt><a name="OpenAI-model_validate_strings"><strong>model_validate_strings</strong></a>(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Validate&nbsp;the&nbsp;given&nbsp;object&nbsp;with&nbsp;string&nbsp;data&nbsp;against&nbsp;the&nbsp;Pydantic&nbsp;model.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;obj:&nbsp;The&nbsp;object&nbsp;containing&nbsp;string&nbsp;data&nbsp;to&nbsp;validate.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;Whether&nbsp;to&nbsp;enforce&nbsp;types&nbsp;strictly.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Extra&nbsp;variables&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;validator.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;validated&nbsp;Pydantic&nbsp;model.</tt></dd></dl>

<dl><dt><a name="OpenAI-parse_file"><strong>parse_file</strong></a>(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAI-parse_obj"><strong>parse_obj</strong></a>(obj: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAI-parse_raw"><strong>parse_raw</strong></a>(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAI-schema"><strong>schema</strong></a>(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -&gt; 'Dict[str, Any]'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAI-schema_json"><strong>schema_json</strong></a>(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -&gt; 'str'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAI-update_forward_refs"><strong>update_forward_refs</strong></a>(**localns: 'Any') -&gt; 'None'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAI-validate"><strong>validate</strong></a>(value: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<hr>
Readonly properties inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><strong>__fields_set__</strong></dt>
</dl>
<dl><dt><strong>model_computed_fields</strong></dt>
<dd><tt>Get&nbsp;metadata&nbsp;about&nbsp;the&nbsp;computed&nbsp;fields&nbsp;defined&nbsp;on&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Deprecation&nbsp;warning:&nbsp;you&nbsp;should&nbsp;be&nbsp;getting&nbsp;this&nbsp;information&nbsp;from&nbsp;the&nbsp;model&nbsp;class,&nbsp;not&nbsp;from&nbsp;an&nbsp;instance.<br>
In&nbsp;V3,&nbsp;this&nbsp;property&nbsp;will&nbsp;be&nbsp;removed&nbsp;from&nbsp;the&nbsp;`BaseModel`&nbsp;class.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;mapping&nbsp;of&nbsp;computed&nbsp;field&nbsp;names&nbsp;to&nbsp;[`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo]&nbsp;objects.</tt></dd>
</dl>
<dl><dt><strong>model_extra</strong></dt>
<dd><tt>Get&nbsp;extra&nbsp;fields&nbsp;set&nbsp;during&nbsp;validation.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;dictionary&nbsp;of&nbsp;extra&nbsp;fields,&nbsp;or&nbsp;`None`&nbsp;if&nbsp;`config.extra`&nbsp;is&nbsp;not&nbsp;set&nbsp;to&nbsp;`"allow"`.</tt></dd>
</dl>
<dl><dt><strong>model_fields</strong></dt>
<dd><tt>Get&nbsp;metadata&nbsp;about&nbsp;the&nbsp;fields&nbsp;defined&nbsp;on&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Deprecation&nbsp;warning:&nbsp;you&nbsp;should&nbsp;be&nbsp;getting&nbsp;this&nbsp;information&nbsp;from&nbsp;the&nbsp;model&nbsp;class,&nbsp;not&nbsp;from&nbsp;an&nbsp;instance.<br>
In&nbsp;V3,&nbsp;this&nbsp;property&nbsp;will&nbsp;be&nbsp;removed&nbsp;from&nbsp;the&nbsp;`BaseModel`&nbsp;class.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;mapping&nbsp;of&nbsp;field&nbsp;names&nbsp;to&nbsp;[`FieldInfo`][pydantic.fields.FieldInfo]&nbsp;objects.</tt></dd>
</dl>
<dl><dt><strong>model_fields_set</strong></dt>
<dd><tt>Returns&nbsp;the&nbsp;set&nbsp;of&nbsp;fields&nbsp;that&nbsp;have&nbsp;been&nbsp;explicitly&nbsp;set&nbsp;on&nbsp;this&nbsp;model&nbsp;instance.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;set&nbsp;of&nbsp;strings&nbsp;representing&nbsp;the&nbsp;fields&nbsp;that&nbsp;have&nbsp;been&nbsp;set,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i.e.&nbsp;that&nbsp;were&nbsp;not&nbsp;filled&nbsp;from&nbsp;defaults.</tt></dd>
</dl>
<hr>
Data descriptors inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__pydantic_extra__</strong></dt>
</dl>
<dl><dt><strong>__pydantic_fields_set__</strong></dt>
</dl>
<dl><dt><strong>__pydantic_private__</strong></dt>
</dl>
<hr>
Data and other attributes inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><strong>__hash__</strong> = None</dl>

<dl><dt><strong>__pydantic_root_model__</strong> = False</dl>

<hr>
Methods inherited from <a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a>:<br>
<dl><dt><a name="OpenAI-__or__"><strong>__or__</strong></a>(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -&gt; 'RunnableSerializable[Input, Other]'</dt><dd><tt>Compose&nbsp;this&nbsp;Runnable&nbsp;with&nbsp;another&nbsp;object&nbsp;to&nbsp;create&nbsp;a&nbsp;RunnableSequence.</tt></dd></dl>

<dl><dt><a name="OpenAI-__ror__"><strong>__ror__</strong></a>(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -&gt; 'RunnableSerializable[Other, Output]'</dt><dd><tt>Compose&nbsp;this&nbsp;Runnable&nbsp;with&nbsp;another&nbsp;object&nbsp;to&nbsp;create&nbsp;a&nbsp;RunnableSequence.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-abatch_as_completed"><strong>abatch_as_completed</strong></a>(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -&gt; 'AsyncIterator[tuple[int, Union[Output, Exception]]]'</dt><dd><tt>Run&nbsp;ainvoke&nbsp;in&nbsp;parallel&nbsp;on&nbsp;a&nbsp;list&nbsp;of&nbsp;inputs,<br>
yielding&nbsp;results&nbsp;as&nbsp;they&nbsp;complete.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;inputs:&nbsp;A&nbsp;list&nbsp;of&nbsp;inputs&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;invoking&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;config&nbsp;supports&nbsp;standard&nbsp;keys&nbsp;like&nbsp;'tags',&nbsp;'metadata'&nbsp;for&nbsp;tracing<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;purposes,&nbsp;'max_concurrency'&nbsp;for&nbsp;controlling&nbsp;how&nbsp;much&nbsp;work&nbsp;to&nbsp;do<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;parallel,&nbsp;and&nbsp;other&nbsp;keys.&nbsp;Please&nbsp;refer&nbsp;to&nbsp;the&nbsp;RunnableConfig<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;more&nbsp;details.&nbsp;Defaults&nbsp;to&nbsp;None.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;return_exceptions:&nbsp;Whether&nbsp;to&nbsp;return&nbsp;exceptions&nbsp;instead&nbsp;of&nbsp;raising&nbsp;them.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;False.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;tuple&nbsp;of&nbsp;the&nbsp;index&nbsp;of&nbsp;the&nbsp;input&nbsp;and&nbsp;the&nbsp;output&nbsp;from&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="OpenAI-as_tool"><strong>as_tool</strong></a>(self, args_schema: 'Optional[type[BaseModel]]' = None, *, name: 'Optional[str]' = None, description: 'Optional[str]' = None, arg_types: 'Optional[dict[str, type]]' = None) -&gt; 'BaseTool'</dt><dd><tt>..&nbsp;beta::<br>
&nbsp;&nbsp;&nbsp;This&nbsp;API&nbsp;is&nbsp;in&nbsp;beta&nbsp;and&nbsp;may&nbsp;change&nbsp;in&nbsp;the&nbsp;future.<br>
&nbsp;<br>
Create&nbsp;a&nbsp;BaseTool&nbsp;from&nbsp;a&nbsp;Runnable.<br>
&nbsp;<br>
``as_tool``&nbsp;will&nbsp;instantiate&nbsp;a&nbsp;BaseTool&nbsp;with&nbsp;a&nbsp;name,&nbsp;description,&nbsp;and<br>
``args_schema``&nbsp;from&nbsp;a&nbsp;Runnable.&nbsp;Where&nbsp;possible,&nbsp;schemas&nbsp;are&nbsp;inferred<br>
from&nbsp;``runnable.get_input_schema``.&nbsp;Alternatively&nbsp;(e.g.,&nbsp;if&nbsp;the<br>
Runnable&nbsp;takes&nbsp;a&nbsp;dict&nbsp;as&nbsp;input&nbsp;and&nbsp;the&nbsp;specific&nbsp;dict&nbsp;keys&nbsp;are&nbsp;not&nbsp;typed),<br>
the&nbsp;schema&nbsp;can&nbsp;be&nbsp;specified&nbsp;directly&nbsp;with&nbsp;``args_schema``.&nbsp;You&nbsp;can&nbsp;also<br>
pass&nbsp;``arg_types``&nbsp;to&nbsp;just&nbsp;specify&nbsp;the&nbsp;required&nbsp;arguments&nbsp;and&nbsp;their&nbsp;types.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;args_schema:&nbsp;The&nbsp;schema&nbsp;for&nbsp;the&nbsp;tool.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;name:&nbsp;The&nbsp;name&nbsp;of&nbsp;the&nbsp;tool.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;description:&nbsp;The&nbsp;description&nbsp;of&nbsp;the&nbsp;tool.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;arg_types:&nbsp;A&nbsp;dictionary&nbsp;of&nbsp;argument&nbsp;names&nbsp;to&nbsp;types.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;BaseTool&nbsp;instance.<br>
&nbsp;<br>
Typed&nbsp;dict&nbsp;input:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing&nbsp;import&nbsp;List<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing_extensions&nbsp;import&nbsp;TypedDict<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;Args(TypedDict):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b:&nbsp;List[int]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;f(x:&nbsp;Args)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;str(x["a"]&nbsp;*&nbsp;max(x["b"]))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(f)<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool&nbsp;=&nbsp;runnable.<a href="#OpenAI-as_tool">as_tool</a>()<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool.<a href="#OpenAI-invoke">invoke</a>({"a":&nbsp;3,&nbsp;"b":&nbsp;[1,&nbsp;2]})<br>
&nbsp;<br>
``dict``&nbsp;input,&nbsp;specifying&nbsp;schema&nbsp;via&nbsp;``args_schema``:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing&nbsp;import&nbsp;Any,&nbsp;Dict,&nbsp;List<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;pydantic&nbsp;import&nbsp;BaseModel,&nbsp;Field<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;f(x:&nbsp;Dict[str,&nbsp;Any])&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;str(x["a"]&nbsp;*&nbsp;max(x["b"]))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;FSchema(BaseModel):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Apply&nbsp;a&nbsp;function&nbsp;to&nbsp;an&nbsp;integer&nbsp;and&nbsp;list&nbsp;of&nbsp;integers."""<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a:&nbsp;int&nbsp;=&nbsp;Field(...,&nbsp;description="Integer")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b:&nbsp;List[int]&nbsp;=&nbsp;Field(...,&nbsp;description="List&nbsp;of&nbsp;ints")<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(f)<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool&nbsp;=&nbsp;runnable.<a href="#OpenAI-as_tool">as_tool</a>(FSchema)<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool.<a href="#OpenAI-invoke">invoke</a>({"a":&nbsp;3,&nbsp;"b":&nbsp;[1,&nbsp;2]})<br>
&nbsp;<br>
``dict``&nbsp;input,&nbsp;specifying&nbsp;schema&nbsp;via&nbsp;``arg_types``:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing&nbsp;import&nbsp;Any,&nbsp;Dict,&nbsp;List<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;f(x:&nbsp;Dict[str,&nbsp;Any])&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;str(x["a"]&nbsp;*&nbsp;max(x["b"]))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(f)<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool&nbsp;=&nbsp;runnable.<a href="#OpenAI-as_tool">as_tool</a>(arg_types={"a":&nbsp;int,&nbsp;"b":&nbsp;List[int]})<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool.<a href="#OpenAI-invoke">invoke</a>({"a":&nbsp;3,&nbsp;"b":&nbsp;[1,&nbsp;2]})<br>
&nbsp;<br>
String&nbsp;input:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;f(x:&nbsp;str)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;+&nbsp;"a"<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;g(x:&nbsp;str)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;+&nbsp;"z"<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(f)&nbsp;|&nbsp;g<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool&nbsp;=&nbsp;runnable.<a href="#OpenAI-as_tool">as_tool</a>()<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool.<a href="#OpenAI-invoke">invoke</a>("b")<br>
&nbsp;<br>
..&nbsp;versionadded::&nbsp;0.2.14</tt></dd></dl>

<dl><dt><a name="OpenAI-assign"><strong>assign</strong></a>(self, **kwargs: 'Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any], Mapping[str, Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any]]]]') -&gt; 'RunnableSerializable[Any, Any]'</dt><dd><tt>Assigns&nbsp;new&nbsp;fields&nbsp;to&nbsp;the&nbsp;dict&nbsp;output&nbsp;of&nbsp;this&nbsp;Runnable.<br>
Returns&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_community.llms.fake&nbsp;import&nbsp;FakeStreamingListLLM<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.output_parsers&nbsp;import&nbsp;StrOutputParser<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.prompts&nbsp;import&nbsp;SystemMessagePromptTemplate<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;Runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;operator&nbsp;import&nbsp;itemgetter<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;=&nbsp;(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SystemMessagePromptTemplate.from_template("You&nbsp;are&nbsp;a&nbsp;nice&nbsp;assistant.")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+&nbsp;"{question}"<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;llm&nbsp;=&nbsp;FakeStreamingListLLM(responses=["foo-lish"])<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain:&nbsp;Runnable&nbsp;=&nbsp;prompt&nbsp;|&nbsp;llm&nbsp;|&nbsp;{"str":&nbsp;StrOutputParser()}<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain_with_assign&nbsp;=&nbsp;chain.<a href="#OpenAI-assign">assign</a>(hello=itemgetter("str")&nbsp;|&nbsp;llm)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;print(chain_with_assign.input_schema.<a href="#OpenAI-model_json_schema">model_json_schema</a>())<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;{'title':&nbsp;'PromptInput',&nbsp;'type':&nbsp;'object',&nbsp;'properties':<br>
&nbsp;&nbsp;&nbsp;&nbsp;{'question':&nbsp;{'title':&nbsp;'Question',&nbsp;'type':&nbsp;'string'}}}<br>
&nbsp;&nbsp;&nbsp;&nbsp;print(chain_with_assign.output_schema.<a href="#OpenAI-model_json_schema">model_json_schema</a>())<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;{'title':&nbsp;'RunnableSequenceOutput',&nbsp;'type':&nbsp;'object',&nbsp;'properties':<br>
&nbsp;&nbsp;&nbsp;&nbsp;{'str':&nbsp;{'title':&nbsp;'Str',<br>
&nbsp;&nbsp;&nbsp;&nbsp;'type':&nbsp;'string'},&nbsp;'hello':&nbsp;{'title':&nbsp;'Hello',&nbsp;'type':&nbsp;'string'}}}</tt></dd></dl>

<dl><dt>async <a name="OpenAI-astream_events"><strong>astream_events</strong></a>(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: "Literal['v1', 'v2']" = 'v2', include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'AsyncIterator[StreamEvent]'</dt><dd><tt>Generate&nbsp;a&nbsp;stream&nbsp;of&nbsp;events.<br>
&nbsp;<br>
Use&nbsp;to&nbsp;create&nbsp;an&nbsp;iterator&nbsp;over&nbsp;StreamEvents&nbsp;that&nbsp;provide&nbsp;real-time&nbsp;information<br>
about&nbsp;the&nbsp;progress&nbsp;of&nbsp;the&nbsp;Runnable,&nbsp;including&nbsp;StreamEvents&nbsp;from&nbsp;intermediate<br>
results.<br>
&nbsp;<br>
A&nbsp;StreamEvent&nbsp;is&nbsp;a&nbsp;dictionary&nbsp;with&nbsp;the&nbsp;following&nbsp;schema:<br>
&nbsp;<br>
-&nbsp;``event``:&nbsp;**str**&nbsp;-&nbsp;Event&nbsp;names&nbsp;are&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;format:&nbsp;on_[runnable_type]_(start|stream|end).<br>
-&nbsp;``name``:&nbsp;**str**&nbsp;-&nbsp;The&nbsp;name&nbsp;of&nbsp;the&nbsp;Runnable&nbsp;that&nbsp;generated&nbsp;the&nbsp;event.<br>
-&nbsp;``run_id``:&nbsp;**str**&nbsp;-&nbsp;randomly&nbsp;generated&nbsp;ID&nbsp;associated&nbsp;with&nbsp;the&nbsp;given&nbsp;execution&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;Runnable&nbsp;that&nbsp;emitted&nbsp;the&nbsp;event.<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;child&nbsp;Runnable&nbsp;that&nbsp;gets&nbsp;invoked&nbsp;as&nbsp;part&nbsp;of&nbsp;the&nbsp;execution&nbsp;of&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;parent&nbsp;Runnable&nbsp;is&nbsp;assigned&nbsp;its&nbsp;own&nbsp;unique&nbsp;ID.<br>
-&nbsp;``parent_ids``:&nbsp;**List[str]**&nbsp;-&nbsp;The&nbsp;IDs&nbsp;of&nbsp;the&nbsp;parent&nbsp;runnables&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;generated&nbsp;the&nbsp;event.&nbsp;The&nbsp;root&nbsp;Runnable&nbsp;will&nbsp;have&nbsp;an&nbsp;empty&nbsp;list.<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;order&nbsp;of&nbsp;the&nbsp;parent&nbsp;IDs&nbsp;is&nbsp;from&nbsp;the&nbsp;root&nbsp;to&nbsp;the&nbsp;immediate&nbsp;parent.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Only&nbsp;available&nbsp;for&nbsp;v2&nbsp;version&nbsp;of&nbsp;the&nbsp;API.&nbsp;The&nbsp;v1&nbsp;version&nbsp;of&nbsp;the&nbsp;API<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;return&nbsp;an&nbsp;empty&nbsp;list.<br>
-&nbsp;``tags``:&nbsp;**Optional[List[str]]**&nbsp;-&nbsp;The&nbsp;tags&nbsp;of&nbsp;the&nbsp;Runnable&nbsp;that&nbsp;generated<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;event.<br>
-&nbsp;``metadata``:&nbsp;**Optional[Dict[str,&nbsp;Any]]**&nbsp;-&nbsp;The&nbsp;metadata&nbsp;of&nbsp;the&nbsp;Runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;generated&nbsp;the&nbsp;event.<br>
-&nbsp;``data``:&nbsp;**Dict[str,&nbsp;Any]**<br>
&nbsp;<br>
&nbsp;<br>
Below&nbsp;is&nbsp;a&nbsp;table&nbsp;that&nbsp;illustrates&nbsp;some&nbsp;events&nbsp;that&nbsp;might&nbsp;be&nbsp;emitted&nbsp;by&nbsp;various<br>
chains.&nbsp;Metadata&nbsp;fields&nbsp;have&nbsp;been&nbsp;omitted&nbsp;from&nbsp;the&nbsp;table&nbsp;for&nbsp;brevity.<br>
Chain&nbsp;definitions&nbsp;have&nbsp;been&nbsp;included&nbsp;after&nbsp;the&nbsp;table.<br>
&nbsp;<br>
**ATTENTION**&nbsp;This&nbsp;reference&nbsp;table&nbsp;is&nbsp;for&nbsp;the&nbsp;V2&nbsp;version&nbsp;of&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;event&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;chunk&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;input&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;output&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+======================+==================+=================================+===============================================+=================================================+<br>
|&nbsp;on_chat_model_start&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"messages":&nbsp;[[SystemMessage,&nbsp;HumanMessage]]}&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_chat_model_stream&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;AIMessageChunk(content="hello")&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_chat_model_end&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"messages":&nbsp;[[SystemMessage,&nbsp;HumanMessage]]}&nbsp;|&nbsp;AIMessageChunk(content="hello&nbsp;world")&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_llm_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{'input':&nbsp;'hello'}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_llm_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;'Hello'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_llm_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;'Hello&nbsp;human!'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_chain_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_chain_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;"hello&nbsp;world!,&nbsp;goodbye&nbsp;world!"&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_chain_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[Document(...)]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;"hello&nbsp;world!,&nbsp;goodbye&nbsp;world!"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_tool_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_tool_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_retriever_start&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"query":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_retriever_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"query":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[Document(...),&nbsp;..]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_prompt_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[template_name]&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"question":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_prompt_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[template_name]&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"question":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;ChatPromptValue(messages:&nbsp;[SystemMessage,&nbsp;...])&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
&nbsp;<br>
In&nbsp;addition&nbsp;to&nbsp;the&nbsp;standard&nbsp;events,&nbsp;users&nbsp;can&nbsp;also&nbsp;dispatch&nbsp;custom&nbsp;events&nbsp;(see&nbsp;example&nbsp;below).<br>
&nbsp;<br>
Custom&nbsp;events&nbsp;will&nbsp;be&nbsp;only&nbsp;be&nbsp;surfaced&nbsp;with&nbsp;in&nbsp;the&nbsp;`v2`&nbsp;version&nbsp;of&nbsp;the&nbsp;API!<br>
&nbsp;<br>
A&nbsp;custom&nbsp;event&nbsp;has&nbsp;following&nbsp;format:<br>
&nbsp;<br>
+-----------+------+-----------------------------------------------------------------------------------------------------------+<br>
|&nbsp;Attribute&nbsp;|&nbsp;Type&nbsp;|&nbsp;Description&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+===========+======+===========================================================================================================+<br>
|&nbsp;name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;str&nbsp;&nbsp;|&nbsp;A&nbsp;user&nbsp;defined&nbsp;name&nbsp;for&nbsp;the&nbsp;event.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+-----------+------+-----------------------------------------------------------------------------------------------------------+<br>
|&nbsp;data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;Any&nbsp;&nbsp;|&nbsp;The&nbsp;data&nbsp;associated&nbsp;with&nbsp;the&nbsp;event.&nbsp;This&nbsp;can&nbsp;be&nbsp;anything,&nbsp;though&nbsp;we&nbsp;suggest&nbsp;making&nbsp;it&nbsp;JSON&nbsp;serializable.&nbsp;&nbsp;|<br>
+-----------+------+-----------------------------------------------------------------------------------------------------------+<br>
&nbsp;<br>
Here&nbsp;are&nbsp;declarations&nbsp;associated&nbsp;with&nbsp;the&nbsp;standard&nbsp;events&nbsp;shown&nbsp;above:<br>
&nbsp;<br>
`format_docs`:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;format_docs(docs:&nbsp;List[Document])&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'''Format&nbsp;the&nbsp;docs.'''<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;",&nbsp;".join([doc.page_content&nbsp;for&nbsp;doc&nbsp;in&nbsp;docs])<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;format_docs&nbsp;=&nbsp;RunnableLambda(format_docs)<br>
&nbsp;<br>
`some_tool`:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;@tool<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;some_tool(x:&nbsp;int,&nbsp;y:&nbsp;str)&nbsp;-&gt;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'''Some_tool.'''<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;{"x":&nbsp;x,&nbsp;"y":&nbsp;y}<br>
&nbsp;<br>
`prompt`:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;template&nbsp;=&nbsp;ChatPromptTemplate.from_messages(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[("system",&nbsp;"You&nbsp;are&nbsp;Cat&nbsp;Agent&nbsp;007"),&nbsp;("human",&nbsp;"{question}")]<br>
&nbsp;&nbsp;&nbsp;&nbsp;).<a href="#OpenAI-with_config">with_config</a>({"run_name":&nbsp;"my_template",&nbsp;"tags":&nbsp;["my_template"]})<br>
&nbsp;<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;reverse(s:&nbsp;str)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;s[::-1]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;RunnableLambda(func=reverse)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;events&nbsp;=&nbsp;[<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;event&nbsp;async&nbsp;for&nbsp;event&nbsp;in&nbsp;chain.<a href="#OpenAI-astream_events">astream_events</a>("hello",&nbsp;version="v2")<br>
&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;will&nbsp;produce&nbsp;the&nbsp;following&nbsp;events&nbsp;(run_id,&nbsp;and&nbsp;parent_ids<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;has&nbsp;been&nbsp;omitted&nbsp;for&nbsp;brevity):<br>
&nbsp;&nbsp;&nbsp;&nbsp;[<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"input":&nbsp;"hello"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_start",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"chunk":&nbsp;"olleh"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_stream",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"output":&nbsp;"olleh"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_end",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;<br>
&nbsp;<br>
Example:&nbsp;Dispatch&nbsp;Custom&nbsp;Event<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.callbacks.manager&nbsp;import&nbsp;(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;adispatch_custom_event,<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda,&nbsp;RunnableConfig<br>
&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;asyncio<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;slow_thing(some_input:&nbsp;str,&nbsp;config:&nbsp;RunnableConfig)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Do&nbsp;something&nbsp;that&nbsp;takes&nbsp;a&nbsp;long&nbsp;time."""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(1)&nbsp;#&nbsp;Placeholder&nbsp;for&nbsp;some&nbsp;slow&nbsp;operation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;adispatch_custom_event(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"progress_event",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{"message":&nbsp;"Finished&nbsp;step&nbsp;1&nbsp;of&nbsp;3"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;config=config&nbsp;#&nbsp;Must&nbsp;be&nbsp;included&nbsp;for&nbsp;python&nbsp;&lt;&nbsp;3.10<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(1)&nbsp;#&nbsp;Placeholder&nbsp;for&nbsp;some&nbsp;slow&nbsp;operation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;adispatch_custom_event(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"progress_event",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{"message":&nbsp;"Finished&nbsp;step&nbsp;2&nbsp;of&nbsp;3"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;config=config&nbsp;#&nbsp;Must&nbsp;be&nbsp;included&nbsp;for&nbsp;python&nbsp;&lt;&nbsp;3.10<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(1)&nbsp;#&nbsp;Placeholder&nbsp;for&nbsp;some&nbsp;slow&nbsp;operation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;"Done"<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;slow_thing&nbsp;=&nbsp;RunnableLambda(slow_thing)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;for&nbsp;event&nbsp;in&nbsp;slow_thing.<a href="#OpenAI-astream_events">astream_events</a>("some_input",&nbsp;version="v2"):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(event)<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;version:&nbsp;The&nbsp;version&nbsp;of&nbsp;the&nbsp;schema&nbsp;to&nbsp;use&nbsp;either&nbsp;`v2`&nbsp;or&nbsp;`v1`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Users&nbsp;should&nbsp;use&nbsp;`v2`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`v1`&nbsp;is&nbsp;for&nbsp;backwards&nbsp;compatibility&nbsp;and&nbsp;will&nbsp;be&nbsp;deprecated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;0.4.0.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;No&nbsp;default&nbsp;will&nbsp;be&nbsp;assigned&nbsp;until&nbsp;the&nbsp;API&nbsp;is&nbsp;stabilized.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;custom&nbsp;events&nbsp;will&nbsp;only&nbsp;be&nbsp;surfaced&nbsp;in&nbsp;`v2`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_names:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_types:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_tags:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_names:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_types:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_tags:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;These&nbsp;will&nbsp;be&nbsp;passed&nbsp;to&nbsp;astream_log&nbsp;as&nbsp;this&nbsp;implementation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;astream_events&nbsp;is&nbsp;built&nbsp;on&nbsp;top&nbsp;of&nbsp;astream_log.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;async&nbsp;stream&nbsp;of&nbsp;StreamEvents.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;NotImplementedError:&nbsp;If&nbsp;the&nbsp;version&nbsp;is&nbsp;not&nbsp;`v1`&nbsp;or&nbsp;`v2`.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-astream_log"><strong>astream_log</strong></a>(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'</dt><dd><tt>Stream&nbsp;all&nbsp;output&nbsp;from&nbsp;a&nbsp;Runnable,&nbsp;as&nbsp;reported&nbsp;to&nbsp;the&nbsp;callback&nbsp;system.<br>
This&nbsp;includes&nbsp;all&nbsp;inner&nbsp;runs&nbsp;of&nbsp;LLMs,&nbsp;Retrievers,&nbsp;Tools,&nbsp;etc.<br>
&nbsp;<br>
Output&nbsp;is&nbsp;streamed&nbsp;as&nbsp;Log&nbsp;objects,&nbsp;which&nbsp;include&nbsp;a&nbsp;list&nbsp;of<br>
Jsonpatch&nbsp;ops&nbsp;that&nbsp;describe&nbsp;how&nbsp;the&nbsp;state&nbsp;of&nbsp;the&nbsp;run&nbsp;has&nbsp;changed&nbsp;in&nbsp;each<br>
step,&nbsp;and&nbsp;the&nbsp;final&nbsp;state&nbsp;of&nbsp;the&nbsp;run.<br>
&nbsp;<br>
The&nbsp;Jsonpatch&nbsp;ops&nbsp;can&nbsp;be&nbsp;applied&nbsp;in&nbsp;order&nbsp;to&nbsp;construct&nbsp;state.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;diff:&nbsp;Whether&nbsp;to&nbsp;yield&nbsp;diffs&nbsp;between&nbsp;each&nbsp;step&nbsp;or&nbsp;the&nbsp;current&nbsp;state.<br>
&nbsp;&nbsp;&nbsp;&nbsp;with_streamed_output_list:&nbsp;Whether&nbsp;to&nbsp;yield&nbsp;the&nbsp;streamed_output&nbsp;list.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_names:&nbsp;Only&nbsp;include&nbsp;logs&nbsp;with&nbsp;these&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_types:&nbsp;Only&nbsp;include&nbsp;logs&nbsp;with&nbsp;these&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_tags:&nbsp;Only&nbsp;include&nbsp;logs&nbsp;with&nbsp;these&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_names:&nbsp;Exclude&nbsp;logs&nbsp;with&nbsp;these&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_types:&nbsp;Exclude&nbsp;logs&nbsp;with&nbsp;these&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_tags:&nbsp;Exclude&nbsp;logs&nbsp;with&nbsp;these&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;RunLogPatch&nbsp;or&nbsp;RunLog&nbsp;object.</tt></dd></dl>

<dl><dt>async <a name="OpenAI-atransform"><strong>atransform</strong></a>(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -&gt; 'AsyncIterator[Output]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;atransform,&nbsp;which&nbsp;buffers&nbsp;input&nbsp;and&nbsp;calls&nbsp;astream.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;start&nbsp;producing&nbsp;output&nbsp;while<br>
input&nbsp;is&nbsp;still&nbsp;being&nbsp;generated.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;An&nbsp;async&nbsp;iterator&nbsp;of&nbsp;inputs&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="OpenAI-batch_as_completed"><strong>batch_as_completed</strong></a>(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -&gt; 'Iterator[tuple[int, Union[Output, Exception]]]'</dt><dd><tt>Run&nbsp;invoke&nbsp;in&nbsp;parallel&nbsp;on&nbsp;a&nbsp;list&nbsp;of&nbsp;inputs,<br>
yielding&nbsp;results&nbsp;as&nbsp;they&nbsp;complete.</tt></dd></dl>

<dl><dt><a name="OpenAI-bind"><strong>bind</strong></a>(self, **kwargs: 'Any') -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;arguments&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
Useful&nbsp;when&nbsp;a&nbsp;Runnable&nbsp;in&nbsp;a&nbsp;chain&nbsp;requires&nbsp;an&nbsp;argument&nbsp;that&nbsp;is&nbsp;not<br>
in&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;previous&nbsp;Runnable&nbsp;or&nbsp;included&nbsp;in&nbsp;the&nbsp;user&nbsp;input.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;The&nbsp;arguments&nbsp;to&nbsp;bind&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;arguments&nbsp;bound.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_community.chat_models&nbsp;import&nbsp;ChatOllama<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.output_parsers&nbsp;import&nbsp;StrOutputParser<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;llm&nbsp;=&nbsp;ChatOllama(model='llama2')<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Without&nbsp;bind.<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;llm<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;StrOutputParser()<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain.<a href="#OpenAI-invoke">invoke</a>("Repeat&nbsp;quoted&nbsp;words&nbsp;exactly:&nbsp;'One&nbsp;two&nbsp;three&nbsp;four&nbsp;five.'")<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Output&nbsp;is&nbsp;'One&nbsp;two&nbsp;three&nbsp;four&nbsp;five.'<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;With&nbsp;bind.<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;llm.<a href="#OpenAI-bind">bind</a>(stop=["three"])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;StrOutputParser()<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain.<a href="#OpenAI-invoke">invoke</a>("Repeat&nbsp;quoted&nbsp;words&nbsp;exactly:&nbsp;'One&nbsp;two&nbsp;three&nbsp;four&nbsp;five.'")<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Output&nbsp;is&nbsp;'One&nbsp;two'</tt></dd></dl>

<dl><dt><a name="OpenAI-config_schema"><strong>config_schema</strong></a>(self, *, include: 'Optional[Sequence[str]]' = None) -&gt; 'type[BaseModel]'</dt><dd><tt>The&nbsp;type&nbsp;of&nbsp;config&nbsp;this&nbsp;Runnable&nbsp;accepts&nbsp;specified&nbsp;as&nbsp;a&nbsp;pydantic&nbsp;model.<br>
&nbsp;<br>
To&nbsp;mark&nbsp;a&nbsp;field&nbsp;as&nbsp;configurable,&nbsp;see&nbsp;the&nbsp;`configurable_fields`<br>
and&nbsp;`configurable_alternatives`&nbsp;methods.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;A&nbsp;list&nbsp;of&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;config&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;config.</tt></dd></dl>

<dl><dt><a name="OpenAI-get_config_jsonschema"><strong>get_config_jsonschema</strong></a>(self, *, include: 'Optional[Sequence[str]]' = None) -&gt; 'dict[str, Any]'</dt><dd><tt>Get&nbsp;a&nbsp;JSON&nbsp;schema&nbsp;that&nbsp;represents&nbsp;the&nbsp;config&nbsp;of&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;A&nbsp;list&nbsp;of&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;config&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;JSON&nbsp;schema&nbsp;that&nbsp;represents&nbsp;the&nbsp;config&nbsp;of&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
..&nbsp;versionadded::&nbsp;0.3.0</tt></dd></dl>

<dl><dt><a name="OpenAI-get_graph"><strong>get_graph</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'Graph'</dt><dd><tt>Return&nbsp;a&nbsp;graph&nbsp;representation&nbsp;of&nbsp;this&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="OpenAI-get_input_jsonschema"><strong>get_input_jsonschema</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'dict[str, Any]'</dt><dd><tt>Get&nbsp;a&nbsp;JSON&nbsp;schema&nbsp;that&nbsp;represents&nbsp;the&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;generating&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;JSON&nbsp;schema&nbsp;that&nbsp;represents&nbsp;the&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;add_one(x:&nbsp;int)&nbsp;-&gt;&nbsp;int:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;+&nbsp;1<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(add_one)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(runnable.<a href="#OpenAI-get_input_jsonschema">get_input_jsonschema</a>())<br>
&nbsp;<br>
..&nbsp;versionadded::&nbsp;0.3.0</tt></dd></dl>

<dl><dt><a name="OpenAI-get_input_schema"><strong>get_input_schema</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'type[BaseModel]'</dt><dd><tt>Get&nbsp;a&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Runnables&nbsp;that&nbsp;leverage&nbsp;the&nbsp;configurable_fields&nbsp;and&nbsp;configurable_alternatives<br>
methods&nbsp;will&nbsp;have&nbsp;a&nbsp;dynamic&nbsp;input&nbsp;schema&nbsp;that&nbsp;depends&nbsp;on&nbsp;which<br>
configuration&nbsp;the&nbsp;Runnable&nbsp;is&nbsp;invoked&nbsp;with.<br>
&nbsp;<br>
This&nbsp;method&nbsp;allows&nbsp;to&nbsp;get&nbsp;an&nbsp;input&nbsp;schema&nbsp;for&nbsp;a&nbsp;specific&nbsp;configuration.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;generating&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;input.</tt></dd></dl>

<dl><dt><a name="OpenAI-get_name"><strong>get_name</strong></a>(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -&gt; 'str'</dt><dd><tt>Get&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="OpenAI-get_output_jsonschema"><strong>get_output_jsonschema</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'dict[str, Any]'</dt><dd><tt>Get&nbsp;a&nbsp;JSON&nbsp;schema&nbsp;that&nbsp;represents&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;generating&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;JSON&nbsp;schema&nbsp;that&nbsp;represents&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;add_one(x:&nbsp;int)&nbsp;-&gt;&nbsp;int:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;+&nbsp;1<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(add_one)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(runnable.<a href="#OpenAI-get_output_jsonschema">get_output_jsonschema</a>())<br>
&nbsp;<br>
..&nbsp;versionadded::&nbsp;0.3.0</tt></dd></dl>

<dl><dt><a name="OpenAI-get_output_schema"><strong>get_output_schema</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'type[BaseModel]'</dt><dd><tt>Get&nbsp;a&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;output&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Runnables&nbsp;that&nbsp;leverage&nbsp;the&nbsp;configurable_fields&nbsp;and&nbsp;configurable_alternatives<br>
methods&nbsp;will&nbsp;have&nbsp;a&nbsp;dynamic&nbsp;output&nbsp;schema&nbsp;that&nbsp;depends&nbsp;on&nbsp;which<br>
configuration&nbsp;the&nbsp;Runnable&nbsp;is&nbsp;invoked&nbsp;with.<br>
&nbsp;<br>
This&nbsp;method&nbsp;allows&nbsp;to&nbsp;get&nbsp;an&nbsp;output&nbsp;schema&nbsp;for&nbsp;a&nbsp;specific&nbsp;configuration.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;generating&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;output.</tt></dd></dl>

<dl><dt><a name="OpenAI-get_prompts"><strong>get_prompts</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'list[BasePromptTemplate]'</dt><dd><tt>Return&nbsp;a&nbsp;list&nbsp;of&nbsp;prompts&nbsp;used&nbsp;by&nbsp;this&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="OpenAI-map"><strong>map</strong></a>(self) -&gt; 'Runnable[list[Input], list[Output]]'</dt><dd><tt>Return&nbsp;a&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;maps&nbsp;a&nbsp;list&nbsp;of&nbsp;inputs&nbsp;to&nbsp;a&nbsp;list&nbsp;of&nbsp;outputs,<br>
by&nbsp;calling&nbsp;<a href="#OpenAI-invoke">invoke</a>()&nbsp;with&nbsp;each&nbsp;input.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;maps&nbsp;a&nbsp;list&nbsp;of&nbsp;inputs&nbsp;to&nbsp;a&nbsp;list&nbsp;of&nbsp;outputs.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;_lambda(x:&nbsp;int)&nbsp;-&gt;&nbsp;int:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;+&nbsp;1<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(_lambda)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(runnable.<a href="#OpenAI-map">map</a>().<a href="#OpenAI-invoke">invoke</a>([1,&nbsp;2,&nbsp;3]))&nbsp;#&nbsp;[2,&nbsp;3,&nbsp;4]</tt></dd></dl>

<dl><dt><a name="OpenAI-pick"><strong>pick</strong></a>(self, keys: 'Union[str, list[str]]') -&gt; 'RunnableSerializable[Any, Any]'</dt><dd><tt>Pick&nbsp;keys&nbsp;from&nbsp;the&nbsp;output&nbsp;dict&nbsp;of&nbsp;this&nbsp;Runnable.<br>
&nbsp;<br>
Pick&nbsp;single&nbsp;key:<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;json<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda,&nbsp;RunnableMap<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as_str&nbsp;=&nbsp;RunnableLambda(str)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as_json&nbsp;=&nbsp;RunnableLambda(json.loads)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;RunnableMap(str=as_str,&nbsp;json=as_json)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chain.<a href="#OpenAI-invoke">invoke</a>("[1,&nbsp;2,&nbsp;3]")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;{"str":&nbsp;"[1,&nbsp;2,&nbsp;3]",&nbsp;"json":&nbsp;[1,&nbsp;2,&nbsp;3]}<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;json_only_chain&nbsp;=&nbsp;chain.<a href="#OpenAI-pick">pick</a>("json")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;json_only_chain.<a href="#OpenAI-invoke">invoke</a>("[1,&nbsp;2,&nbsp;3]")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;[1,&nbsp;2,&nbsp;3]<br>
&nbsp;<br>
Pick&nbsp;list&nbsp;of&nbsp;keys:<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing&nbsp;import&nbsp;Any<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;json<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda,&nbsp;RunnableMap<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as_str&nbsp;=&nbsp;RunnableLambda(str)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as_json&nbsp;=&nbsp;RunnableLambda(json.loads)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;as_bytes(x:&nbsp;Any)&nbsp;-&gt;&nbsp;bytes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;bytes(x,&nbsp;"utf-8")<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;RunnableMap(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;str=as_str,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;json=as_json,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bytes=RunnableLambda(as_bytes)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chain.<a href="#OpenAI-invoke">invoke</a>("[1,&nbsp;2,&nbsp;3]")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;{"str":&nbsp;"[1,&nbsp;2,&nbsp;3]",&nbsp;"json":&nbsp;[1,&nbsp;2,&nbsp;3],&nbsp;"bytes":&nbsp;b"[1,&nbsp;2,&nbsp;3]"}<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;json_and_bytes_chain&nbsp;=&nbsp;chain.<a href="#OpenAI-pick">pick</a>(["json",&nbsp;"bytes"])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;json_and_bytes_chain.<a href="#OpenAI-invoke">invoke</a>("[1,&nbsp;2,&nbsp;3]")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;{"json":&nbsp;[1,&nbsp;2,&nbsp;3],&nbsp;"bytes":&nbsp;b"[1,&nbsp;2,&nbsp;3]"}</tt></dd></dl>

<dl><dt><a name="OpenAI-pipe"><strong>pipe</strong></a>(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -&gt; 'RunnableSerializable[Input, Other]'</dt><dd><tt>Compose&nbsp;this&nbsp;Runnable&nbsp;with&nbsp;Runnable-like&nbsp;objects&nbsp;to&nbsp;make&nbsp;a&nbsp;RunnableSequence.<br>
&nbsp;<br>
Equivalent&nbsp;to&nbsp;`RunnableSequence(self,&nbsp;*others)`&nbsp;or&nbsp;`self&nbsp;|&nbsp;others[0]&nbsp;|&nbsp;...`<br>
&nbsp;<br>
Example:<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;add_one(x:&nbsp;int)&nbsp;-&gt;&nbsp;int:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;+&nbsp;1<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;mul_two(x:&nbsp;int)&nbsp;-&gt;&nbsp;int:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;*&nbsp;2<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable_1&nbsp;=&nbsp;RunnableLambda(add_one)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable_2&nbsp;=&nbsp;RunnableLambda(mul_two)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sequence&nbsp;=&nbsp;runnable_1.<a href="#OpenAI-pipe">pipe</a>(runnable_2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Or&nbsp;equivalently:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;sequence&nbsp;=&nbsp;runnable_1&nbsp;|&nbsp;runnable_2<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;sequence&nbsp;=&nbsp;RunnableSequence(first=runnable_1,&nbsp;last=runnable_2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sequence.<a href="#OpenAI-invoke">invoke</a>(1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;sequence.<a href="#OpenAI-ainvoke">ainvoke</a>(1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;4<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sequence.<a href="#OpenAI-batch">batch</a>([1,&nbsp;2,&nbsp;3])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;sequence.<a href="#OpenAI-abatch">abatch</a>([1,&nbsp;2,&nbsp;3])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;[4,&nbsp;6,&nbsp;8]</tt></dd></dl>

<dl><dt><a name="OpenAI-transform"><strong>transform</strong></a>(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -&gt; 'Iterator[Output]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;transform,&nbsp;which&nbsp;buffers&nbsp;input&nbsp;and&nbsp;calls&nbsp;astream.<br>
&nbsp;<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;start&nbsp;producing&nbsp;output&nbsp;while<br>
input&nbsp;is&nbsp;still&nbsp;being&nbsp;generated.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;An&nbsp;iterator&nbsp;of&nbsp;inputs&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="OpenAI-with_alisteners"><strong>with_alisteners</strong></a>(self, *, on_start: 'Optional[AsyncListener]' = None, on_end: 'Optional[AsyncListener]' = None, on_error: 'Optional[AsyncListener]' = None) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;async&nbsp;lifecycle&nbsp;listeners&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
on_start:&nbsp;Asynchronously&nbsp;called&nbsp;before&nbsp;the&nbsp;Runnable&nbsp;starts&nbsp;running.<br>
on_end:&nbsp;Asynchronously&nbsp;called&nbsp;after&nbsp;the&nbsp;Runnable&nbsp;finishes&nbsp;running.<br>
on_error:&nbsp;Asynchronously&nbsp;called&nbsp;if&nbsp;the&nbsp;Runnable&nbsp;throws&nbsp;an&nbsp;error.<br>
&nbsp;<br>
The&nbsp;Run&nbsp;object&nbsp;contains&nbsp;information&nbsp;about&nbsp;the&nbsp;run,&nbsp;including&nbsp;its&nbsp;id,<br>
type,&nbsp;input,&nbsp;output,&nbsp;error,&nbsp;start_time,&nbsp;end_time,&nbsp;and&nbsp;any&nbsp;tags&nbsp;or&nbsp;metadata<br>
added&nbsp;to&nbsp;the&nbsp;run.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_start:&nbsp;Asynchronously&nbsp;called&nbsp;before&nbsp;the&nbsp;Runnable&nbsp;starts&nbsp;running.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_end:&nbsp;Asynchronously&nbsp;called&nbsp;after&nbsp;the&nbsp;Runnable&nbsp;finishes&nbsp;running.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_error:&nbsp;Asynchronously&nbsp;called&nbsp;if&nbsp;the&nbsp;Runnable&nbsp;throws&nbsp;an&nbsp;error.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;listeners&nbsp;bound.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda,&nbsp;Runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;datetime&nbsp;import&nbsp;datetime,&nbsp;timezone<br>
&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;time<br>
&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;asyncio<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;format_t(timestamp:&nbsp;float)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;datetime.fromtimestamp(timestamp,&nbsp;tz=timezone.utc).isoformat()<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;test_runnable(time_to_sleep&nbsp;:&nbsp;int):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"Runnable[{time_to_sleep}s]:&nbsp;starts&nbsp;at&nbsp;{format_t(time.time())}")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(time_to_sleep)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"Runnable[{time_to_sleep}s]:&nbsp;ends&nbsp;at&nbsp;{format_t(time.time())}")<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;fn_start(run_obj&nbsp;:&nbsp;Runnable):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"on&nbsp;start&nbsp;callback&nbsp;starts&nbsp;at&nbsp;{format_t(time.time())}")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"on&nbsp;start&nbsp;callback&nbsp;ends&nbsp;at&nbsp;{format_t(time.time())}")<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;fn_end(run_obj&nbsp;:&nbsp;Runnable):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"on&nbsp;end&nbsp;callback&nbsp;starts&nbsp;at&nbsp;{format_t(time.time())}")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"on&nbsp;end&nbsp;callback&nbsp;ends&nbsp;at&nbsp;{format_t(time.time())}")<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(test_runnable).<a href="#OpenAI-with_alisteners">with_alisteners</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on_start=fn_start,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on_end=fn_end<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;concurrent_runs():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.gather(runnable.<a href="#OpenAI-ainvoke">ainvoke</a>(2),&nbsp;runnable.<a href="#OpenAI-ainvoke">ainvoke</a>(3))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;asyncio.run(concurrent_runs())<br>
&nbsp;&nbsp;&nbsp;&nbsp;Result:<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;start&nbsp;callback&nbsp;starts&nbsp;at&nbsp;2025-03-01T07:05:22.875378+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;start&nbsp;callback&nbsp;starts&nbsp;at&nbsp;2025-03-01T07:05:22.875495+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;start&nbsp;callback&nbsp;ends&nbsp;at&nbsp;2025-03-01T07:05:25.878862+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;start&nbsp;callback&nbsp;ends&nbsp;at&nbsp;2025-03-01T07:05:25.878947+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;Runnable[2s]:&nbsp;starts&nbsp;at&nbsp;2025-03-01T07:05:25.879392+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;Runnable[3s]:&nbsp;starts&nbsp;at&nbsp;2025-03-01T07:05:25.879804+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;Runnable[2s]:&nbsp;ends&nbsp;at&nbsp;2025-03-01T07:05:27.881998+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;end&nbsp;callback&nbsp;starts&nbsp;at&nbsp;2025-03-01T07:05:27.882360+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;Runnable[3s]:&nbsp;ends&nbsp;at&nbsp;2025-03-01T07:05:28.881737+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;end&nbsp;callback&nbsp;starts&nbsp;at&nbsp;2025-03-01T07:05:28.882428+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;end&nbsp;callback&nbsp;ends&nbsp;at&nbsp;2025-03-01T07:05:29.883893+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;end&nbsp;callback&nbsp;ends&nbsp;at&nbsp;2025-03-01T07:05:30.884831+00:00</tt></dd></dl>

<dl><dt><a name="OpenAI-with_config"><strong>with_config</strong></a>(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;config&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;bind&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;config&nbsp;bound.</tt></dd></dl>

<dl><dt><a name="OpenAI-with_fallbacks"><strong>with_fallbacks</strong></a>(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'tuple[type[BaseException], ...]' = (&lt;class 'Exception'&gt;,), exception_key: 'Optional[str]' = None) -&gt; 'RunnableWithFallbacksT[Input, Output]'</dt><dd><tt>Add&nbsp;fallbacks&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
The&nbsp;new&nbsp;Runnable&nbsp;will&nbsp;try&nbsp;the&nbsp;original&nbsp;Runnable,&nbsp;and&nbsp;then&nbsp;each&nbsp;fallback<br>
in&nbsp;order,&nbsp;upon&nbsp;failures.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fallbacks:&nbsp;A&nbsp;sequence&nbsp;of&nbsp;runnables&nbsp;to&nbsp;try&nbsp;if&nbsp;the&nbsp;original&nbsp;Runnable&nbsp;fails.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exceptions_to_handle:&nbsp;A&nbsp;tuple&nbsp;of&nbsp;exception&nbsp;types&nbsp;to&nbsp;handle.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;(Exception,).<br>
&nbsp;&nbsp;&nbsp;&nbsp;exception_key:&nbsp;If&nbsp;string&nbsp;is&nbsp;specified&nbsp;then&nbsp;handled&nbsp;exceptions&nbsp;will&nbsp;be&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;fallbacks&nbsp;as&nbsp;part&nbsp;of&nbsp;the&nbsp;input&nbsp;under&nbsp;the&nbsp;specified&nbsp;key.&nbsp;If&nbsp;None,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exceptions&nbsp;will&nbsp;not&nbsp;be&nbsp;passed&nbsp;to&nbsp;fallbacks.&nbsp;If&nbsp;used,&nbsp;the&nbsp;base&nbsp;Runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;its&nbsp;fallbacks&nbsp;must&nbsp;accept&nbsp;a&nbsp;dictionary&nbsp;as&nbsp;input.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;will&nbsp;try&nbsp;the&nbsp;original&nbsp;Runnable,&nbsp;and&nbsp;then&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;fallback&nbsp;in&nbsp;order,&nbsp;upon&nbsp;failures.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing&nbsp;import&nbsp;Iterator<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableGenerator<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;_generate_immediate_error(input:&nbsp;Iterator)&nbsp;-&gt;&nbsp;Iterator[str]:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise&nbsp;ValueError()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yield&nbsp;""<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;_generate(input:&nbsp;Iterator)&nbsp;-&gt;&nbsp;Iterator[str]:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yield&nbsp;from&nbsp;"foo&nbsp;bar"<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableGenerator(_generate_immediate_error).<a href="#OpenAI-with_fallbacks">with_fallbacks</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[RunnableGenerator(_generate)]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(''.join(runnable.<a href="#OpenAI-stream">stream</a>({})))&nbsp;#foo&nbsp;bar<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fallbacks:&nbsp;A&nbsp;sequence&nbsp;of&nbsp;runnables&nbsp;to&nbsp;try&nbsp;if&nbsp;the&nbsp;original&nbsp;Runnable&nbsp;fails.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exceptions_to_handle:&nbsp;A&nbsp;tuple&nbsp;of&nbsp;exception&nbsp;types&nbsp;to&nbsp;handle.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exception_key:&nbsp;If&nbsp;string&nbsp;is&nbsp;specified&nbsp;then&nbsp;handled&nbsp;exceptions&nbsp;will&nbsp;be&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;fallbacks&nbsp;as&nbsp;part&nbsp;of&nbsp;the&nbsp;input&nbsp;under&nbsp;the&nbsp;specified&nbsp;key.&nbsp;If&nbsp;None,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exceptions&nbsp;will&nbsp;not&nbsp;be&nbsp;passed&nbsp;to&nbsp;fallbacks.&nbsp;If&nbsp;used,&nbsp;the&nbsp;base&nbsp;Runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;its&nbsp;fallbacks&nbsp;must&nbsp;accept&nbsp;a&nbsp;dictionary&nbsp;as&nbsp;input.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;will&nbsp;try&nbsp;the&nbsp;original&nbsp;Runnable,&nbsp;and&nbsp;then&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;fallback&nbsp;in&nbsp;order,&nbsp;upon&nbsp;failures.</tt></dd></dl>

<dl><dt><a name="OpenAI-with_listeners"><strong>with_listeners</strong></a>(self, *, on_start: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_end: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_error: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;lifecycle&nbsp;listeners&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
on_start:&nbsp;Called&nbsp;before&nbsp;the&nbsp;Runnable&nbsp;starts&nbsp;running,&nbsp;with&nbsp;the&nbsp;Run&nbsp;object.<br>
on_end:&nbsp;Called&nbsp;after&nbsp;the&nbsp;Runnable&nbsp;finishes&nbsp;running,&nbsp;with&nbsp;the&nbsp;Run&nbsp;object.<br>
on_error:&nbsp;Called&nbsp;if&nbsp;the&nbsp;Runnable&nbsp;throws&nbsp;an&nbsp;error,&nbsp;with&nbsp;the&nbsp;Run&nbsp;object.<br>
&nbsp;<br>
The&nbsp;Run&nbsp;object&nbsp;contains&nbsp;information&nbsp;about&nbsp;the&nbsp;run,&nbsp;including&nbsp;its&nbsp;id,<br>
type,&nbsp;input,&nbsp;output,&nbsp;error,&nbsp;start_time,&nbsp;end_time,&nbsp;and&nbsp;any&nbsp;tags&nbsp;or&nbsp;metadata<br>
added&nbsp;to&nbsp;the&nbsp;run.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_start:&nbsp;Called&nbsp;before&nbsp;the&nbsp;Runnable&nbsp;starts&nbsp;running.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_end:&nbsp;Called&nbsp;after&nbsp;the&nbsp;Runnable&nbsp;finishes&nbsp;running.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_error:&nbsp;Called&nbsp;if&nbsp;the&nbsp;Runnable&nbsp;throws&nbsp;an&nbsp;error.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;listeners&nbsp;bound.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.tracers.schemas&nbsp;import&nbsp;Run<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;time<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;test_runnable(time_to_sleep&nbsp;:&nbsp;int):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(time_to_sleep)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;fn_start(run_obj:&nbsp;Run):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("start_time:",&nbsp;run_obj.start_time)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;fn_end(run_obj:&nbsp;Run):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("end_time:",&nbsp;run_obj.end_time)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;RunnableLambda(test_runnable).<a href="#OpenAI-with_listeners">with_listeners</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on_start=fn_start,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on_end=fn_end<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain.<a href="#OpenAI-invoke">invoke</a>(2)</tt></dd></dl>

<dl><dt><a name="OpenAI-with_retry"><strong>with_retry</strong></a>(self, *, retry_if_exception_type: 'tuple[type[BaseException], ...]' = (&lt;class 'Exception'&gt;,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Create&nbsp;a&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;retries&nbsp;the&nbsp;original&nbsp;Runnable&nbsp;on&nbsp;exceptions.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;retry_if_exception_type:&nbsp;A&nbsp;tuple&nbsp;of&nbsp;exception&nbsp;types&nbsp;to&nbsp;retry&nbsp;on.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;(Exception,).<br>
&nbsp;&nbsp;&nbsp;&nbsp;wait_exponential_jitter:&nbsp;Whether&nbsp;to&nbsp;add&nbsp;jitter&nbsp;to&nbsp;the&nbsp;wait<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;time&nbsp;between&nbsp;retries.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop_after_attempt:&nbsp;The&nbsp;maximum&nbsp;number&nbsp;of&nbsp;attempts&nbsp;to&nbsp;make&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;giving&nbsp;up.&nbsp;Defaults&nbsp;to&nbsp;3.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;retries&nbsp;the&nbsp;original&nbsp;Runnable&nbsp;on&nbsp;exceptions.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;count&nbsp;=&nbsp;0<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;_lambda(x:&nbsp;int)&nbsp;-&gt;&nbsp;None:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;global&nbsp;count<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;count&nbsp;=&nbsp;count&nbsp;+&nbsp;1<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;x&nbsp;==&nbsp;1:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise&nbsp;ValueError("x&nbsp;is&nbsp;1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(_lambda)<br>
&nbsp;&nbsp;&nbsp;&nbsp;try:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable.<a href="#OpenAI-with_retry">with_retry</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stop_after_attempt=2,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;retry_if_exception_type=(ValueError,),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;).<a href="#OpenAI-invoke">invoke</a>(1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;except&nbsp;ValueError:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;assert&nbsp;(count&nbsp;==&nbsp;2)<br>
&nbsp;<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;retry_if_exception_type:&nbsp;A&nbsp;tuple&nbsp;of&nbsp;exception&nbsp;types&nbsp;to&nbsp;retry&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;wait_exponential_jitter:&nbsp;Whether&nbsp;to&nbsp;add&nbsp;jitter&nbsp;to&nbsp;the&nbsp;wait&nbsp;time<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;between&nbsp;retries<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop_after_attempt:&nbsp;The&nbsp;maximum&nbsp;number&nbsp;of&nbsp;attempts&nbsp;to&nbsp;make&nbsp;before&nbsp;giving&nbsp;up<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;retries&nbsp;the&nbsp;original&nbsp;Runnable&nbsp;on&nbsp;exceptions.</tt></dd></dl>

<dl><dt><a name="OpenAI-with_types"><strong>with_types</strong></a>(self, *, input_type: 'Optional[type[Input]]' = None, output_type: 'Optional[type[Output]]' = None) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_type:&nbsp;The&nbsp;input&nbsp;type&nbsp;to&nbsp;bind&nbsp;to&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_type:&nbsp;The&nbsp;output&nbsp;type&nbsp;to&nbsp;bind&nbsp;to&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;types&nbsp;bound.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a>:<br>
<dl><dt><strong>config_specs</strong></dt>
<dd><tt>List&nbsp;configurable&nbsp;fields&nbsp;for&nbsp;this&nbsp;Runnable.</tt></dd>
</dl>
<dl><dt><strong>input_schema</strong></dt>
<dd><tt>The&nbsp;type&nbsp;of&nbsp;input&nbsp;this&nbsp;Runnable&nbsp;accepts&nbsp;specified&nbsp;as&nbsp;a&nbsp;pydantic&nbsp;model.</tt></dd>
</dl>
<dl><dt><strong>output_schema</strong></dt>
<dd><tt>The&nbsp;type&nbsp;of&nbsp;output&nbsp;this&nbsp;Runnable&nbsp;produces&nbsp;specified&nbsp;as&nbsp;a&nbsp;pydantic&nbsp;model.</tt></dd>
</dl>
<hr>
Class methods inherited from <a href="typing.html#Generic">typing.Generic</a>:<br>
<dl><dt><a name="OpenAI-__init_subclass__"><strong>__init_subclass__</strong></a>(*args, **kwargs)<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>This&nbsp;method&nbsp;is&nbsp;called&nbsp;when&nbsp;a&nbsp;class&nbsp;is&nbsp;subclassed.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;does&nbsp;nothing.&nbsp;It&nbsp;may&nbsp;be<br>
overridden&nbsp;to&nbsp;extend&nbsp;subclasses.</tt></dd></dl>

</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="OpenAIEmbeddings">class <strong>OpenAIEmbeddings</strong></a>(<a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>, <a href="langchain_openai.embeddings.base.html#OpenAIEmbeddings">langchain_openai.embeddings.base.OpenAIEmbeddings</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt><a href="#OpenAIEmbeddings">OpenAIEmbeddings</a>(*args,&nbsp;client:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;async_client:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;model:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;dimensions:&nbsp;Optional[int]&nbsp;=&nbsp;None,&nbsp;deployment:&nbsp;Optional[str]&nbsp;=&nbsp;'text-embedding-ada-002',&nbsp;api_version:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;base_url:&nbsp;Optional[str]&nbsp;=&nbsp;&amp;lt;factory&amp;gt;,&nbsp;openai_api_type:&nbsp;Optional[str]&nbsp;=&nbsp;&amp;lt;factory&amp;gt;,&nbsp;openai_proxy:&nbsp;Optional[str]&nbsp;=&nbsp;&amp;lt;factory&amp;gt;,&nbsp;embedding_ctx_length:&nbsp;int&nbsp;=&nbsp;8191,&nbsp;api_key:&nbsp;Optional[pydantic.types.SecretStr]&nbsp;=&nbsp;&amp;lt;factory&amp;gt;,&nbsp;organization:&nbsp;Optional[str]&nbsp;=&nbsp;&amp;lt;factory&amp;gt;,&nbsp;allowed_special:&nbsp;Union[Literal['all'],&nbsp;Set[str],&nbsp;NoneType]&nbsp;=&nbsp;None,&nbsp;disallowed_special:&nbsp;Union[Literal['all'],&nbsp;Set[str],&nbsp;Sequence[str],&nbsp;NoneType]&nbsp;=&nbsp;None,&nbsp;chunk_size:&nbsp;int&nbsp;=&nbsp;16,&nbsp;max_retries:&nbsp;int&nbsp;=&nbsp;2,&nbsp;timeout:&nbsp;Union[float,&nbsp;Tuple[float,&nbsp;float],&nbsp;Any,&nbsp;NoneType]&nbsp;=&nbsp;None,&nbsp;headers:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;tiktoken_enabled:&nbsp;bool&nbsp;=&nbsp;True,&nbsp;tiktoken_model_name:&nbsp;Optional[str]&nbsp;=&nbsp;'text-embedding-ada-002',&nbsp;show_progress_bar:&nbsp;bool&nbsp;=&nbsp;False,&nbsp;model_kwargs:&nbsp;Dict[str,&nbsp;Any]&nbsp;=&nbsp;&amp;lt;factory&amp;gt;,&nbsp;skip_empty:&nbsp;bool&nbsp;=&nbsp;False,&nbsp;default_headers:&nbsp;Optional[Mapping[str,&nbsp;str]]&nbsp;=&nbsp;None,&nbsp;default_query:&nbsp;Optional[Mapping[str,&nbsp;object]]&nbsp;=&nbsp;None,&nbsp;retry_min_seconds:&nbsp;int&nbsp;=&nbsp;4,&nbsp;retry_max_seconds:&nbsp;int&nbsp;=&nbsp;20,&nbsp;http_client:&nbsp;Optional[Any]&nbsp;=&nbsp;None,&nbsp;http_async_client:&nbsp;Optional[Any]&nbsp;=&nbsp;None,&nbsp;check_embedding_ctx_length:&nbsp;bool&nbsp;=&nbsp;True,&nbsp;proxy_client:&nbsp;Optional[Any]&nbsp;=&nbsp;None,&nbsp;deployment_id:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;config_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;config_id:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;proxy_model_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;**kwargs)&nbsp;-&amp;gt;&nbsp;None<br>
&nbsp;<br>
<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="gen_ai_hub.proxy.langchain.openai.html#OpenAIEmbeddings">OpenAIEmbeddings</a></dd>
<dd><a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a></dd>
<dd><a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a></dd>
<dd><a href="langchain_openai.embeddings.base.html#OpenAIEmbeddings">langchain_openai.embeddings.base.OpenAIEmbeddings</a></dd>
<dd><a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a></dd>
<dd><a href="langchain_core.embeddings.embeddings.html#Embeddings">langchain_core.embeddings.embeddings.Embeddings</a></dd>
<dd><a href="abc.html#ABC">abc.ABC</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="OpenAIEmbeddings-__init__"><strong>__init__</strong></a>(self, *args, **kwargs)</dt><dd><tt>Create&nbsp;a&nbsp;new&nbsp;model&nbsp;by&nbsp;parsing&nbsp;and&nbsp;validating&nbsp;input&nbsp;data&nbsp;from&nbsp;keyword&nbsp;arguments.<br>
&nbsp;<br>
Raises&nbsp;[`ValidationError`][pydantic_core.ValidationError]&nbsp;if&nbsp;the&nbsp;input&nbsp;data&nbsp;cannot&nbsp;be<br>
validated&nbsp;to&nbsp;form&nbsp;a&nbsp;valid&nbsp;model.<br>
&nbsp;<br>
`self`&nbsp;is&nbsp;explicitly&nbsp;positional-only&nbsp;to&nbsp;allow&nbsp;`self`&nbsp;as&nbsp;a&nbsp;field&nbsp;name.</tt></dd></dl>

<hr>
Class methods defined here:<br>
<dl><dt><a name="OpenAIEmbeddings-validate_environment"><strong>validate_environment</strong></a>(values: Dict) -&gt; Dict<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Validates&nbsp;the&nbsp;environment.<br>
&nbsp;<br>
:param&nbsp;values:&nbsp;The&nbsp;input&nbsp;values<br>
:type&nbsp;values:&nbsp;Dict<br>
:return:&nbsp;The&nbsp;validated&nbsp;values<br>
:rtype:&nbsp;Dict</tt></dd></dl>

<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>__abstractmethods__</strong> = frozenset()</dl>

<dl><dt><strong>__annotations__</strong> = {'chunk_size': &lt;class 'int'&gt;, 'model': typing.Optional[str], 'openai_api_version': typing.Optional[str], 'tiktoken_model_name': typing.Optional[str]}</dl>

<dl><dt><strong>__class_vars__</strong> = set()</dl>

<dl><dt><strong>__private_attributes__</strong> = {}</dl>

<dl><dt><strong>__pydantic_complete__</strong> = True</dl>

<dl><dt><strong>__pydantic_computed_fields__</strong> = {}</dl>

<dl><dt><strong>__pydantic_core_schema__</strong> = {'cls': &lt;class 'gen_ai_hub.proxy.langchain.openai.OpenAIEmbeddings'&gt;, 'config': {'extra_fields_behavior': 'allow', 'populate_by_name': True, 'title': 'OpenAIEmbeddings'}, 'custom_init': True, 'metadata': {'pydantic_js_functions': [&lt;bound method BaseModel.__get_pydantic_json_sche...ai_hub.proxy.langchain.openai.OpenAIEmbeddings'&gt;&gt;]}, 'ref': 'gen_ai_hub.proxy.langchain.openai.OpenAIEmbeddings:140673046150000', 'root_model': False, 'schema': {'function': {'function': &lt;bound method OpenAIEmbeddings.validate_environm...ai_hub.proxy.langchain.openai.OpenAIEmbeddings'&gt;&gt;, 'type': 'no-info'}, 'schema': {'function': {'function': &lt;bound method OpenAIEmbeddings.build_extra of &lt;c...ai_hub.proxy.langchain.openai.OpenAIEmbeddings'&gt;&gt;, 'type': 'no-info'}, 'schema': {'computed_fields': [], 'fields': {'allowed_special': {'metadata': {}, 'schema': {...}, 'type': 'model-field'}, 'async_client': {'metadata': {}, 'schema': {...}, 'serialization_exclude': True, 'type': 'model-field'}, 'check_embedding_ctx_length': {'metadata': {}, 'schema': {...}, 'type': 'model-field'}, 'chunk_size': {'metadata': {}, 'schema': {...}, 'type': 'model-field'}, 'client': {'metadata': {}, 'schema': {...}, 'serialization_exclude': True, 'type': 'model-field'}, 'config_id': {'metadata': {}, 'schema': {...}, 'type': 'model-field'}, 'config_name': {'metadata': {}, 'schema': {...}, 'type': 'model-field'}, 'default_headers': {'metadata': {}, 'schema': {...}, 'type': 'model-field'}, 'default_query': {'metadata': {}, 'schema': {...}, 'type': 'model-field'}, 'deployment': {'metadata': {}, 'schema': {...}, 'type': 'model-field'}, ...}, 'model_name': 'OpenAIEmbeddings', 'type': 'model-fields'}, 'type': 'function-before'}, 'type': 'function-before'}, 'type': 'model'}</dl>

<dl><dt><strong>__pydantic_custom_init__</strong> = True</dl>

<dl><dt><strong>__pydantic_decorators__</strong> = DecoratorInfos(validators={}, field_validators={...coratorInfo(mode='before'))}, computed_fields={})</dl>

<dl><dt><strong>__pydantic_fields__</strong> = {'allowed_special': FieldInfo(annotation=Union[Literal['all'], Set[str], NoneType], required=False, default=None), 'async_client': FieldInfo(annotation=Any, required=False, default=None, exclude=True), 'check_embedding_ctx_length': FieldInfo(annotation=bool, required=False, default=True), 'chunk_size': FieldInfo(annotation=int, required=False, default=16), 'client': FieldInfo(annotation=Any, required=False, default=None, exclude=True), 'config_id': FieldInfo(annotation=Union[str, NoneType], required=False, default=None), 'config_name': FieldInfo(annotation=Union[str, NoneType], required=False, default=None), 'default_headers': FieldInfo(annotation=Union[Mapping[str, str], NoneType], required=False, default=None), 'default_query': FieldInfo(annotation=Union[Mapping[str, object], NoneType], required=False, default=None), 'deployment': FieldInfo(annotation=Union[str, NoneType], required=False, default='text-embedding-ada-002'), ...}</dl>

<dl><dt><strong>__pydantic_generic_metadata__</strong> = {'args': (), 'origin': None, 'parameters': ()}</dl>

<dl><dt><strong>__pydantic_parent_namespace__</strong> = None</dl>

<dl><dt><strong>__pydantic_post_init__</strong> = None</dl>

<dl><dt><strong>__pydantic_serializer__</strong> = SchemaSerializer(serializer=Model(
    ModelSeri...me: "OpenAIEmbeddings",
    },
), definitions=[])</dl>

<dl><dt><strong>__pydantic_validator__</strong> = SchemaValidator(title="OpenAIEmbeddings", valida...s",
    },
), definitions=[], cache_strings=True)</dl>

<dl><dt><strong>__signature__</strong> = &lt;Signature (*args, client: Any = None, async_cli...el_name: Optional[str] = None, **kwargs) -&gt; None&gt;</dl>

<dl><dt><strong>model_config</strong> = {'extra': 'allow', 'populate_by_name': True, 'protected_namespaces': ()}</dl>

<hr>
Class methods inherited from <a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a>:<br>
<dl><dt><a name="OpenAIEmbeddings-validate_clients"><strong>validate_clients</strong></a>(values: Dict) -&gt; Dict<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<hr>
Data descriptors inherited from <a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a>:<br>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Methods inherited from <a href="langchain_openai.embeddings.base.html#OpenAIEmbeddings">langchain_openai.embeddings.base.OpenAIEmbeddings</a>:<br>
<dl><dt>async <a name="OpenAIEmbeddings-aembed_documents"><strong>aembed_documents</strong></a>(self, texts: 'List[str]', chunk_size: 'int | None' = None) -&gt; 'List[List[float]]'</dt><dd><tt>Call&nbsp;out&nbsp;to&nbsp;<a href="#OpenAI">OpenAI</a>'s&nbsp;embedding&nbsp;endpoint&nbsp;async&nbsp;for&nbsp;embedding&nbsp;search&nbsp;docs.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;texts:&nbsp;The&nbsp;list&nbsp;of&nbsp;texts&nbsp;to&nbsp;embed.<br>
&nbsp;&nbsp;&nbsp;&nbsp;chunk_size:&nbsp;The&nbsp;chunk&nbsp;size&nbsp;of&nbsp;embeddings.&nbsp;If&nbsp;None,&nbsp;will&nbsp;use&nbsp;the&nbsp;chunk&nbsp;size<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;specified&nbsp;by&nbsp;the&nbsp;class.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;List&nbsp;of&nbsp;embeddings,&nbsp;one&nbsp;for&nbsp;each&nbsp;text.</tt></dd></dl>

<dl><dt>async <a name="OpenAIEmbeddings-aembed_query"><strong>aembed_query</strong></a>(self, text: 'str') -&gt; 'List[float]'</dt><dd><tt>Call&nbsp;out&nbsp;to&nbsp;<a href="#OpenAI">OpenAI</a>'s&nbsp;embedding&nbsp;endpoint&nbsp;async&nbsp;for&nbsp;embedding&nbsp;query&nbsp;text.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;text:&nbsp;The&nbsp;text&nbsp;to&nbsp;embed.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Embedding&nbsp;for&nbsp;the&nbsp;text.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-embed_documents"><strong>embed_documents</strong></a>(self, texts: 'List[str]', chunk_size: 'int | None' = None) -&gt; 'List[List[float]]'</dt><dd><tt>Call&nbsp;out&nbsp;to&nbsp;<a href="#OpenAI">OpenAI</a>'s&nbsp;embedding&nbsp;endpoint&nbsp;for&nbsp;embedding&nbsp;search&nbsp;docs.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;texts:&nbsp;The&nbsp;list&nbsp;of&nbsp;texts&nbsp;to&nbsp;embed.<br>
&nbsp;&nbsp;&nbsp;&nbsp;chunk_size:&nbsp;The&nbsp;chunk&nbsp;size&nbsp;of&nbsp;embeddings.&nbsp;If&nbsp;None,&nbsp;will&nbsp;use&nbsp;the&nbsp;chunk&nbsp;size<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;specified&nbsp;by&nbsp;the&nbsp;class.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;List&nbsp;of&nbsp;embeddings,&nbsp;one&nbsp;for&nbsp;each&nbsp;text.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-embed_query"><strong>embed_query</strong></a>(self, text: 'str') -&gt; 'List[float]'</dt><dd><tt>Call&nbsp;out&nbsp;to&nbsp;<a href="#OpenAI">OpenAI</a>'s&nbsp;embedding&nbsp;endpoint&nbsp;for&nbsp;embedding&nbsp;query&nbsp;text.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;text:&nbsp;The&nbsp;text&nbsp;to&nbsp;embed.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Embedding&nbsp;for&nbsp;the&nbsp;text.</tt></dd></dl>

<hr>
Class methods inherited from <a href="langchain_openai.embeddings.base.html#OpenAIEmbeddings">langchain_openai.embeddings.base.OpenAIEmbeddings</a>:<br>
<dl><dt><a name="OpenAIEmbeddings-build_extra"><strong>build_extra</strong></a>(values: 'Dict[str, Any]') -&gt; 'Any'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Build&nbsp;extra&nbsp;kwargs&nbsp;from&nbsp;additional&nbsp;params&nbsp;that&nbsp;were&nbsp;passed&nbsp;in.</tt></dd></dl>

<hr>
Methods inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><a name="OpenAIEmbeddings-__copy__"><strong>__copy__</strong></a>(self) -&gt; 'Self'</dt><dd><tt>Returns&nbsp;a&nbsp;shallow&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__deepcopy__"><strong>__deepcopy__</strong></a>(self, memo: 'dict[int, Any] | None' = None) -&gt; 'Self'</dt><dd><tt>Returns&nbsp;a&nbsp;deep&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__delattr__"><strong>__delattr__</strong></a>(self, item: 'str') -&gt; 'Any'</dt><dd><tt>Implement&nbsp;delattr(self,&nbsp;name).</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__eq__"><strong>__eq__</strong></a>(self, other: 'Any') -&gt; 'bool'</dt><dd><tt>Return&nbsp;self==value.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__getattr__"><strong>__getattr__</strong></a>(self, item: 'str') -&gt; 'Any'</dt></dl>

<dl><dt><a name="OpenAIEmbeddings-__getstate__"><strong>__getstate__</strong></a>(self) -&gt; 'dict[Any, Any]'</dt></dl>

<dl><dt><a name="OpenAIEmbeddings-__iter__"><strong>__iter__</strong></a>(self) -&gt; 'TupleGenerator'</dt><dd><tt>So&nbsp;`<a href="#OpenAIEmbeddings-dict">dict</a>(model)`&nbsp;works.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__pretty__"><strong>__pretty__</strong></a>(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -&gt; 'typing.Generator[Any, None, None]'</dt><dd><tt>Used&nbsp;by&nbsp;devtools&nbsp;(<a href="https://python-devtools.helpmanual.io/">https://python-devtools.helpmanual.io/</a>)&nbsp;to&nbsp;pretty&nbsp;print&nbsp;objects.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__replace__"><strong>__replace__</strong></a>(self, **changes: 'Any') -&gt; 'Self'</dt><dd><tt>#&nbsp;Because&nbsp;we&nbsp;make&nbsp;use&nbsp;of&nbsp;`@dataclass_transform()`,&nbsp;`__replace__`&nbsp;is&nbsp;already&nbsp;synthesized&nbsp;by<br>
#&nbsp;type&nbsp;checkers,&nbsp;so&nbsp;we&nbsp;define&nbsp;the&nbsp;implementation&nbsp;in&nbsp;this&nbsp;`if&nbsp;not&nbsp;TYPE_CHECKING:`&nbsp;block:</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__repr__"><strong>__repr__</strong></a>(self) -&gt; 'str'</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__repr_args__"><strong>__repr_args__</strong></a>(self) -&gt; '_repr.ReprArgs'</dt></dl>

<dl><dt><a name="OpenAIEmbeddings-__repr_name__"><strong>__repr_name__</strong></a>(self) -&gt; 'str'</dt><dd><tt>Name&nbsp;of&nbsp;the&nbsp;instance's&nbsp;class,&nbsp;used&nbsp;in&nbsp;__repr__.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__repr_recursion__"><strong>__repr_recursion__</strong></a>(self, object: 'Any') -&gt; 'str'</dt><dd><tt>Returns&nbsp;the&nbsp;string&nbsp;representation&nbsp;of&nbsp;a&nbsp;recursive&nbsp;object.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__repr_str__"><strong>__repr_str__</strong></a>(self, join_str: 'str') -&gt; 'str'</dt></dl>

<dl><dt><a name="OpenAIEmbeddings-__rich_repr__"><strong>__rich_repr__</strong></a>(self) -&gt; 'RichReprResult'</dt><dd><tt>Used&nbsp;by&nbsp;Rich&nbsp;(<a href="https://rich.readthedocs.io/en/stable/pretty.html">https://rich.readthedocs.io/en/stable/pretty.html</a>)&nbsp;to&nbsp;pretty&nbsp;print&nbsp;objects.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__setattr__"><strong>__setattr__</strong></a>(self, name: 'str', value: 'Any') -&gt; 'None'</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__setstate__"><strong>__setstate__</strong></a>(self, state: 'dict[Any, Any]') -&gt; 'None'</dt></dl>

<dl><dt><a name="OpenAIEmbeddings-__str__"><strong>__str__</strong></a>(self) -&gt; 'str'</dt><dd><tt>Return&nbsp;str(self).</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-copy"><strong>copy</strong></a>(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -&gt; 'Self'</dt><dd><tt>Returns&nbsp;a&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.<br>
&nbsp;<br>
!!!&nbsp;warning&nbsp;"Deprecated"<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;is&nbsp;now&nbsp;deprecated;&nbsp;use&nbsp;`model_copy`&nbsp;instead.<br>
&nbsp;<br>
If&nbsp;you&nbsp;need&nbsp;`include`&nbsp;or&nbsp;`exclude`,&nbsp;use:<br>
&nbsp;<br>
```python&nbsp;{test="skip"&nbsp;lint="skip"}<br>
data&nbsp;=&nbsp;self.<a href="#OpenAIEmbeddings-model_dump">model_dump</a>(include=include,&nbsp;exclude=exclude,&nbsp;round_trip=True)<br>
data&nbsp;=&nbsp;{**data,&nbsp;**(update&nbsp;or&nbsp;{})}<br>
copied&nbsp;=&nbsp;self.<a href="#OpenAIEmbeddings-model_validate">model_validate</a>(data)<br>
```<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;Optional&nbsp;set&nbsp;or&nbsp;mapping&nbsp;specifying&nbsp;which&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;copied&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude:&nbsp;Optional&nbsp;set&nbsp;or&nbsp;mapping&nbsp;specifying&nbsp;which&nbsp;fields&nbsp;to&nbsp;exclude&nbsp;in&nbsp;the&nbsp;copied&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;update:&nbsp;Optional&nbsp;dictionary&nbsp;of&nbsp;field-value&nbsp;pairs&nbsp;to&nbsp;override&nbsp;field&nbsp;values&nbsp;in&nbsp;the&nbsp;copied&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;deep:&nbsp;If&nbsp;True,&nbsp;the&nbsp;values&nbsp;of&nbsp;fields&nbsp;that&nbsp;are&nbsp;Pydantic&nbsp;models&nbsp;will&nbsp;be&nbsp;deep-copied.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;copy&nbsp;of&nbsp;the&nbsp;model&nbsp;with&nbsp;included,&nbsp;excluded&nbsp;and&nbsp;updated&nbsp;fields&nbsp;as&nbsp;specified.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-dict"><strong>dict</strong></a>(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -&gt; 'Dict[str, Any]'</dt></dl>

<dl><dt><a name="OpenAIEmbeddings-json"><strong>json</strong></a>(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -&gt; 'str'</dt></dl>

<dl><dt><a name="OpenAIEmbeddings-model_copy"><strong>model_copy</strong></a>(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -&gt; 'Self'</dt><dd><tt>Usage&nbsp;docs:&nbsp;<a href="https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy">https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy</a><br>
&nbsp;<br>
Returns&nbsp;a&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;update:&nbsp;Values&nbsp;to&nbsp;change/add&nbsp;in&nbsp;the&nbsp;new&nbsp;model.&nbsp;Note:&nbsp;the&nbsp;data&nbsp;is&nbsp;not&nbsp;validated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;before&nbsp;creating&nbsp;the&nbsp;new&nbsp;model.&nbsp;You&nbsp;should&nbsp;trust&nbsp;this&nbsp;data.<br>
&nbsp;&nbsp;&nbsp;&nbsp;deep:&nbsp;Set&nbsp;to&nbsp;`True`&nbsp;to&nbsp;make&nbsp;a&nbsp;deep&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;New&nbsp;model&nbsp;instance.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-model_dump"><strong>model_dump</strong></a>(self, *, mode: "Literal['json', 'python'] | str" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: "bool | Literal['none', 'warn', 'error']" = True, serialize_as_any: 'bool' = False) -&gt; 'dict[str, Any]'</dt><dd><tt>Usage&nbsp;docs:&nbsp;<a href="https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump">https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump</a><br>
&nbsp;<br>
Generate&nbsp;a&nbsp;dictionary&nbsp;representation&nbsp;of&nbsp;the&nbsp;model,&nbsp;optionally&nbsp;specifying&nbsp;which&nbsp;fields&nbsp;to&nbsp;include&nbsp;or&nbsp;exclude.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode:&nbsp;The&nbsp;mode&nbsp;in&nbsp;which&nbsp;`to_python`&nbsp;should&nbsp;run.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;mode&nbsp;is&nbsp;'json',&nbsp;the&nbsp;output&nbsp;will&nbsp;only&nbsp;contain&nbsp;JSON&nbsp;serializable&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;mode&nbsp;is&nbsp;'python',&nbsp;the&nbsp;output&nbsp;may&nbsp;contain&nbsp;non-JSON-serializable&nbsp;Python&nbsp;objects.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;A&nbsp;set&nbsp;of&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;output.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude:&nbsp;A&nbsp;set&nbsp;of&nbsp;fields&nbsp;to&nbsp;exclude&nbsp;from&nbsp;the&nbsp;output.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Additional&nbsp;context&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;serializer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;by_alias:&nbsp;Whether&nbsp;to&nbsp;use&nbsp;the&nbsp;field's&nbsp;alias&nbsp;in&nbsp;the&nbsp;dictionary&nbsp;key&nbsp;if&nbsp;defined.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_unset:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;have&nbsp;not&nbsp;been&nbsp;explicitly&nbsp;set.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_defaults:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;are&nbsp;set&nbsp;to&nbsp;their&nbsp;default&nbsp;value.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_none:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;have&nbsp;a&nbsp;value&nbsp;of&nbsp;`None`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;round_trip:&nbsp;If&nbsp;True,&nbsp;dumped&nbsp;values&nbsp;should&nbsp;be&nbsp;valid&nbsp;as&nbsp;input&nbsp;for&nbsp;non-idempotent&nbsp;types&nbsp;such&nbsp;as&nbsp;Json[T].<br>
&nbsp;&nbsp;&nbsp;&nbsp;warnings:&nbsp;How&nbsp;to&nbsp;handle&nbsp;serialization&nbsp;errors.&nbsp;False/"none"&nbsp;ignores&nbsp;them,&nbsp;True/"warn"&nbsp;logs&nbsp;errors,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"error"&nbsp;raises&nbsp;a&nbsp;[`PydanticSerializationError`][pydantic_core.PydanticSerializationError].<br>
&nbsp;&nbsp;&nbsp;&nbsp;serialize_as_any:&nbsp;Whether&nbsp;to&nbsp;serialize&nbsp;fields&nbsp;with&nbsp;duck-typing&nbsp;serialization&nbsp;behavior.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;dictionary&nbsp;representation&nbsp;of&nbsp;the&nbsp;model.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-model_dump_json"><strong>model_dump_json</strong></a>(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: "bool | Literal['none', 'warn', 'error']" = True, serialize_as_any: 'bool' = False) -&gt; 'str'</dt><dd><tt>Usage&nbsp;docs:&nbsp;<a href="https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json">https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json</a><br>
&nbsp;<br>
Generates&nbsp;a&nbsp;JSON&nbsp;representation&nbsp;of&nbsp;the&nbsp;model&nbsp;using&nbsp;Pydantic's&nbsp;`to_json`&nbsp;method.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;indent:&nbsp;Indentation&nbsp;to&nbsp;use&nbsp;in&nbsp;the&nbsp;JSON&nbsp;output.&nbsp;If&nbsp;None&nbsp;is&nbsp;passed,&nbsp;the&nbsp;output&nbsp;will&nbsp;be&nbsp;compact.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;Field(s)&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;JSON&nbsp;output.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude:&nbsp;Field(s)&nbsp;to&nbsp;exclude&nbsp;from&nbsp;the&nbsp;JSON&nbsp;output.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Additional&nbsp;context&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;serializer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;by_alias:&nbsp;Whether&nbsp;to&nbsp;serialize&nbsp;using&nbsp;field&nbsp;aliases.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_unset:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;have&nbsp;not&nbsp;been&nbsp;explicitly&nbsp;set.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_defaults:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;are&nbsp;set&nbsp;to&nbsp;their&nbsp;default&nbsp;value.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_none:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;have&nbsp;a&nbsp;value&nbsp;of&nbsp;`None`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;round_trip:&nbsp;If&nbsp;True,&nbsp;dumped&nbsp;values&nbsp;should&nbsp;be&nbsp;valid&nbsp;as&nbsp;input&nbsp;for&nbsp;non-idempotent&nbsp;types&nbsp;such&nbsp;as&nbsp;Json[T].<br>
&nbsp;&nbsp;&nbsp;&nbsp;warnings:&nbsp;How&nbsp;to&nbsp;handle&nbsp;serialization&nbsp;errors.&nbsp;False/"none"&nbsp;ignores&nbsp;them,&nbsp;True/"warn"&nbsp;logs&nbsp;errors,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"error"&nbsp;raises&nbsp;a&nbsp;[`PydanticSerializationError`][pydantic_core.PydanticSerializationError].<br>
&nbsp;&nbsp;&nbsp;&nbsp;serialize_as_any:&nbsp;Whether&nbsp;to&nbsp;serialize&nbsp;fields&nbsp;with&nbsp;duck-typing&nbsp;serialization&nbsp;behavior.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;JSON&nbsp;string&nbsp;representation&nbsp;of&nbsp;the&nbsp;model.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-model_post_init"><strong>model_post_init</strong></a>(self, _BaseModel__context: 'Any') -&gt; 'None'</dt><dd><tt>Override&nbsp;this&nbsp;method&nbsp;to&nbsp;perform&nbsp;additional&nbsp;initialization&nbsp;after&nbsp;`__init__`&nbsp;and&nbsp;`model_construct`.<br>
This&nbsp;is&nbsp;useful&nbsp;if&nbsp;you&nbsp;want&nbsp;to&nbsp;do&nbsp;some&nbsp;validation&nbsp;that&nbsp;requires&nbsp;the&nbsp;entire&nbsp;model&nbsp;to&nbsp;be&nbsp;initialized.</tt></dd></dl>

<hr>
Class methods inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><a name="OpenAIEmbeddings-__class_getitem__"><strong>__class_getitem__</strong></a>(typevar_values: 'type[Any] | tuple[type[Any], ...]') -&gt; 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAIEmbeddings-__get_pydantic_core_schema__"><strong>__get_pydantic_core_schema__</strong></a>(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -&gt; 'CoreSchema'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Hook&nbsp;into&nbsp;generating&nbsp;the&nbsp;model's&nbsp;CoreSchema.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;source:&nbsp;The&nbsp;class&nbsp;we&nbsp;are&nbsp;generating&nbsp;a&nbsp;schema&nbsp;for.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;will&nbsp;generally&nbsp;be&nbsp;the&nbsp;same&nbsp;as&nbsp;the&nbsp;`cls`&nbsp;argument&nbsp;if&nbsp;this&nbsp;is&nbsp;a&nbsp;classmethod.<br>
&nbsp;&nbsp;&nbsp;&nbsp;handler:&nbsp;A&nbsp;callable&nbsp;that&nbsp;calls&nbsp;into&nbsp;Pydantic's&nbsp;internal&nbsp;CoreSchema&nbsp;generation&nbsp;logic.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;`pydantic-core`&nbsp;`CoreSchema`.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__get_pydantic_json_schema__"><strong>__get_pydantic_json_schema__</strong></a>(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -&gt; 'JsonSchemaValue'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Hook&nbsp;into&nbsp;generating&nbsp;the&nbsp;model's&nbsp;JSON&nbsp;schema.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;core_schema:&nbsp;A&nbsp;`pydantic-core`&nbsp;CoreSchema.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You&nbsp;can&nbsp;ignore&nbsp;this&nbsp;argument&nbsp;and&nbsp;call&nbsp;the&nbsp;handler&nbsp;with&nbsp;a&nbsp;new&nbsp;CoreSchema,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;wrap&nbsp;this&nbsp;CoreSchema&nbsp;(`{'type':&nbsp;'nullable',&nbsp;'schema':&nbsp;current_schema}`),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;just&nbsp;call&nbsp;the&nbsp;handler&nbsp;with&nbsp;the&nbsp;original&nbsp;schema.<br>
&nbsp;&nbsp;&nbsp;&nbsp;handler:&nbsp;Call&nbsp;into&nbsp;Pydantic's&nbsp;internal&nbsp;JSON&nbsp;schema&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;will&nbsp;raise&nbsp;a&nbsp;`pydantic.errors.PydanticInvalidForJsonSchema`&nbsp;if&nbsp;JSON&nbsp;schema<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;generation&nbsp;fails.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Since&nbsp;this&nbsp;gets&nbsp;called&nbsp;by&nbsp;`BaseModel.model_json_schema`&nbsp;you&nbsp;can&nbsp;override&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`schema_generator`&nbsp;argument&nbsp;to&nbsp;that&nbsp;function&nbsp;to&nbsp;change&nbsp;JSON&nbsp;schema&nbsp;generation&nbsp;globally<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;a&nbsp;type.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;JSON&nbsp;schema,&nbsp;as&nbsp;a&nbsp;Python&nbsp;object.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-__pydantic_init_subclass__"><strong>__pydantic_init_subclass__</strong></a>(**kwargs: 'Any') -&gt; 'None'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>This&nbsp;is&nbsp;intended&nbsp;to&nbsp;behave&nbsp;just&nbsp;like&nbsp;`__init_subclass__`,&nbsp;but&nbsp;is&nbsp;called&nbsp;by&nbsp;`ModelMetaclass`<br>
only&nbsp;after&nbsp;the&nbsp;class&nbsp;is&nbsp;actually&nbsp;fully&nbsp;initialized.&nbsp;In&nbsp;particular,&nbsp;attributes&nbsp;like&nbsp;`model_fields`&nbsp;will<br>
be&nbsp;present&nbsp;when&nbsp;this&nbsp;is&nbsp;called.<br>
&nbsp;<br>
This&nbsp;is&nbsp;necessary&nbsp;because&nbsp;`__init_subclass__`&nbsp;will&nbsp;always&nbsp;be&nbsp;called&nbsp;by&nbsp;`type.__new__`,<br>
and&nbsp;it&nbsp;would&nbsp;require&nbsp;a&nbsp;prohibitively&nbsp;large&nbsp;refactor&nbsp;to&nbsp;the&nbsp;`ModelMetaclass`&nbsp;to&nbsp;ensure&nbsp;that<br>
`type.__new__`&nbsp;was&nbsp;called&nbsp;in&nbsp;such&nbsp;a&nbsp;manner&nbsp;that&nbsp;the&nbsp;class&nbsp;would&nbsp;already&nbsp;be&nbsp;sufficiently&nbsp;initialized.<br>
&nbsp;<br>
This&nbsp;will&nbsp;receive&nbsp;the&nbsp;same&nbsp;`kwargs`&nbsp;that&nbsp;would&nbsp;be&nbsp;passed&nbsp;to&nbsp;the&nbsp;standard&nbsp;`__init_subclass__`,&nbsp;namely,<br>
any&nbsp;kwargs&nbsp;passed&nbsp;to&nbsp;the&nbsp;class&nbsp;definition&nbsp;that&nbsp;aren't&nbsp;used&nbsp;internally&nbsp;by&nbsp;pydantic.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Any&nbsp;keyword&nbsp;arguments&nbsp;passed&nbsp;to&nbsp;the&nbsp;class&nbsp;definition&nbsp;that&nbsp;aren't&nbsp;used&nbsp;internally<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;pydantic.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-construct"><strong>construct</strong></a>(_fields_set: 'set[str] | None' = None, **values: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAIEmbeddings-from_orm"><strong>from_orm</strong></a>(obj: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAIEmbeddings-model_construct"><strong>model_construct</strong></a>(_fields_set: 'set[str] | None' = None, **values: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Creates&nbsp;a&nbsp;new&nbsp;instance&nbsp;of&nbsp;the&nbsp;`Model`&nbsp;class&nbsp;with&nbsp;validated&nbsp;data.<br>
&nbsp;<br>
Creates&nbsp;a&nbsp;new&nbsp;model&nbsp;setting&nbsp;`__dict__`&nbsp;and&nbsp;`__pydantic_fields_set__`&nbsp;from&nbsp;trusted&nbsp;or&nbsp;pre-validated&nbsp;data.<br>
Default&nbsp;values&nbsp;are&nbsp;respected,&nbsp;but&nbsp;no&nbsp;other&nbsp;validation&nbsp;is&nbsp;performed.<br>
&nbsp;<br>
!!!&nbsp;note<br>
&nbsp;&nbsp;&nbsp;&nbsp;`<a href="#OpenAIEmbeddings-model_construct">model_construct</a>()`&nbsp;generally&nbsp;respects&nbsp;the&nbsp;`model_config.extra`&nbsp;setting&nbsp;on&nbsp;the&nbsp;provided&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;That&nbsp;is,&nbsp;if&nbsp;`model_config.extra&nbsp;==&nbsp;'allow'`,&nbsp;then&nbsp;all&nbsp;extra&nbsp;passed&nbsp;values&nbsp;are&nbsp;added&nbsp;to&nbsp;the&nbsp;model&nbsp;instance's&nbsp;`__dict__`<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;`__pydantic_extra__`&nbsp;fields.&nbsp;If&nbsp;`model_config.extra&nbsp;==&nbsp;'ignore'`&nbsp;(the&nbsp;default),&nbsp;then&nbsp;all&nbsp;extra&nbsp;passed&nbsp;values&nbsp;are&nbsp;ignored.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Because&nbsp;no&nbsp;validation&nbsp;is&nbsp;performed&nbsp;with&nbsp;a&nbsp;call&nbsp;to&nbsp;`<a href="#OpenAIEmbeddings-model_construct">model_construct</a>()`,&nbsp;having&nbsp;`model_config.extra&nbsp;==&nbsp;'forbid'`&nbsp;does&nbsp;not&nbsp;result&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;an&nbsp;error&nbsp;if&nbsp;extra&nbsp;values&nbsp;are&nbsp;passed,&nbsp;but&nbsp;they&nbsp;will&nbsp;be&nbsp;ignored.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;_fields_set:&nbsp;A&nbsp;set&nbsp;of&nbsp;field&nbsp;names&nbsp;that&nbsp;were&nbsp;originally&nbsp;explicitly&nbsp;set&nbsp;during&nbsp;instantiation.&nbsp;If&nbsp;provided,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;is&nbsp;directly&nbsp;used&nbsp;for&nbsp;the&nbsp;[`model_fields_set`][pydantic.BaseModel.model_fields_set]&nbsp;attribute.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Otherwise,&nbsp;the&nbsp;field&nbsp;names&nbsp;from&nbsp;the&nbsp;`values`&nbsp;argument&nbsp;will&nbsp;be&nbsp;used.<br>
&nbsp;&nbsp;&nbsp;&nbsp;values:&nbsp;Trusted&nbsp;or&nbsp;pre-validated&nbsp;data&nbsp;dictionary.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;instance&nbsp;of&nbsp;the&nbsp;`Model`&nbsp;class&nbsp;with&nbsp;validated&nbsp;data.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-model_json_schema"><strong>model_json_schema</strong></a>(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = &lt;class 'pydantic.json_schema.GenerateJsonSchema'&gt;, mode: 'JsonSchemaMode' = 'validation') -&gt; 'dict[str, Any]'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Generates&nbsp;a&nbsp;JSON&nbsp;schema&nbsp;for&nbsp;a&nbsp;model&nbsp;class.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;by_alias:&nbsp;Whether&nbsp;to&nbsp;use&nbsp;attribute&nbsp;aliases&nbsp;or&nbsp;not.<br>
&nbsp;&nbsp;&nbsp;&nbsp;ref_template:&nbsp;The&nbsp;reference&nbsp;template.<br>
&nbsp;&nbsp;&nbsp;&nbsp;schema_generator:&nbsp;To&nbsp;override&nbsp;the&nbsp;logic&nbsp;used&nbsp;to&nbsp;generate&nbsp;the&nbsp;JSON&nbsp;schema,&nbsp;as&nbsp;a&nbsp;subclass&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`GenerateJsonSchema`&nbsp;with&nbsp;your&nbsp;desired&nbsp;modifications<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode:&nbsp;The&nbsp;mode&nbsp;in&nbsp;which&nbsp;to&nbsp;generate&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;JSON&nbsp;schema&nbsp;for&nbsp;the&nbsp;given&nbsp;model&nbsp;class.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-model_parametrized_name"><strong>model_parametrized_name</strong></a>(params: 'tuple[type[Any], ...]') -&gt; 'str'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Compute&nbsp;the&nbsp;class&nbsp;name&nbsp;for&nbsp;parametrizations&nbsp;of&nbsp;generic&nbsp;classes.<br>
&nbsp;<br>
This&nbsp;method&nbsp;can&nbsp;be&nbsp;overridden&nbsp;to&nbsp;achieve&nbsp;a&nbsp;custom&nbsp;naming&nbsp;scheme&nbsp;for&nbsp;generic&nbsp;BaseModels.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;params:&nbsp;Tuple&nbsp;of&nbsp;types&nbsp;of&nbsp;the&nbsp;class.&nbsp;Given&nbsp;a&nbsp;generic&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`Model`&nbsp;with&nbsp;2&nbsp;type&nbsp;variables&nbsp;and&nbsp;a&nbsp;concrete&nbsp;model&nbsp;`Model[str,&nbsp;int]`,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;value&nbsp;`(str,&nbsp;int)`&nbsp;would&nbsp;be&nbsp;passed&nbsp;to&nbsp;`params`.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;String&nbsp;representing&nbsp;the&nbsp;new&nbsp;class&nbsp;where&nbsp;`params`&nbsp;are&nbsp;passed&nbsp;to&nbsp;`cls`&nbsp;as&nbsp;type&nbsp;variables.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;TypeError:&nbsp;Raised&nbsp;when&nbsp;trying&nbsp;to&nbsp;generate&nbsp;concrete&nbsp;names&nbsp;for&nbsp;non-generic&nbsp;models.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-model_rebuild"><strong>model_rebuild</strong></a>(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -&gt; 'bool | None'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Try&nbsp;to&nbsp;rebuild&nbsp;the&nbsp;pydantic-core&nbsp;schema&nbsp;for&nbsp;the&nbsp;model.<br>
&nbsp;<br>
This&nbsp;may&nbsp;be&nbsp;necessary&nbsp;when&nbsp;one&nbsp;of&nbsp;the&nbsp;annotations&nbsp;is&nbsp;a&nbsp;ForwardRef&nbsp;which&nbsp;could&nbsp;not&nbsp;be&nbsp;resolved&nbsp;during<br>
the&nbsp;initial&nbsp;attempt&nbsp;to&nbsp;build&nbsp;the&nbsp;schema,&nbsp;and&nbsp;automatic&nbsp;rebuilding&nbsp;fails.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;force:&nbsp;Whether&nbsp;to&nbsp;force&nbsp;the&nbsp;rebuilding&nbsp;of&nbsp;the&nbsp;model&nbsp;schema,&nbsp;defaults&nbsp;to&nbsp;`False`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;raise_errors:&nbsp;Whether&nbsp;to&nbsp;raise&nbsp;errors,&nbsp;defaults&nbsp;to&nbsp;`True`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;_parent_namespace_depth:&nbsp;The&nbsp;depth&nbsp;level&nbsp;of&nbsp;the&nbsp;parent&nbsp;namespace,&nbsp;defaults&nbsp;to&nbsp;2.<br>
&nbsp;&nbsp;&nbsp;&nbsp;_types_namespace:&nbsp;The&nbsp;types&nbsp;namespace,&nbsp;defaults&nbsp;to&nbsp;`None`.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Returns&nbsp;`None`&nbsp;if&nbsp;the&nbsp;schema&nbsp;is&nbsp;already&nbsp;"complete"&nbsp;and&nbsp;rebuilding&nbsp;was&nbsp;not&nbsp;required.<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;rebuilding&nbsp;_was_&nbsp;required,&nbsp;returns&nbsp;`True`&nbsp;if&nbsp;rebuilding&nbsp;was&nbsp;successful,&nbsp;otherwise&nbsp;`False`.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-model_validate"><strong>model_validate</strong></a>(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Validate&nbsp;a&nbsp;pydantic&nbsp;model&nbsp;instance.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;obj:&nbsp;The&nbsp;object&nbsp;to&nbsp;validate.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;Whether&nbsp;to&nbsp;enforce&nbsp;types&nbsp;strictly.<br>
&nbsp;&nbsp;&nbsp;&nbsp;from_attributes:&nbsp;Whether&nbsp;to&nbsp;extract&nbsp;data&nbsp;from&nbsp;object&nbsp;attributes.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Additional&nbsp;context&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;validator.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ValidationError:&nbsp;If&nbsp;the&nbsp;object&nbsp;could&nbsp;not&nbsp;be&nbsp;validated.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;validated&nbsp;model&nbsp;instance.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-model_validate_json"><strong>model_validate_json</strong></a>(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Usage&nbsp;docs:&nbsp;<a href="https://docs.pydantic.dev/2.10/concepts/json/#json-parsing">https://docs.pydantic.dev/2.10/concepts/json/#json-parsing</a><br>
&nbsp;<br>
Validate&nbsp;the&nbsp;given&nbsp;JSON&nbsp;data&nbsp;against&nbsp;the&nbsp;Pydantic&nbsp;model.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;json_data:&nbsp;The&nbsp;JSON&nbsp;data&nbsp;to&nbsp;validate.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;Whether&nbsp;to&nbsp;enforce&nbsp;types&nbsp;strictly.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Extra&nbsp;variables&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;validator.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;validated&nbsp;Pydantic&nbsp;model.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ValidationError:&nbsp;If&nbsp;`json_data`&nbsp;is&nbsp;not&nbsp;a&nbsp;JSON&nbsp;string&nbsp;or&nbsp;the&nbsp;object&nbsp;could&nbsp;not&nbsp;be&nbsp;validated.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-model_validate_strings"><strong>model_validate_strings</strong></a>(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Validate&nbsp;the&nbsp;given&nbsp;object&nbsp;with&nbsp;string&nbsp;data&nbsp;against&nbsp;the&nbsp;Pydantic&nbsp;model.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;obj:&nbsp;The&nbsp;object&nbsp;containing&nbsp;string&nbsp;data&nbsp;to&nbsp;validate.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;Whether&nbsp;to&nbsp;enforce&nbsp;types&nbsp;strictly.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Extra&nbsp;variables&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;validator.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;validated&nbsp;Pydantic&nbsp;model.</tt></dd></dl>

<dl><dt><a name="OpenAIEmbeddings-parse_file"><strong>parse_file</strong></a>(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAIEmbeddings-parse_obj"><strong>parse_obj</strong></a>(obj: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAIEmbeddings-parse_raw"><strong>parse_raw</strong></a>(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAIEmbeddings-schema"><strong>schema</strong></a>(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -&gt; 'Dict[str, Any]'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAIEmbeddings-schema_json"><strong>schema_json</strong></a>(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -&gt; 'str'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAIEmbeddings-update_forward_refs"><strong>update_forward_refs</strong></a>(**localns: 'Any') -&gt; 'None'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="OpenAIEmbeddings-validate"><strong>validate</strong></a>(value: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<hr>
Readonly properties inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><strong>__fields_set__</strong></dt>
</dl>
<dl><dt><strong>model_computed_fields</strong></dt>
<dd><tt>Get&nbsp;metadata&nbsp;about&nbsp;the&nbsp;computed&nbsp;fields&nbsp;defined&nbsp;on&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Deprecation&nbsp;warning:&nbsp;you&nbsp;should&nbsp;be&nbsp;getting&nbsp;this&nbsp;information&nbsp;from&nbsp;the&nbsp;model&nbsp;class,&nbsp;not&nbsp;from&nbsp;an&nbsp;instance.<br>
In&nbsp;V3,&nbsp;this&nbsp;property&nbsp;will&nbsp;be&nbsp;removed&nbsp;from&nbsp;the&nbsp;`BaseModel`&nbsp;class.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;mapping&nbsp;of&nbsp;computed&nbsp;field&nbsp;names&nbsp;to&nbsp;[`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo]&nbsp;objects.</tt></dd>
</dl>
<dl><dt><strong>model_extra</strong></dt>
<dd><tt>Get&nbsp;extra&nbsp;fields&nbsp;set&nbsp;during&nbsp;validation.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;dictionary&nbsp;of&nbsp;extra&nbsp;fields,&nbsp;or&nbsp;`None`&nbsp;if&nbsp;`config.extra`&nbsp;is&nbsp;not&nbsp;set&nbsp;to&nbsp;`"allow"`.</tt></dd>
</dl>
<dl><dt><strong>model_fields</strong></dt>
<dd><tt>Get&nbsp;metadata&nbsp;about&nbsp;the&nbsp;fields&nbsp;defined&nbsp;on&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Deprecation&nbsp;warning:&nbsp;you&nbsp;should&nbsp;be&nbsp;getting&nbsp;this&nbsp;information&nbsp;from&nbsp;the&nbsp;model&nbsp;class,&nbsp;not&nbsp;from&nbsp;an&nbsp;instance.<br>
In&nbsp;V3,&nbsp;this&nbsp;property&nbsp;will&nbsp;be&nbsp;removed&nbsp;from&nbsp;the&nbsp;`BaseModel`&nbsp;class.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;mapping&nbsp;of&nbsp;field&nbsp;names&nbsp;to&nbsp;[`FieldInfo`][pydantic.fields.FieldInfo]&nbsp;objects.</tt></dd>
</dl>
<dl><dt><strong>model_fields_set</strong></dt>
<dd><tt>Returns&nbsp;the&nbsp;set&nbsp;of&nbsp;fields&nbsp;that&nbsp;have&nbsp;been&nbsp;explicitly&nbsp;set&nbsp;on&nbsp;this&nbsp;model&nbsp;instance.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;set&nbsp;of&nbsp;strings&nbsp;representing&nbsp;the&nbsp;fields&nbsp;that&nbsp;have&nbsp;been&nbsp;set,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i.e.&nbsp;that&nbsp;were&nbsp;not&nbsp;filled&nbsp;from&nbsp;defaults.</tt></dd>
</dl>
<hr>
Data descriptors inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__pydantic_extra__</strong></dt>
</dl>
<dl><dt><strong>__pydantic_fields_set__</strong></dt>
</dl>
<dl><dt><strong>__pydantic_private__</strong></dt>
</dl>
<hr>
Data and other attributes inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><strong>__hash__</strong> = None</dl>

<dl><dt><strong>__pydantic_root_model__</strong> = False</dl>

</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="ProxyOpenAI">class <strong>ProxyOpenAI</strong></a>(<a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt><a href="#ProxyOpenAI">ProxyOpenAI</a>(*,&nbsp;proxy_client:&nbsp;Optional[Any]&nbsp;=&nbsp;None,&nbsp;deployment_id:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;config_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;config_id:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;proxy_model_name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;**extra_data:&nbsp;Any)&nbsp;-&amp;gt;&nbsp;None<br>
&nbsp;<br>
<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="gen_ai_hub.proxy.langchain.openai.html#ProxyOpenAI">ProxyOpenAI</a></dd>
<dd><a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a></dd>
<dd><a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Class methods defined here:<br>
<dl><dt><a name="ProxyOpenAI-validate_clients"><strong>validate_clients</strong></a>(values: Dict) -&gt; Dict<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>__abstractmethods__</strong> = frozenset()</dl>

<dl><dt><strong>__class_vars__</strong> = set()</dl>

<dl><dt><strong>__private_attributes__</strong> = {}</dl>

<dl><dt><strong>__pydantic_complete__</strong> = True</dl>

<dl><dt><strong>__pydantic_computed_fields__</strong> = {}</dl>

<dl><dt><strong>__pydantic_core_schema__</strong> = {'cls': &lt;class 'gen_ai_hub.proxy.langchain.openai.ProxyOpenAI'&gt;, 'config': {'extra_fields_behavior': 'allow', 'title': 'ProxyOpenAI'}, 'custom_init': False, 'metadata': {'pydantic_js_functions': [&lt;bound method BaseModel.__get_pydantic_json_sche...'gen_ai_hub.proxy.langchain.openai.ProxyOpenAI'&gt;&gt;]}, 'ref': 'gen_ai_hub.proxy.langchain.openai.ProxyOpenAI:140673046515568', 'root_model': False, 'schema': {'computed_fields': [], 'fields': {'config_id': {'metadata': {}, 'schema': {'default': None, 'schema': {'schema': {...}, 'type': 'nullable'}, 'type': 'default'}, 'type': 'model-field'}, 'config_name': {'metadata': {}, 'schema': {'default': None, 'schema': {'schema': {...}, 'type': 'nullable'}, 'type': 'default'}, 'type': 'model-field'}, 'deployment_id': {'metadata': {}, 'schema': {'default': None, 'schema': {'schema': {...}, 'type': 'nullable'}, 'type': 'default'}, 'type': 'model-field'}, 'proxy_client': {'metadata': {}, 'schema': {'default': None, 'schema': {'schema': {...}, 'type': 'nullable'}, 'type': 'default'}, 'type': 'model-field'}, 'proxy_model_name': {'metadata': {}, 'schema': {'default': None, 'schema': {'schema': {...}, 'type': 'nullable'}, 'type': 'default'}, 'type': 'model-field'}}, 'model_name': 'ProxyOpenAI', 'type': 'model-fields'}, 'type': 'model'}</dl>

<dl><dt><strong>__pydantic_custom_init__</strong> = False</dl>

<dl><dt><strong>__pydantic_decorators__</strong> = DecoratorInfos(validators={}, field_validators={...zers={}, model_validators={}, computed_fields={})</dl>

<dl><dt><strong>__pydantic_fields__</strong> = {'config_id': FieldInfo(annotation=Union[str, NoneType], required=False, default=None), 'config_name': FieldInfo(annotation=Union[str, NoneType], required=False, default=None), 'deployment_id': FieldInfo(annotation=Union[str, NoneType], required=False, default=None), 'proxy_client': FieldInfo(annotation=Union[Any, NoneType], required=False, default=None), 'proxy_model_name': FieldInfo(annotation=Union[str, NoneType], required=False, default=None)}</dl>

<dl><dt><strong>__pydantic_generic_metadata__</strong> = {'args': (), 'origin': None, 'parameters': ()}</dl>

<dl><dt><strong>__pydantic_parent_namespace__</strong> = None</dl>

<dl><dt><strong>__pydantic_post_init__</strong> = None</dl>

<dl><dt><strong>__pydantic_serializer__</strong> = SchemaSerializer(serializer=Model(
    ModelSeri...   name: "ProxyOpenAI",
    },
), definitions=[])</dl>

<dl><dt><strong>__pydantic_validator__</strong> = SchemaValidator(title="ProxyOpenAI", validator=M...I",
    },
), definitions=[], cache_strings=True)</dl>

<dl><dt><strong>__signature__</strong> = &lt;Signature (*, proxy_client: Optional[Any] = Non...Optional[str] = None, **extra_data: Any) -&gt; None&gt;</dl>

<dl><dt><strong>model_config</strong> = {'extra': 'allow'}</dl>

<hr>
Data descriptors inherited from <a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a>:<br>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="gen_ai_hub.proxy.langchain.base.html#BaseAuth">gen_ai_hub.proxy.langchain.base.BaseAuth</a>:<br>
<dl><dt><strong>__annotations__</strong> = {'config_id': typing.Optional[str], 'config_name': typing.Optional[str], 'deployment_id': typing.Optional[str], 'proxy_client': typing.Optional[typing.Any], 'proxy_model_name': typing.Optional[str]}</dl>

<hr>
Methods inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><a name="ProxyOpenAI-__copy__"><strong>__copy__</strong></a>(self) -&gt; 'Self'</dt><dd><tt>Returns&nbsp;a&nbsp;shallow&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__deepcopy__"><strong>__deepcopy__</strong></a>(self, memo: 'dict[int, Any] | None' = None) -&gt; 'Self'</dt><dd><tt>Returns&nbsp;a&nbsp;deep&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__delattr__"><strong>__delattr__</strong></a>(self, item: 'str') -&gt; 'Any'</dt><dd><tt>Implement&nbsp;delattr(self,&nbsp;name).</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__eq__"><strong>__eq__</strong></a>(self, other: 'Any') -&gt; 'bool'</dt><dd><tt>Return&nbsp;self==value.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__getattr__"><strong>__getattr__</strong></a>(self, item: 'str') -&gt; 'Any'</dt></dl>

<dl><dt><a name="ProxyOpenAI-__getstate__"><strong>__getstate__</strong></a>(self) -&gt; 'dict[Any, Any]'</dt></dl>

<dl><dt><a name="ProxyOpenAI-__init__"><strong>__init__</strong></a>(self, /, **data: 'Any') -&gt; 'None'</dt><dd><tt>Create&nbsp;a&nbsp;new&nbsp;model&nbsp;by&nbsp;parsing&nbsp;and&nbsp;validating&nbsp;input&nbsp;data&nbsp;from&nbsp;keyword&nbsp;arguments.<br>
&nbsp;<br>
Raises&nbsp;[`ValidationError`][pydantic_core.ValidationError]&nbsp;if&nbsp;the&nbsp;input&nbsp;data&nbsp;cannot&nbsp;be<br>
validated&nbsp;to&nbsp;form&nbsp;a&nbsp;valid&nbsp;model.<br>
&nbsp;<br>
`self`&nbsp;is&nbsp;explicitly&nbsp;positional-only&nbsp;to&nbsp;allow&nbsp;`self`&nbsp;as&nbsp;a&nbsp;field&nbsp;name.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__iter__"><strong>__iter__</strong></a>(self) -&gt; 'TupleGenerator'</dt><dd><tt>So&nbsp;`<a href="#ProxyOpenAI-dict">dict</a>(model)`&nbsp;works.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__pretty__"><strong>__pretty__</strong></a>(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -&gt; 'typing.Generator[Any, None, None]'</dt><dd><tt>Used&nbsp;by&nbsp;devtools&nbsp;(<a href="https://python-devtools.helpmanual.io/">https://python-devtools.helpmanual.io/</a>)&nbsp;to&nbsp;pretty&nbsp;print&nbsp;objects.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__replace__"><strong>__replace__</strong></a>(self, **changes: 'Any') -&gt; 'Self'</dt><dd><tt>#&nbsp;Because&nbsp;we&nbsp;make&nbsp;use&nbsp;of&nbsp;`@dataclass_transform()`,&nbsp;`__replace__`&nbsp;is&nbsp;already&nbsp;synthesized&nbsp;by<br>
#&nbsp;type&nbsp;checkers,&nbsp;so&nbsp;we&nbsp;define&nbsp;the&nbsp;implementation&nbsp;in&nbsp;this&nbsp;`if&nbsp;not&nbsp;TYPE_CHECKING:`&nbsp;block:</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__repr__"><strong>__repr__</strong></a>(self) -&gt; 'str'</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__repr_args__"><strong>__repr_args__</strong></a>(self) -&gt; '_repr.ReprArgs'</dt></dl>

<dl><dt><a name="ProxyOpenAI-__repr_name__"><strong>__repr_name__</strong></a>(self) -&gt; 'str'</dt><dd><tt>Name&nbsp;of&nbsp;the&nbsp;instance's&nbsp;class,&nbsp;used&nbsp;in&nbsp;__repr__.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__repr_recursion__"><strong>__repr_recursion__</strong></a>(self, object: 'Any') -&gt; 'str'</dt><dd><tt>Returns&nbsp;the&nbsp;string&nbsp;representation&nbsp;of&nbsp;a&nbsp;recursive&nbsp;object.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__repr_str__"><strong>__repr_str__</strong></a>(self, join_str: 'str') -&gt; 'str'</dt></dl>

<dl><dt><a name="ProxyOpenAI-__rich_repr__"><strong>__rich_repr__</strong></a>(self) -&gt; 'RichReprResult'</dt><dd><tt>Used&nbsp;by&nbsp;Rich&nbsp;(<a href="https://rich.readthedocs.io/en/stable/pretty.html">https://rich.readthedocs.io/en/stable/pretty.html</a>)&nbsp;to&nbsp;pretty&nbsp;print&nbsp;objects.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__setattr__"><strong>__setattr__</strong></a>(self, name: 'str', value: 'Any') -&gt; 'None'</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__setstate__"><strong>__setstate__</strong></a>(self, state: 'dict[Any, Any]') -&gt; 'None'</dt></dl>

<dl><dt><a name="ProxyOpenAI-__str__"><strong>__str__</strong></a>(self) -&gt; 'str'</dt><dd><tt>Return&nbsp;str(self).</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-copy"><strong>copy</strong></a>(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -&gt; 'Self'</dt><dd><tt>Returns&nbsp;a&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.<br>
&nbsp;<br>
!!!&nbsp;warning&nbsp;"Deprecated"<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;is&nbsp;now&nbsp;deprecated;&nbsp;use&nbsp;`model_copy`&nbsp;instead.<br>
&nbsp;<br>
If&nbsp;you&nbsp;need&nbsp;`include`&nbsp;or&nbsp;`exclude`,&nbsp;use:<br>
&nbsp;<br>
```python&nbsp;{test="skip"&nbsp;lint="skip"}<br>
data&nbsp;=&nbsp;self.<a href="#ProxyOpenAI-model_dump">model_dump</a>(include=include,&nbsp;exclude=exclude,&nbsp;round_trip=True)<br>
data&nbsp;=&nbsp;{**data,&nbsp;**(update&nbsp;or&nbsp;{})}<br>
copied&nbsp;=&nbsp;self.<a href="#ProxyOpenAI-model_validate">model_validate</a>(data)<br>
```<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;Optional&nbsp;set&nbsp;or&nbsp;mapping&nbsp;specifying&nbsp;which&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;copied&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude:&nbsp;Optional&nbsp;set&nbsp;or&nbsp;mapping&nbsp;specifying&nbsp;which&nbsp;fields&nbsp;to&nbsp;exclude&nbsp;in&nbsp;the&nbsp;copied&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;update:&nbsp;Optional&nbsp;dictionary&nbsp;of&nbsp;field-value&nbsp;pairs&nbsp;to&nbsp;override&nbsp;field&nbsp;values&nbsp;in&nbsp;the&nbsp;copied&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;deep:&nbsp;If&nbsp;True,&nbsp;the&nbsp;values&nbsp;of&nbsp;fields&nbsp;that&nbsp;are&nbsp;Pydantic&nbsp;models&nbsp;will&nbsp;be&nbsp;deep-copied.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;copy&nbsp;of&nbsp;the&nbsp;model&nbsp;with&nbsp;included,&nbsp;excluded&nbsp;and&nbsp;updated&nbsp;fields&nbsp;as&nbsp;specified.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-dict"><strong>dict</strong></a>(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -&gt; 'Dict[str, Any]'</dt></dl>

<dl><dt><a name="ProxyOpenAI-json"><strong>json</strong></a>(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -&gt; 'str'</dt></dl>

<dl><dt><a name="ProxyOpenAI-model_copy"><strong>model_copy</strong></a>(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -&gt; 'Self'</dt><dd><tt>Usage&nbsp;docs:&nbsp;<a href="https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy">https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy</a><br>
&nbsp;<br>
Returns&nbsp;a&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;update:&nbsp;Values&nbsp;to&nbsp;change/add&nbsp;in&nbsp;the&nbsp;new&nbsp;model.&nbsp;Note:&nbsp;the&nbsp;data&nbsp;is&nbsp;not&nbsp;validated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;before&nbsp;creating&nbsp;the&nbsp;new&nbsp;model.&nbsp;You&nbsp;should&nbsp;trust&nbsp;this&nbsp;data.<br>
&nbsp;&nbsp;&nbsp;&nbsp;deep:&nbsp;Set&nbsp;to&nbsp;`True`&nbsp;to&nbsp;make&nbsp;a&nbsp;deep&nbsp;copy&nbsp;of&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;New&nbsp;model&nbsp;instance.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-model_dump"><strong>model_dump</strong></a>(self, *, mode: "Literal['json', 'python'] | str" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: "bool | Literal['none', 'warn', 'error']" = True, serialize_as_any: 'bool' = False) -&gt; 'dict[str, Any]'</dt><dd><tt>Usage&nbsp;docs:&nbsp;<a href="https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump">https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump</a><br>
&nbsp;<br>
Generate&nbsp;a&nbsp;dictionary&nbsp;representation&nbsp;of&nbsp;the&nbsp;model,&nbsp;optionally&nbsp;specifying&nbsp;which&nbsp;fields&nbsp;to&nbsp;include&nbsp;or&nbsp;exclude.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode:&nbsp;The&nbsp;mode&nbsp;in&nbsp;which&nbsp;`to_python`&nbsp;should&nbsp;run.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;mode&nbsp;is&nbsp;'json',&nbsp;the&nbsp;output&nbsp;will&nbsp;only&nbsp;contain&nbsp;JSON&nbsp;serializable&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;mode&nbsp;is&nbsp;'python',&nbsp;the&nbsp;output&nbsp;may&nbsp;contain&nbsp;non-JSON-serializable&nbsp;Python&nbsp;objects.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;A&nbsp;set&nbsp;of&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;output.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude:&nbsp;A&nbsp;set&nbsp;of&nbsp;fields&nbsp;to&nbsp;exclude&nbsp;from&nbsp;the&nbsp;output.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Additional&nbsp;context&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;serializer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;by_alias:&nbsp;Whether&nbsp;to&nbsp;use&nbsp;the&nbsp;field's&nbsp;alias&nbsp;in&nbsp;the&nbsp;dictionary&nbsp;key&nbsp;if&nbsp;defined.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_unset:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;have&nbsp;not&nbsp;been&nbsp;explicitly&nbsp;set.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_defaults:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;are&nbsp;set&nbsp;to&nbsp;their&nbsp;default&nbsp;value.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_none:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;have&nbsp;a&nbsp;value&nbsp;of&nbsp;`None`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;round_trip:&nbsp;If&nbsp;True,&nbsp;dumped&nbsp;values&nbsp;should&nbsp;be&nbsp;valid&nbsp;as&nbsp;input&nbsp;for&nbsp;non-idempotent&nbsp;types&nbsp;such&nbsp;as&nbsp;Json[T].<br>
&nbsp;&nbsp;&nbsp;&nbsp;warnings:&nbsp;How&nbsp;to&nbsp;handle&nbsp;serialization&nbsp;errors.&nbsp;False/"none"&nbsp;ignores&nbsp;them,&nbsp;True/"warn"&nbsp;logs&nbsp;errors,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"error"&nbsp;raises&nbsp;a&nbsp;[`PydanticSerializationError`][pydantic_core.PydanticSerializationError].<br>
&nbsp;&nbsp;&nbsp;&nbsp;serialize_as_any:&nbsp;Whether&nbsp;to&nbsp;serialize&nbsp;fields&nbsp;with&nbsp;duck-typing&nbsp;serialization&nbsp;behavior.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;dictionary&nbsp;representation&nbsp;of&nbsp;the&nbsp;model.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-model_dump_json"><strong>model_dump_json</strong></a>(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: "bool | Literal['none', 'warn', 'error']" = True, serialize_as_any: 'bool' = False) -&gt; 'str'</dt><dd><tt>Usage&nbsp;docs:&nbsp;<a href="https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json">https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json</a><br>
&nbsp;<br>
Generates&nbsp;a&nbsp;JSON&nbsp;representation&nbsp;of&nbsp;the&nbsp;model&nbsp;using&nbsp;Pydantic's&nbsp;`to_json`&nbsp;method.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;indent:&nbsp;Indentation&nbsp;to&nbsp;use&nbsp;in&nbsp;the&nbsp;JSON&nbsp;output.&nbsp;If&nbsp;None&nbsp;is&nbsp;passed,&nbsp;the&nbsp;output&nbsp;will&nbsp;be&nbsp;compact.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;Field(s)&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;JSON&nbsp;output.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude:&nbsp;Field(s)&nbsp;to&nbsp;exclude&nbsp;from&nbsp;the&nbsp;JSON&nbsp;output.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Additional&nbsp;context&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;serializer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;by_alias:&nbsp;Whether&nbsp;to&nbsp;serialize&nbsp;using&nbsp;field&nbsp;aliases.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_unset:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;have&nbsp;not&nbsp;been&nbsp;explicitly&nbsp;set.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_defaults:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;are&nbsp;set&nbsp;to&nbsp;their&nbsp;default&nbsp;value.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_none:&nbsp;Whether&nbsp;to&nbsp;exclude&nbsp;fields&nbsp;that&nbsp;have&nbsp;a&nbsp;value&nbsp;of&nbsp;`None`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;round_trip:&nbsp;If&nbsp;True,&nbsp;dumped&nbsp;values&nbsp;should&nbsp;be&nbsp;valid&nbsp;as&nbsp;input&nbsp;for&nbsp;non-idempotent&nbsp;types&nbsp;such&nbsp;as&nbsp;Json[T].<br>
&nbsp;&nbsp;&nbsp;&nbsp;warnings:&nbsp;How&nbsp;to&nbsp;handle&nbsp;serialization&nbsp;errors.&nbsp;False/"none"&nbsp;ignores&nbsp;them,&nbsp;True/"warn"&nbsp;logs&nbsp;errors,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"error"&nbsp;raises&nbsp;a&nbsp;[`PydanticSerializationError`][pydantic_core.PydanticSerializationError].<br>
&nbsp;&nbsp;&nbsp;&nbsp;serialize_as_any:&nbsp;Whether&nbsp;to&nbsp;serialize&nbsp;fields&nbsp;with&nbsp;duck-typing&nbsp;serialization&nbsp;behavior.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;JSON&nbsp;string&nbsp;representation&nbsp;of&nbsp;the&nbsp;model.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-model_post_init"><strong>model_post_init</strong></a>(self, _BaseModel__context: 'Any') -&gt; 'None'</dt><dd><tt>Override&nbsp;this&nbsp;method&nbsp;to&nbsp;perform&nbsp;additional&nbsp;initialization&nbsp;after&nbsp;`__init__`&nbsp;and&nbsp;`model_construct`.<br>
This&nbsp;is&nbsp;useful&nbsp;if&nbsp;you&nbsp;want&nbsp;to&nbsp;do&nbsp;some&nbsp;validation&nbsp;that&nbsp;requires&nbsp;the&nbsp;entire&nbsp;model&nbsp;to&nbsp;be&nbsp;initialized.</tt></dd></dl>

<hr>
Class methods inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><a name="ProxyOpenAI-__class_getitem__"><strong>__class_getitem__</strong></a>(typevar_values: 'type[Any] | tuple[type[Any], ...]') -&gt; 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ProxyOpenAI-__get_pydantic_core_schema__"><strong>__get_pydantic_core_schema__</strong></a>(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -&gt; 'CoreSchema'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Hook&nbsp;into&nbsp;generating&nbsp;the&nbsp;model's&nbsp;CoreSchema.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;source:&nbsp;The&nbsp;class&nbsp;we&nbsp;are&nbsp;generating&nbsp;a&nbsp;schema&nbsp;for.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;will&nbsp;generally&nbsp;be&nbsp;the&nbsp;same&nbsp;as&nbsp;the&nbsp;`cls`&nbsp;argument&nbsp;if&nbsp;this&nbsp;is&nbsp;a&nbsp;classmethod.<br>
&nbsp;&nbsp;&nbsp;&nbsp;handler:&nbsp;A&nbsp;callable&nbsp;that&nbsp;calls&nbsp;into&nbsp;Pydantic's&nbsp;internal&nbsp;CoreSchema&nbsp;generation&nbsp;logic.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;`pydantic-core`&nbsp;`CoreSchema`.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__get_pydantic_json_schema__"><strong>__get_pydantic_json_schema__</strong></a>(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -&gt; 'JsonSchemaValue'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Hook&nbsp;into&nbsp;generating&nbsp;the&nbsp;model's&nbsp;JSON&nbsp;schema.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;core_schema:&nbsp;A&nbsp;`pydantic-core`&nbsp;CoreSchema.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You&nbsp;can&nbsp;ignore&nbsp;this&nbsp;argument&nbsp;and&nbsp;call&nbsp;the&nbsp;handler&nbsp;with&nbsp;a&nbsp;new&nbsp;CoreSchema,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;wrap&nbsp;this&nbsp;CoreSchema&nbsp;(`{'type':&nbsp;'nullable',&nbsp;'schema':&nbsp;current_schema}`),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;just&nbsp;call&nbsp;the&nbsp;handler&nbsp;with&nbsp;the&nbsp;original&nbsp;schema.<br>
&nbsp;&nbsp;&nbsp;&nbsp;handler:&nbsp;Call&nbsp;into&nbsp;Pydantic's&nbsp;internal&nbsp;JSON&nbsp;schema&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;will&nbsp;raise&nbsp;a&nbsp;`pydantic.errors.PydanticInvalidForJsonSchema`&nbsp;if&nbsp;JSON&nbsp;schema<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;generation&nbsp;fails.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Since&nbsp;this&nbsp;gets&nbsp;called&nbsp;by&nbsp;`BaseModel.model_json_schema`&nbsp;you&nbsp;can&nbsp;override&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`schema_generator`&nbsp;argument&nbsp;to&nbsp;that&nbsp;function&nbsp;to&nbsp;change&nbsp;JSON&nbsp;schema&nbsp;generation&nbsp;globally<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;a&nbsp;type.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;JSON&nbsp;schema,&nbsp;as&nbsp;a&nbsp;Python&nbsp;object.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-__pydantic_init_subclass__"><strong>__pydantic_init_subclass__</strong></a>(**kwargs: 'Any') -&gt; 'None'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>This&nbsp;is&nbsp;intended&nbsp;to&nbsp;behave&nbsp;just&nbsp;like&nbsp;`__init_subclass__`,&nbsp;but&nbsp;is&nbsp;called&nbsp;by&nbsp;`ModelMetaclass`<br>
only&nbsp;after&nbsp;the&nbsp;class&nbsp;is&nbsp;actually&nbsp;fully&nbsp;initialized.&nbsp;In&nbsp;particular,&nbsp;attributes&nbsp;like&nbsp;`model_fields`&nbsp;will<br>
be&nbsp;present&nbsp;when&nbsp;this&nbsp;is&nbsp;called.<br>
&nbsp;<br>
This&nbsp;is&nbsp;necessary&nbsp;because&nbsp;`__init_subclass__`&nbsp;will&nbsp;always&nbsp;be&nbsp;called&nbsp;by&nbsp;`type.__new__`,<br>
and&nbsp;it&nbsp;would&nbsp;require&nbsp;a&nbsp;prohibitively&nbsp;large&nbsp;refactor&nbsp;to&nbsp;the&nbsp;`ModelMetaclass`&nbsp;to&nbsp;ensure&nbsp;that<br>
`type.__new__`&nbsp;was&nbsp;called&nbsp;in&nbsp;such&nbsp;a&nbsp;manner&nbsp;that&nbsp;the&nbsp;class&nbsp;would&nbsp;already&nbsp;be&nbsp;sufficiently&nbsp;initialized.<br>
&nbsp;<br>
This&nbsp;will&nbsp;receive&nbsp;the&nbsp;same&nbsp;`kwargs`&nbsp;that&nbsp;would&nbsp;be&nbsp;passed&nbsp;to&nbsp;the&nbsp;standard&nbsp;`__init_subclass__`,&nbsp;namely,<br>
any&nbsp;kwargs&nbsp;passed&nbsp;to&nbsp;the&nbsp;class&nbsp;definition&nbsp;that&nbsp;aren't&nbsp;used&nbsp;internally&nbsp;by&nbsp;pydantic.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Any&nbsp;keyword&nbsp;arguments&nbsp;passed&nbsp;to&nbsp;the&nbsp;class&nbsp;definition&nbsp;that&nbsp;aren't&nbsp;used&nbsp;internally<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;pydantic.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-construct"><strong>construct</strong></a>(_fields_set: 'set[str] | None' = None, **values: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ProxyOpenAI-from_orm"><strong>from_orm</strong></a>(obj: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ProxyOpenAI-model_construct"><strong>model_construct</strong></a>(_fields_set: 'set[str] | None' = None, **values: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Creates&nbsp;a&nbsp;new&nbsp;instance&nbsp;of&nbsp;the&nbsp;`Model`&nbsp;class&nbsp;with&nbsp;validated&nbsp;data.<br>
&nbsp;<br>
Creates&nbsp;a&nbsp;new&nbsp;model&nbsp;setting&nbsp;`__dict__`&nbsp;and&nbsp;`__pydantic_fields_set__`&nbsp;from&nbsp;trusted&nbsp;or&nbsp;pre-validated&nbsp;data.<br>
Default&nbsp;values&nbsp;are&nbsp;respected,&nbsp;but&nbsp;no&nbsp;other&nbsp;validation&nbsp;is&nbsp;performed.<br>
&nbsp;<br>
!!!&nbsp;note<br>
&nbsp;&nbsp;&nbsp;&nbsp;`<a href="#ProxyOpenAI-model_construct">model_construct</a>()`&nbsp;generally&nbsp;respects&nbsp;the&nbsp;`model_config.extra`&nbsp;setting&nbsp;on&nbsp;the&nbsp;provided&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;That&nbsp;is,&nbsp;if&nbsp;`model_config.extra&nbsp;==&nbsp;'allow'`,&nbsp;then&nbsp;all&nbsp;extra&nbsp;passed&nbsp;values&nbsp;are&nbsp;added&nbsp;to&nbsp;the&nbsp;model&nbsp;instance's&nbsp;`__dict__`<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;`__pydantic_extra__`&nbsp;fields.&nbsp;If&nbsp;`model_config.extra&nbsp;==&nbsp;'ignore'`&nbsp;(the&nbsp;default),&nbsp;then&nbsp;all&nbsp;extra&nbsp;passed&nbsp;values&nbsp;are&nbsp;ignored.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Because&nbsp;no&nbsp;validation&nbsp;is&nbsp;performed&nbsp;with&nbsp;a&nbsp;call&nbsp;to&nbsp;`<a href="#ProxyOpenAI-model_construct">model_construct</a>()`,&nbsp;having&nbsp;`model_config.extra&nbsp;==&nbsp;'forbid'`&nbsp;does&nbsp;not&nbsp;result&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;an&nbsp;error&nbsp;if&nbsp;extra&nbsp;values&nbsp;are&nbsp;passed,&nbsp;but&nbsp;they&nbsp;will&nbsp;be&nbsp;ignored.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;_fields_set:&nbsp;A&nbsp;set&nbsp;of&nbsp;field&nbsp;names&nbsp;that&nbsp;were&nbsp;originally&nbsp;explicitly&nbsp;set&nbsp;during&nbsp;instantiation.&nbsp;If&nbsp;provided,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;is&nbsp;directly&nbsp;used&nbsp;for&nbsp;the&nbsp;[`model_fields_set`][pydantic.BaseModel.model_fields_set]&nbsp;attribute.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Otherwise,&nbsp;the&nbsp;field&nbsp;names&nbsp;from&nbsp;the&nbsp;`values`&nbsp;argument&nbsp;will&nbsp;be&nbsp;used.<br>
&nbsp;&nbsp;&nbsp;&nbsp;values:&nbsp;Trusted&nbsp;or&nbsp;pre-validated&nbsp;data&nbsp;dictionary.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;instance&nbsp;of&nbsp;the&nbsp;`Model`&nbsp;class&nbsp;with&nbsp;validated&nbsp;data.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-model_json_schema"><strong>model_json_schema</strong></a>(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = &lt;class 'pydantic.json_schema.GenerateJsonSchema'&gt;, mode: 'JsonSchemaMode' = 'validation') -&gt; 'dict[str, Any]'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Generates&nbsp;a&nbsp;JSON&nbsp;schema&nbsp;for&nbsp;a&nbsp;model&nbsp;class.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;by_alias:&nbsp;Whether&nbsp;to&nbsp;use&nbsp;attribute&nbsp;aliases&nbsp;or&nbsp;not.<br>
&nbsp;&nbsp;&nbsp;&nbsp;ref_template:&nbsp;The&nbsp;reference&nbsp;template.<br>
&nbsp;&nbsp;&nbsp;&nbsp;schema_generator:&nbsp;To&nbsp;override&nbsp;the&nbsp;logic&nbsp;used&nbsp;to&nbsp;generate&nbsp;the&nbsp;JSON&nbsp;schema,&nbsp;as&nbsp;a&nbsp;subclass&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`GenerateJsonSchema`&nbsp;with&nbsp;your&nbsp;desired&nbsp;modifications<br>
&nbsp;&nbsp;&nbsp;&nbsp;mode:&nbsp;The&nbsp;mode&nbsp;in&nbsp;which&nbsp;to&nbsp;generate&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;JSON&nbsp;schema&nbsp;for&nbsp;the&nbsp;given&nbsp;model&nbsp;class.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-model_parametrized_name"><strong>model_parametrized_name</strong></a>(params: 'tuple[type[Any], ...]') -&gt; 'str'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Compute&nbsp;the&nbsp;class&nbsp;name&nbsp;for&nbsp;parametrizations&nbsp;of&nbsp;generic&nbsp;classes.<br>
&nbsp;<br>
This&nbsp;method&nbsp;can&nbsp;be&nbsp;overridden&nbsp;to&nbsp;achieve&nbsp;a&nbsp;custom&nbsp;naming&nbsp;scheme&nbsp;for&nbsp;generic&nbsp;BaseModels.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;params:&nbsp;Tuple&nbsp;of&nbsp;types&nbsp;of&nbsp;the&nbsp;class.&nbsp;Given&nbsp;a&nbsp;generic&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`Model`&nbsp;with&nbsp;2&nbsp;type&nbsp;variables&nbsp;and&nbsp;a&nbsp;concrete&nbsp;model&nbsp;`Model[str,&nbsp;int]`,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;value&nbsp;`(str,&nbsp;int)`&nbsp;would&nbsp;be&nbsp;passed&nbsp;to&nbsp;`params`.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;String&nbsp;representing&nbsp;the&nbsp;new&nbsp;class&nbsp;where&nbsp;`params`&nbsp;are&nbsp;passed&nbsp;to&nbsp;`cls`&nbsp;as&nbsp;type&nbsp;variables.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;TypeError:&nbsp;Raised&nbsp;when&nbsp;trying&nbsp;to&nbsp;generate&nbsp;concrete&nbsp;names&nbsp;for&nbsp;non-generic&nbsp;models.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-model_rebuild"><strong>model_rebuild</strong></a>(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -&gt; 'bool | None'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Try&nbsp;to&nbsp;rebuild&nbsp;the&nbsp;pydantic-core&nbsp;schema&nbsp;for&nbsp;the&nbsp;model.<br>
&nbsp;<br>
This&nbsp;may&nbsp;be&nbsp;necessary&nbsp;when&nbsp;one&nbsp;of&nbsp;the&nbsp;annotations&nbsp;is&nbsp;a&nbsp;ForwardRef&nbsp;which&nbsp;could&nbsp;not&nbsp;be&nbsp;resolved&nbsp;during<br>
the&nbsp;initial&nbsp;attempt&nbsp;to&nbsp;build&nbsp;the&nbsp;schema,&nbsp;and&nbsp;automatic&nbsp;rebuilding&nbsp;fails.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;force:&nbsp;Whether&nbsp;to&nbsp;force&nbsp;the&nbsp;rebuilding&nbsp;of&nbsp;the&nbsp;model&nbsp;schema,&nbsp;defaults&nbsp;to&nbsp;`False`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;raise_errors:&nbsp;Whether&nbsp;to&nbsp;raise&nbsp;errors,&nbsp;defaults&nbsp;to&nbsp;`True`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;_parent_namespace_depth:&nbsp;The&nbsp;depth&nbsp;level&nbsp;of&nbsp;the&nbsp;parent&nbsp;namespace,&nbsp;defaults&nbsp;to&nbsp;2.<br>
&nbsp;&nbsp;&nbsp;&nbsp;_types_namespace:&nbsp;The&nbsp;types&nbsp;namespace,&nbsp;defaults&nbsp;to&nbsp;`None`.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Returns&nbsp;`None`&nbsp;if&nbsp;the&nbsp;schema&nbsp;is&nbsp;already&nbsp;"complete"&nbsp;and&nbsp;rebuilding&nbsp;was&nbsp;not&nbsp;required.<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;rebuilding&nbsp;_was_&nbsp;required,&nbsp;returns&nbsp;`True`&nbsp;if&nbsp;rebuilding&nbsp;was&nbsp;successful,&nbsp;otherwise&nbsp;`False`.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-model_validate"><strong>model_validate</strong></a>(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Validate&nbsp;a&nbsp;pydantic&nbsp;model&nbsp;instance.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;obj:&nbsp;The&nbsp;object&nbsp;to&nbsp;validate.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;Whether&nbsp;to&nbsp;enforce&nbsp;types&nbsp;strictly.<br>
&nbsp;&nbsp;&nbsp;&nbsp;from_attributes:&nbsp;Whether&nbsp;to&nbsp;extract&nbsp;data&nbsp;from&nbsp;object&nbsp;attributes.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Additional&nbsp;context&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;validator.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ValidationError:&nbsp;If&nbsp;the&nbsp;object&nbsp;could&nbsp;not&nbsp;be&nbsp;validated.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;validated&nbsp;model&nbsp;instance.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-model_validate_json"><strong>model_validate_json</strong></a>(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Usage&nbsp;docs:&nbsp;<a href="https://docs.pydantic.dev/2.10/concepts/json/#json-parsing">https://docs.pydantic.dev/2.10/concepts/json/#json-parsing</a><br>
&nbsp;<br>
Validate&nbsp;the&nbsp;given&nbsp;JSON&nbsp;data&nbsp;against&nbsp;the&nbsp;Pydantic&nbsp;model.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;json_data:&nbsp;The&nbsp;JSON&nbsp;data&nbsp;to&nbsp;validate.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;Whether&nbsp;to&nbsp;enforce&nbsp;types&nbsp;strictly.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Extra&nbsp;variables&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;validator.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;validated&nbsp;Pydantic&nbsp;model.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ValidationError:&nbsp;If&nbsp;`json_data`&nbsp;is&nbsp;not&nbsp;a&nbsp;JSON&nbsp;string&nbsp;or&nbsp;the&nbsp;object&nbsp;could&nbsp;not&nbsp;be&nbsp;validated.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-model_validate_strings"><strong>model_validate_strings</strong></a>(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt><dd><tt>Validate&nbsp;the&nbsp;given&nbsp;object&nbsp;with&nbsp;string&nbsp;data&nbsp;against&nbsp;the&nbsp;Pydantic&nbsp;model.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;obj:&nbsp;The&nbsp;object&nbsp;containing&nbsp;string&nbsp;data&nbsp;to&nbsp;validate.<br>
&nbsp;&nbsp;&nbsp;&nbsp;strict:&nbsp;Whether&nbsp;to&nbsp;enforce&nbsp;types&nbsp;strictly.<br>
&nbsp;&nbsp;&nbsp;&nbsp;context:&nbsp;Extra&nbsp;variables&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;validator.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;validated&nbsp;Pydantic&nbsp;model.</tt></dd></dl>

<dl><dt><a name="ProxyOpenAI-parse_file"><strong>parse_file</strong></a>(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ProxyOpenAI-parse_obj"><strong>parse_obj</strong></a>(obj: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ProxyOpenAI-parse_raw"><strong>parse_raw</strong></a>(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ProxyOpenAI-schema"><strong>schema</strong></a>(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -&gt; 'Dict[str, Any]'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ProxyOpenAI-schema_json"><strong>schema_json</strong></a>(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -&gt; 'str'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ProxyOpenAI-update_forward_refs"><strong>update_forward_refs</strong></a>(**localns: 'Any') -&gt; 'None'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ProxyOpenAI-validate"><strong>validate</strong></a>(value: 'Any') -&gt; 'Self'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic._internal._model_construction.html#ModelMetaclass">pydantic._internal._model_construction.ModelMetaclass</a></font></font></dt></dl>

<hr>
Readonly properties inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><strong>__fields_set__</strong></dt>
</dl>
<dl><dt><strong>model_computed_fields</strong></dt>
<dd><tt>Get&nbsp;metadata&nbsp;about&nbsp;the&nbsp;computed&nbsp;fields&nbsp;defined&nbsp;on&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Deprecation&nbsp;warning:&nbsp;you&nbsp;should&nbsp;be&nbsp;getting&nbsp;this&nbsp;information&nbsp;from&nbsp;the&nbsp;model&nbsp;class,&nbsp;not&nbsp;from&nbsp;an&nbsp;instance.<br>
In&nbsp;V3,&nbsp;this&nbsp;property&nbsp;will&nbsp;be&nbsp;removed&nbsp;from&nbsp;the&nbsp;`BaseModel`&nbsp;class.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;mapping&nbsp;of&nbsp;computed&nbsp;field&nbsp;names&nbsp;to&nbsp;[`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo]&nbsp;objects.</tt></dd>
</dl>
<dl><dt><strong>model_extra</strong></dt>
<dd><tt>Get&nbsp;extra&nbsp;fields&nbsp;set&nbsp;during&nbsp;validation.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;dictionary&nbsp;of&nbsp;extra&nbsp;fields,&nbsp;or&nbsp;`None`&nbsp;if&nbsp;`config.extra`&nbsp;is&nbsp;not&nbsp;set&nbsp;to&nbsp;`"allow"`.</tt></dd>
</dl>
<dl><dt><strong>model_fields</strong></dt>
<dd><tt>Get&nbsp;metadata&nbsp;about&nbsp;the&nbsp;fields&nbsp;defined&nbsp;on&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Deprecation&nbsp;warning:&nbsp;you&nbsp;should&nbsp;be&nbsp;getting&nbsp;this&nbsp;information&nbsp;from&nbsp;the&nbsp;model&nbsp;class,&nbsp;not&nbsp;from&nbsp;an&nbsp;instance.<br>
In&nbsp;V3,&nbsp;this&nbsp;property&nbsp;will&nbsp;be&nbsp;removed&nbsp;from&nbsp;the&nbsp;`BaseModel`&nbsp;class.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;mapping&nbsp;of&nbsp;field&nbsp;names&nbsp;to&nbsp;[`FieldInfo`][pydantic.fields.FieldInfo]&nbsp;objects.</tt></dd>
</dl>
<dl><dt><strong>model_fields_set</strong></dt>
<dd><tt>Returns&nbsp;the&nbsp;set&nbsp;of&nbsp;fields&nbsp;that&nbsp;have&nbsp;been&nbsp;explicitly&nbsp;set&nbsp;on&nbsp;this&nbsp;model&nbsp;instance.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;set&nbsp;of&nbsp;strings&nbsp;representing&nbsp;the&nbsp;fields&nbsp;that&nbsp;have&nbsp;been&nbsp;set,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i.e.&nbsp;that&nbsp;were&nbsp;not&nbsp;filled&nbsp;from&nbsp;defaults.</tt></dd>
</dl>
<hr>
Data descriptors inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__pydantic_extra__</strong></dt>
</dl>
<dl><dt><strong>__pydantic_fields_set__</strong></dt>
</dl>
<dl><dt><strong>__pydantic_private__</strong></dt>
</dl>
<hr>
Data and other attributes inherited from <a href="pydantic.main.html#BaseModel">pydantic.main.BaseModel</a>:<br>
<dl><dt><strong>__hash__</strong> = None</dl>

<dl><dt><strong>__pydantic_root_model__</strong> = False</dl>

</td></tr></table></td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#eeaa77">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Functions</strong></big></font></td></tr>
    
<tr><td bgcolor="#eeaa77"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl><dt><a name="-get_client_params"><strong>get_client_params</strong></a>(values)</dt><dd><tt>Get&nbsp;the&nbsp;client&nbsp;parameters.<br>
:param&nbsp;values:&nbsp;The&nbsp;client&nbsp;values<br>
:return:&nbsp;client&nbsp;values&nbsp;+&nbsp;proxy_client</tt></dd></dl>
 <dl><dt><a name="-init_chat_model"><strong>init_chat_model</strong></a>(proxy_client: gen_ai_hub.proxy.core.base.BaseProxyClient, deployment: gen_ai_hub.proxy.core.base.BaseDeployment, temperature: float = 0.0, max_tokens: int = 256, top_k: Optional[int] = None, top_p: float = 1.0)</dt></dl>
 <dl><dt><a name="-init_embedding_model"><strong>init_embedding_model</strong></a>(proxy_client: gen_ai_hub.proxy.core.base.BaseProxyClient, deployment: gen_ai_hub.proxy.core.base.BaseDeployment)</dt></dl>
</td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#55aa55">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Data</strong></big></font></td></tr>
    
<tr><td bgcolor="#55aa55"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><strong>Any</strong> = typing.Any<br>
<strong>DEFAULT_API_VERSION</strong> = '2024-12-01-preview'<br>
<strong>Dict</strong> = typing.Dict<br>
<strong>List</strong> = typing.List<br>
<strong>Optional</strong> = typing.Optional<br>
<strong>catalog</strong> = &lt;gen_ai_hub.proxy.langchain.init_models.Catalog object&gt;</td></tr></table>
</body></html>